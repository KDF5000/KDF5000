<html><head><meta name="generator" content="Hexo 3.9.0"><title>机器学习实战:K-Means聚类算法</title><meta name="keywords" content="KDF5000, OpenHex"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><link href="/css/main.css?v=3" rel="stylesheet" type="text/css"><script src="/js/util.js"></script><script>isMobile()?loadjscssfile("../css/mobile.css","css"):loadjscssfile("../css/desktop.css","css")</script><link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=3"></head><body><h2 class="title">机器学习实战:K-Means聚类算法</h2><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#k-means聚类算法"><span class="toc-text">k-means聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#起始质心"><span class="toc-text">起始质心</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#计算两点的距离"><span class="toc-text">计算两点的距离</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#k-means算法的实现"><span class="toc-text">k-means算法的实现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#测试"><span class="toc-text">测试</span></a></li></ol></li></ol><h4 id="k-means聚类算法"><a href="#k-means聚类算法" class="headerlink" title="k-means聚类算法"></a>k-means聚类算法</h4><blockquote><p>优点: 容易实现<br>缺点: 可能收敛到局部最小值,在大规模数据集上收敛较慢<br>使用数据类型: 数值型护具</p></blockquote><p>k-均值是发现给定数据集的k个簇的算法.k有用户决定.每一个簇通过旗质心,即簇中所有点的中心描述.<br>工作流程: 首先随机确定k个初始点作为其质心.然后讲护具集中的每个点分配到一个簇中,也就是分配到距其最近的质心对应的簇.这一步完成时后,每个簇的质心更新为该簇所有点的平均值.<br>上述过程的伪代码如下:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">创建k个点作为起始质心(随机选择)</span><br><span class="line">当任意一个点的簇分配结果改变时:</span><br><span class="line">    对数据集中的每个数据点</span><br><span class="line">         对每个质心</span><br><span class="line">              计算质心到数据点之间的距离</span><br><span class="line">         讲数据点分配到距其最近的簇</span><br><span class="line">    对每一个簇,计算簇中所有点的均值幷讲均值作为质心</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><h5 id="起始质心"><a href="#起始质心" class="headerlink" title="起始质心"></a>起始质心</h5><p>随机生成指定个数的起始质心,一般可能采取选择数据点中的几个点,本文使用所提供的数据点的各个维度的最大值和最小值随机生成基于最大和最小之间的数值,代码如下:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 随机取k个中心</span><br><span class="line">def randCent(dataSet, k):</span><br><span class="line">    n = shape(dataSet)[1]  # 列数</span><br><span class="line">    centroids = mat(zeros((k, n))) # k行n列的矩阵 也就是取k个n维向量</span><br><span class="line">    for j in range(n):</span><br><span class="line">        minJ = min(dataSet[:, j])</span><br><span class="line">        maxJ = max(dataSet[:, j])</span><br><span class="line">        rangeJ = float(maxJ - minJ)</span><br><span class="line">         # 生成j列向量</span><br><span class="line">        centroids[:, j] = minJ + rangeJ * random.rand(k, 1) </span><br><span class="line"></span><br><span class="line">    return centroids</span><br></pre></td></tr></table></figure><p></p><h5 id="计算两点的距离"><a href="#计算两点的距离" class="headerlink" title="计算两点的距离"></a>计算两点的距离</h5><p>计算两点有时候是两个向量(可以认为是高维的点)之间的距离有很多方法,在机器学习或者数据挖掘中经常需要计算两个向量的相似度,实际上也是计算两个向量的距离.计算距离的方法有很多,比如欧氏距离,曼哈顿距离,夹角余弦等等,本文采用的是欧式距离<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 计算欧式距离</span><br><span class="line">def distEclud(vecA, vecB):</span><br><span class="line">    return sqrt(sum(power(vecA-vecB, 2)))</span><br></pre></td></tr></table></figure><p></p><h5 id="k-means算法的实现"><a href="#k-means算法的实现" class="headerlink" title="k-means算法的实现"></a>k-means算法的实现</h5><p>通过<code>randCent</code>随机选择k个质心,然后计算每个数据点与各个质心的距离,分配到距离最小的质心所在的簇,然后队每个簇根据其均值重新计算质心,然后在队每个点进行距离起算,聚类直到所有点分配结果不在改变为止.<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"># k-means算法</span><br><span class="line">def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    创建k个点作为起始质心(随机选择)</span><br><span class="line">    当任意一个点的簇分配结果改变时:</span><br><span class="line">        对数据集中的每个数据点</span><br><span class="line">             对每个质心</span><br><span class="line">                  计算质心到数据点之间的距离</span><br><span class="line">             讲数据点分配到距其最近的簇</span><br><span class="line">        对每一个簇,计算簇中所有点的均值幷讲均值作为质心</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    m = shape(dataSet)[0]</span><br><span class="line">    # 第一列记录最近簇的索引,第二咧是距离</span><br><span class="line">    clusterAssment = mat(zeros((m, 2)))  </span><br><span class="line">    centroids = createCent(dataSet, k)</span><br><span class="line">    clusterChanged = True</span><br><span class="line">    while clusterChanged:</span><br><span class="line">        clusterChanged = False</span><br><span class="line">        for i in range(m):</span><br><span class="line">            minDist = inf</span><br><span class="line">            minIndex = -1</span><br><span class="line">            for j in range(k):</span><br><span class="line">                distJI = distMeas(centroids[j, :], dataSet[i, :])</span><br><span class="line">                if distJI &lt; minDist:</span><br><span class="line">                    minDist = distJI</span><br><span class="line">                    minIndex = j</span><br><span class="line">            if clusterAssment[i, 0] != minIndex:</span><br><span class="line">                clusterChanged = True</span><br><span class="line">            clusterAssment[i, :] = minIndex, minDist ** 2</span><br><span class="line">        # 更新质心的位置</span><br><span class="line">        for cent in range(k):</span><br><span class="line">            ptsInClust = dataSet[nonzero(clusterAssment[:, 0].A == cent)[0]]</span><br><span class="line">            centroids[cent, :] = mean(ptsInClust, axis=0)</span><br><span class="line">    return centroids, clusterAssment</span><br></pre></td></tr></table></figure><p></p><h5 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h5><p>给定一个测试文件,看看分类的效果,测试文件的内容如下,每行有两个值,一个是x坐标一个是y坐标,使用上面的算法看看聚类效果如何<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1.658985	4.285136</span><br><span class="line">-3.453687	3.424321</span><br><span class="line">4.838138	-1.151539</span><br><span class="line">-5.379713	-3.362104</span><br><span class="line">0.972564	2.924086</span><br><span class="line">-3.567919	1.531611</span><br><span class="line">0.450614	-3.302219</span><br><span class="line">-3.487105	-1.724432</span><br><span class="line">2.668759	1.594842</span><br><span class="line">-3.156485	3.191137</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p></p><p>使用python的图形库matplotlib将测试文件的数据点绘制再二维平面中<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    dataMat = mat(loadDataSet(&apos;testSet.txt&apos;))</span><br><span class="line">    centroids, clusterAssment = kMeans(dataMat, 4)</span><br><span class="line">    pl.plot(centroids[:, 0], centroids[:, 1], &apos;r&apos;)</span><br><span class="line">    pl.plot(dataMat[:, 0], dataMat[:, 1], &apos;bo&apos;)</span><br><span class="line">    pl.show()</span><br></pre></td></tr></table></figure><p></p><p>结果如下:<br><img src="/images/archive/img_figure_1.png" alt><br>图中红色的点是四个簇的质心,可以看出k-means的聚类效果还是挺好的.</p><p><a href="https://github.com/KDF5000/MLPractice/tree/master/ch10" target="_blank" rel="noopener">源码下载</a></p><div style="display:none"><script src="http://s4.cnzz.com/stat.php?id=undefined&web_id=undefined" language="JavaScript"></script>script></div><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>