<html><head><meta name="generator" content="Hexo 3.9.0"><title>机器学习实战:二分k-均值聚类算法</title><meta name="keywords" content="KDF5000, OpenHex"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><link href="/css/main.css?v=3" rel="stylesheet" type="text/css"><script src="/js/util.js"></script><script>isMobile()?loadjscssfile("../css/mobile.css","css"):loadjscssfile("../css/desktop.css","css")</script><link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=3"></head><body><h2 class="title">机器学习实战:二分k-均值聚类算法</h2><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#二分k均值算法"><span class="toc-text">二分k均值算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#测试"><span class="toc-text">测试</span></a></li></ol><p>前面介绍过k-means聚类算法,通过不断的更新簇质心直到收敛为止,但是这个收敛是局部收敛到了最小值,并没有考虑全局的最小值.</p><p>那么一个聚类算法怎么才能称得上效果好呢?要想评价一个算法的好坏,首先需要有一个标准,这也是我们设计算法的时候要首先考虑的,我们设计算法的目的是什么,设计的算法要达到的什么效果,这样才能明确设计算法的目标.一种度量聚类算法效果的指标是SSE(Sum of Squard Error, 误差平方和),也就是前面介绍的计算数据点与质心的欧氏距离的平方和.SSE越小说明数据点越接近它们的质心,聚类效果越好.</p><a id="more"></a><h4 id="二分k均值算法"><a href="#二分k均值算法" class="headerlink" title="二分k均值算法"></a>二分k均值算法</h4><p>为了克服k-均值算法的局部最小值问题,有人提出了二分k均值算法.该算法首先将所有点作为一个簇,然后讲该簇一分为二.之后选择其中一个簇继续划分,选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值.上述基于SSE的划分过程不断重复,直到得到用户指定的簇数目为止.</p><p>二分k均值算法的伪代码形式如下:</p><blockquote><blockquote></blockquote><p>将所有点看成一个簇<br>当簇数目小于k时<br> 对于每一个簇<br> 计算总误差<br> 在给定的簇上进行2均值聚类<br> 计算讲该簇一分为二之后的总误差<br> 选择使得总误差最小的那个簇进行划分操作</p></blockquote><p>下面是python的实现<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">def binKMeans(dataSet, k, distMeas=distEclud):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    :param dataSet:</span><br><span class="line">    :param k:</span><br><span class="line">    :param distMeas:</span><br><span class="line">    :return:</span><br><span class="line">    选择一个初始的簇中心(取均值),加入簇中心列表</span><br><span class="line">    计算每个数据点到簇中心的距离</span><br><span class="line">    当簇的个数小于指定的k时</span><br><span class="line">          对已经存在的每个簇进行2-均值划分,并计算其划分后总的SSE,找到最小的划分簇</span><br><span class="line">          增加一个簇幷更新数据点的簇聚类</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    m = shape(dataSet)[0]</span><br><span class="line">    clusterAssment = mat(zeros((m, 2)))</span><br><span class="line">    # 创建一个初始簇, 取每一维的平均值</span><br><span class="line">    centroid0 = mean(dataSet, axis=0).tolist()[0]</span><br><span class="line">    centList = [centroid0]  # 记录有几个簇</span><br><span class="line">    for j in range(m):</span><br><span class="line">        clusterAssment[j, 1] = distMeas(mat(centroid0), dataSet[j, :]) ** 2</span><br><span class="line">    while len(centList) &lt; k:</span><br><span class="line">        lowestSSE = inf</span><br><span class="line">    #     # 找到对所有簇中单个簇进行2-means可以是所有簇的sse最小的簇</span><br><span class="line">        for i in range(len(centList)):</span><br><span class="line">            # 属于第i簇的数据</span><br><span class="line">            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:, 0].A == i)[0], :]</span><br><span class="line">            # print ptsInCurrCluster</span><br><span class="line">            # 对第i簇进行2-means</span><br><span class="line">            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)</span><br><span class="line">            # 第i簇2-means的sse值</span><br><span class="line">            sseSplit = sum(splitClustAss[:, 1])</span><br><span class="line">            # 不属于第i簇的sse值</span><br><span class="line">            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A != i), 1])</span><br><span class="line">            if (sseSplit + sseNotSplit) &lt; lowestSSE:</span><br><span class="line">                bestCentToSplit = i</span><br><span class="line">                bestNewCents = centroidMat</span><br><span class="line">                bestClustAss = splitClustAss.copy()</span><br><span class="line">                lowestSSE = sseNotSplit + sseSplit</span><br><span class="line">        # 更新簇的分配结果</span><br><span class="line">        #新增的簇编号</span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList)</span><br><span class="line">        #另一个编号改为被分割的簇的编号</span><br><span class="line">        bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit  #</span><br><span class="line">        # 更新被分割的的编号的簇的质心</span><br><span class="line">        centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0]</span><br><span class="line">        # 添加新的簇质心</span><br><span class="line">        centList.append(bestNewCents[1, :].tolist()[0])</span><br><span class="line">        # 更新原来的cluster assment</span><br><span class="line">        clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss</span><br><span class="line">    return mat(centList), clusterAssment</span><br></pre></td></tr></table></figure><p></p><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>给定一个测试文件,看看分类的效果,测试文件的内容如下,每行有两个值,一个是x坐标一个是y坐标,使用上面的算法看看聚类效果如何<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">3.275154	2.957587</span><br><span class="line">-3.344465	2.603513</span><br><span class="line">0.355083	-3.376585</span><br><span class="line">1.852435	3.547351</span><br><span class="line">-2.078973	2.552013</span><br><span class="line">-0.993756	-0.884433</span><br><span class="line">2.682252	4.007573</span><br><span class="line">-3.087776	2.878713</span><br><span class="line">-1.565978	-1.256985</span><br><span class="line">2.441611	0.444826</span><br><span class="line">-0.659487	3.111284</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p></p><p>使用python的图形库matplotlib将测试文件的数据点绘制再二维平面中<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    dataMat = mat(loadDataSet(&apos;testSet2.txt&apos;))</span><br><span class="line">    centroids, clusterAssment = binKMeans(dataMat, 4)</span><br><span class="line">    pl.plot(centroids[:, 0], centroids[:, 1], &apos;ro&apos;)</span><br><span class="line">    pl.plot(dataMat[:, 0], dataMat[:, 1], &apos;bo&apos;)</span><br><span class="line">    pl.show()</span><br></pre></td></tr></table></figure><p></p><p>结果如下:<br><img src="/images/archive/img_figure_2.png" alt><br>图中红色的点是四个簇的质心,可以看出2分k均值算法的聚类效果还是挺好的.</p><p><a href="https://github.com/KDF5000/MLPractice/tree/master/ch10" target="_blank" rel="noopener">源码下载</a></p><div style="display:none"><script src="http://s4.cnzz.com/stat.php?id=undefined&web_id=undefined" language="JavaScript"></script>script></div><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>