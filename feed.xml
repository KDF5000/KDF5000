<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>OpenHex</title>
    <link>http://openhex.cn/</link>
    <description>A place to rock the distributed world!</description>
    <managingEditor>kongdefei@openhex.cn (KDF000)</managingEditor>
    <pubDate>Sat, 05 Dec 2020 16:02:46 +0800</pubDate>
    <item>
      <title>Microsoft Azure Cloud Storage: Giza</title>
      <link>http://openhex.cn/2020/2/17/Microsoft-Azure-Cloud-Storage-Giza.html</link>
      <description>&lt;p&gt;Microsoft Azure Cloud Storage Series:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://webcourse.cs.technion.ac.il/236802/Spring2018/ho/WCFiles/Azure_Cloud_Storage.pdf&#34;&gt;Windows Azure Storage: A Highly Available Cloud Storage Service with Strong&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.cs.princeton.edu/courses/archive/spring13/cos598C/atc12-final181.pdf&#34;&gt;Erasure Coding in Windows&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;http://true/&#34;&gt;Giza: Erasure Coding Objects across Global Data Centers&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h1 id=&#34;overview&#34;&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Giza是构建在Azure Blob Storage之上的一层服务，在此之前Azure BLOB Storage只支持单Datacenter(虽然可以通过异步复制在不同的Stamp之间同步数据，但只能是一个最终一致性的系统)，并且通过LRC的优化在保证可靠性和可用性的前提下将存储成本降到了足够低(1.3)，但是随着业务的发展，对存储的需求要求越来越高，对DC级别的故障容灾的需求越来越强烈，Giza正是为了解决这个问题，为了保证尽量利用现有的存储基础设施，微软选择了在原有存储系统之上构建多DC的存储系统，这就诞生了Giza：&lt;strong&gt;一个强一致的，支持多版本，使用EC编码并且跨全球数据中心的对象存储系统&lt;/strong&gt;。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;当前Azure在全球有38个不同的Region，数EB的数据存储量，为了保护用户的数据在磁盘、机器、机架故障的情况下数据依然完好无损，微软设计LRC[1]在保证数据高可用和高可靠的情况下极大的降低了存储成本。但是随着业务的发展，用户对数据安全性的要求越来越高，希望能够在地震、洪水等可能导致Region或者机房级别故障的情况下，数据依然是可以访问的，因此这就要求Azure需要将数据在其他Region也保存一份，这就导致了存储成功成倍的增加。使用EC在单机房非常有效的将存储成本将了下来，自然而然的如果能够将EC数据分布在多个Region或者Datacenter就能够在保证数据可靠性的情况下将低存储成本。但是将EC的多个分片数据分布在不同的Datacenter就会导致每次请求都要从不同的Datacenter获取数据，导致占用cross-dc的带宽占用并增加请求延迟，而且在Datacenter出现故障恢复后会产生大量的跨DC的数据修复。在Datacenter专线带宽有限且费用昂贵的情况下，这种跨Datacenter的EC方案并非最好的解决方案。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Design Goals&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;最下化请求延迟的前提下保证数据的强一致性&lt;/li&gt;&#xA;&lt;li&gt;尽量充分利用现有的基础设施去实现和部署&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Giza的一个目标之一就是最小化请求延迟，如果做Cross-DC的EC，那么必然会带来跨DC的读写请求，所以如果想要对跨DC的延迟进行优化，最好的方法就是尽量减少跨DC的请求。微软分析了他们内部负载特点，发现确实存在很大的优化空间，比如OneDrive的负载，对象很少更新，并且存在的更新往往都是发生在同一个数据中心，很少出现不同数据中心的更新冲突。同一个数据中心的更新就比较容易了，但是如果出现不同数据中心的更新，就可能导致数据不一致性，那么就需要一种方法去保证多个DC之间数据强一致性。所以要想在这种负载的情况下实现Giza有两个重要的挑战：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对于没有更新冲突的请求在一次跨DC网络下完成&lt;/li&gt;&#xA;&lt;li&gt;确保冲突更新的强一致性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;要想保证强一致性，传统的方案是指定一个主，所有的写入请求relay到主，这样就不存在写入冲突的问题，但是也就也为这所有来自非主DC的请求都会多一次跨DC的网络请求，这与系统的设计目标是相悖的。Giza则使用了FastPaxos和经典Paxos两种一致性算法，在没有冲突的时候使用FastPaxos尽量减少跨DC的请求，当出现更新冲突的情况下则回退到经典Paxos算法，确保数据的强一致。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Giza在现有云存储Azure的基础上实现了一套基于纠删码多版本的、全球强一致的分布式存储系统。当一个对象写入的时候，Giza将该对象EC后的不同Fragment（通常是2+1）分布在不通过的DC，并且将元数据全量同步到不同的DC。当前Giza已经在全球跨三个洲11个数据中心部署并稳定运行。&lt;/p&gt;&#xA;&#xA;&lt;h1 id=&#34;giza-overview&#34;&gt;&lt;strong&gt;Giza overview&lt;/strong&gt;&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;OneDrive Characteristics&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Large Objects Dominate&lt;/li&gt;&#xA;&lt;li&gt;0.9%的存储空间是有小于4MB的对象占用，如果想要降低存储成本，&lt;strong&gt;只用考虑大于等于4MB的对象，小于4MB的对象直接使用多副本的方案&lt;/strong&gt;，这样可以减少小对象进行EC带来的开销（如元数据的存储）&lt;/li&gt;&#xA;&lt;li&gt;对象降温很快&lt;/li&gt;&#xA;&lt;li&gt;大多数据的读都是发生在对象创建的时候&lt;/li&gt;&#xA;&lt;li&gt;47%的读在对象创建一天内发生，87%的读在对象创建一周内发生在，还有不到2%的读发生在对象创建一个月&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;缓存可以减少跨dc请求&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;没有缓存的情况下，读写请求比是2.3，其中跨dc读写比是1.15&lt;/li&gt;&#xA;&lt;li&gt;数据写后缓存到本DC，缓存一天跨dc读写比可以降低到0.61，一周可以降到0.18，一个月降低到0.05&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;并发很少，但是需要多版本&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;57.96%的对象写入后三个月内不会有更新，40.88%会更新一次，1.16%更新多余2次，0.5的对象会出现并发更新&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;删除并不少见，一年前的创建的对象被删除的空间占总空间的26.7%，因此回收被删除的空间很有必要&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Giza tradeoff&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/azure/tradeoffs.png&#34; alt=&#34;Giza Tradeoffs&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Giza采用k+1的方式对数据进行编码，单个DC使用论文[3]描述的LRC方法可以将存储成本降到1.3(12,2,2)，相比Geo-Replication的方法，Giza的k+1 EC在多机房的情况下既降低了存储成本同时也停工了更高的数据可靠性。当然也有存在一些不可避免的成本，Geo-Repl的方案下，每个对象的写也要多一次跨dc的写入，因此写入放大1X，Giza同样需要在本地写入1个分片后，跨DC写入k个数据，写入放大一样是1X。读取的话，Geo-Repl不需要跨DC的请求，在本dc就可以读到完整的数据，但是Giza的方法需要跨机房读取(k-1)个分片，跨DC读取占比(k-1)/k（实际上如果读请求发生在Parity所在机房，那么需要跨dc读取k个数据分片，也就是跨DC需要产生k个流量，当然也可以读取k-1个走EC恢复过程）。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;实际上为了减少跨DC的流量，Facebook F4也提出了一种可行的方案，两个DC以本机房的Volume为单位，进行EC或者XOR，然后将Parity或者XOR结果放到第三个机房，这样能够保证每次读取对象都是在单个机房就可以完成，不过这种方案对删除不是很友好，F4也因此采用了外部DB的方案，记录数据的删除，底层存储空间不进行回收。前面分析过OneDrive的删除比例还是比较高的，因此回收删除数据可以节省很大的成本，因此Giza需要提供支持删除的方案，并且微软认为F4的方案如果想要回收删除空间，需要引入复杂的处理方式，使得系统变得很复杂，这违背了他们的设计原则。&lt;/p&gt;&#xA;&#xA;&lt;h1 id=&#34;设计实现&#34;&gt;&lt;strong&gt;设计实现&lt;/strong&gt;&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/azure/giza_arch.png&#34; alt=&#34;Giza Architecture&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;上图是Giza的整体架构，可以看出Giza充分利用了现有的系统：对象存储和表格存储。这两个系统都是在单DC稳定运行多年，提供高可靠、高可用的服务。Giza将每个对象的写入，采用k+1的方式进行EC编码，每个对象被分成k个分片，经过编码后生成一个Parity分片，分别放在k+1的数据中心，如图中的{a，b,p}。每个对象的写入又分为数据操作和元数据操作，这两个操作是并行完成的，其中元数据包含了每个分片的唯一ID，以及存储位置，在每个dc全量复制一份。k+1个分片，每个dc放置一份，利用单个dc内的容灾能力保证数据的可靠性，也方便单个dc的存储进行单独的优化，Giza只作为一个无状态的服务，可以说这种设计充分利用了当前已有的基础服务，也减少系统内各个模块的耦合度，保证每个模块都可以按照自己的特点进行极致优化。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;技术挑战：&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;利用现有的单DC的Cloud table构建一个强一致的geo-replicated元数据存储&lt;/li&gt;&#xA;&lt;li&gt;联合优化数据访问和元数据访问，实现单次跨DC请求的读写操作&lt;/li&gt;&#xA;&lt;li&gt;高效适时的实现垃圾回收，从而回收删除的对象和元数据&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;强一致的元数据&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;前面提到Giza一个关键的挑战就是实现跨地域的元数据的强一致性，Giza选择了Paxos和FastPaxos一致性算法达到此目的。但是和传统的Paxos实现又有很大的不同，传统的Paxos的Acceptor是一个有状态的进程，可以通过自身的状态决定是否投票，Giza则是基于Azure Table利用其原子条件更新的特性实现Acceptor的逻辑。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Giza的一个很重要的优化目标是减少跨DC的请求，传统的Paxos的流程分为两个阶段：提议和提交，也就是说一次元数据的操作，需要两次跨dc的网络请求。为了减少跨DC的请求，Giza使用FastPaxos [3]将两个过程合并成一次网络请求，但是需要更多的Acceptor同意提议。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;元数据的布局&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/azure/meta_layout.png&#34; alt=&#34;Metadata Layout&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Giza将每个对象的元数据在表格存储中存储一行，比较特殊的是这一行是一个可变列长的一行，其中每个版本的对象占用三列，每个版本包括是哪个记录: 当前看到的最大投票号、当前接受的最大投票号和当前接受的最大值（EC的schema、每个分片ID和所有分片所在的DC）。除此之外，Giza还维护了一个​Known committed versions​ 的集合，记录了当前已经被提交的版本。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;元数据写&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;元数据在写之前会先查询当前DC的​known committed version​表，找出当前DC认为已经提交的最新版本，然后加一作为新的版本号。因为异常情况下，改DC所知道的已经提交的版本可能不是最新的版本，所以在拿着新版本进行提交的时候可能会失败，失败的时候Giza就可以知道当前应该使用的最新版本是多少。按照FastPaxos的流程，Giza会发送一个PreAccept的请求给所有的DC，每个DC收到请求后会进行原则更新操作，如果Giza收到一个Fast Quorum的成功返回后，则认为该元数据的更新成功，此时异步发送请求更新各个DC的​known committed version​表。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;元数据并发写&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Fast Paxos在失败的情况下，可能是因为存在并发写导致不能返回Fast Quorum的成功响应，此时会转入经典的Paxos流程。Giza首先选择一个可以区分的Ballot号，然后发送给各个DC，这个阶段称为Prepare阶段。每个DC接收到Prepare请求后，只有当Prepare中的Ballot号比当前表中的Highest ballot seen大时才返回成功，并且会把整个一行的数据返回。Giza在收到多数成功返回后，会选择一个值进行提交，选择哪个值进行提交的规则如下：&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;在所有响应中选择最大的已经被Accepted的值，如果有的话&lt;/li&gt;&#xA;&lt;li&gt;选择最大的Pre-accepted值，&lt;/li&gt;&#xA;&lt;li&gt;1和2如果存在任何一个就选择这个值使用一个新的版本使用fastpaxos进行提交&lt;/li&gt;&#xA;&lt;li&gt;如果1和2都不存在，那么说明不存在冲突，直接使用当前的metadata进行提交&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;选择一个值进行提交后，每个dc的table只有在当前highest ballot seen和highest accepted ballot比要写入的小的时候才能写入成功。当多数写入成功的时候，Giza返回客户端写入成功，然后异步复制Commit版本到各个dc的table。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;元数据的读&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;元数据的读主要是如何找到改对象已经被提交的版本，因为本地的​known committed version​可能没有包含已经被提交的最大版本，所以Giza需要读取多个dc的元数据的。通常Giza会读取多个DC的对应的元数据行和​known committed version​，当找到多数认可已经提交的版本，并且没有比该版本更高的版本的accpeted value，那么就返回该版本的元数据。但是如果发现存在高于该版本的accepted value，则会走Paxos流程确认这个更高的版本是否已经被提交。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;联合优化数据和元数据的操作&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Giza的读写操作包含两个路径，数据操作和元数据操作。正常的逻辑是先写数据成功后写元数据或者先读元数据然后读数据，完全串行的一个读写流程，并且还是跨DC进行的。为了降低读写延迟，Giza将读写数据和元数据的操作并行进行，这必然会带来一致性的问题，因此需要一些策略来保证读写的正确性。&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;写操作&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;并行去写数据和元数据，Giza等待两者都返回的时候再返回客户端并且提交更新​known committed vesrion​，这个流程可以确保​known committed version​所有的版本的元数据和数据都是提交成功的。但是存在两种异常case：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;数据写入成功，元数据写入失败&lt;/li&gt;&#xA;&lt;li&gt;数据写入成功，元数据写入失败，相当于写入成功的数据成为了孤儿，这个时候对外部来说不会影响读取，因为对客户端来说这个数据是写入失败的，永远不会读取到。但是写入成功的数据需要进行清理，Giza会启动一个清除线程，标记当前版本为​no-op​，意味着孤儿的分片在元数据表里是不存在的，然后从各个数据中心删除孤儿分片&lt;/li&gt;&#xA;&lt;li&gt;元数据写入成功，数据写入失败&lt;/li&gt;&#xA;&lt;li&gt;元数据写入成功，以为着客户端来读取的时候是有可能读取到改版本的元数据，然后尝试去读取数据，但是实际上数据是不存在的，就会导致读取失败，所以在读取的时候需要考虑到这种情况&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;读操作&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Giza首先从本地读取​known committed version​找到该对象最新的版本，然后去读取数据，同时启动一个异步操作去验证改版本确实是最新的版本，如果验证失败那么就说明存在最新的版本，然后拿着最新的版本重新去读取数据。这样确实会可能存在多次读取数据的情况，但是这种情况发生的概率比较低，只有在大量并发的时候才会发生。 针对上面写操作的第二种情况，Giza发现数据读取失败的时候，会马上去查找之前一个​known committed version​的版本读取数据。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;删除和GC&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Giza的删除操作会作为一个更新操作处理，针对特定版本或者整个对象的删除，会修改元数据或者增加一个新的版本标识该对象被删除，这个过程 只涉及到元数据的更新，一旦更新完成便返回客户端删除成功。另外会有个GC服务回收删除对象的空间，回收的过程分为三步：&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;从元数据表读取需要回收的对象&lt;/li&gt;&#xA;&lt;li&gt;从对象存储系统删除对应的分片&lt;/li&gt;&#xA;&lt;li&gt;从元数据行内删除对应的版本（特定列）&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;如果是整个对象的删除，则需要从元数据表中删除整行数据。因为是Giza是允许多点写入的，所以删除正常数据存在冲突的风险，比如一个DC对应的对象已经删除了，但是其他DC还没有删除，这个时候如果有个put请求，那么该DC会认为不存在这个对象，会生成一个最小版本的元数据进行写入，这个时候就会和其他没有删除元数据的DC的数据存在不一致。所以针对删除整个元数据的情况，Giza采用两阶段提交的方法去保证，首先标记所有DC要删除的元数据行为​confined​状态，不接受任何的读写请求，然后第二阶段进行删除。如果存在DC不可用，那么这个操作是不能成功，只有故障的DC恢复后才能继续进行。&lt;/p&gt;&#xA;&#xA;&lt;h1 id=&#34;故障恢复&#34;&gt;&lt;strong&gt;故障恢复&lt;/strong&gt;&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Giza是构建在Azure Blob storage和Azure Table Storage之上的，所以只用关心DC级别的容灾，单个DC的数据容灾交给Blob和Table服务，这让容灾变得会简单许多。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;短暂的DC故障&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;对于短暂的DC界别故障，Giza运行降级的读写，会发生多余一次的跨DC网络请求。不过论文里主要讨论的都是元数据的读写，元数据在多个DC之间是多副本的，但是数据分片的读写并没有太多介绍，所以这里多个EC分片的多数写入读取成功可能也是走的FastPaxos或者Paxos的方法，支持多数写入成功。等故障DC故障恢复后，利用Paxos Learning的机制，将缺失的元数据和数据分片补回来，当然对于EC分片会产生EC重建，计算出缺失的分片数据。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;永久性故障&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;极端情况下，DC可能存在永久性故障，意味着这个DC的数据直接放弃，以及这个DC不在接受任何的读写请求，Giza采用逻辑DC的概念，用户选择的DC只是一个逻辑上的DC，使用一个额外的映射表记录逻辑DC和物理DC的关系，当一个物理DC永久性故障的时候，只需要改变映射指向新的物理DC，然后启动恢复任务从其他DC补全缺失的元数据和数据分片即可。&lt;/p&gt;&#xA;&#xA;&lt;h1 id=&#34;性能评估&#34;&gt;&lt;strong&gt;性能评估&lt;/strong&gt;&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/azure/performance_cmp.png&#34; alt=&#34;Performance Comparision&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;上表是Giza的配置，对比的对象是CockroachDB，因为CockroachDB不支持全球范围的复制，所以只部署了US-2-1的情况进行对比，并且每个DC部署三个实例。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Giza机器配置：16c，56G，1Gbps的虚拟机&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Metadata延迟：Fast Paxos和Classic Paxos对比&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/azure/meta_latency.png&#34; alt=&#34;metadata latency&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Giza延迟&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/azure/overall_latency.png&#34; alt=&#34;Overall Latency&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;对象大小为4MB的情况下，对于put操作，经典的Paxos和FastPaxos的中位数延迟分别是852ms和598ms，Giza的延迟是374ms左右，只比只传输数据的延迟多了30ms。get操作，Giza的延迟只有223ms。&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;和CockroachDB的对比&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;在CockroachDB上实现Giza，因为CockroachDB不支持大对象，所以只测试了128KB的情况，put延迟的中位数CockroachDB有333ms，Giza不到100ms。get操作，cockroachDB延迟比Giza低了20%，因为CockroachDB是读本地盘，但是Giza是读Azure，所以要快一些。&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;并发更新&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/azure/contention_update.png&#34; alt=&#34;Concurrent Update&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;参考文献&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.usenix.org/sites/default/files/conference/protected-files/atc17_slides_chen.pdf&#34;&gt;Giza: Erasure Coding Objects across Global Data Centers&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2005-112.pdf&#34;&gt;Fast Paxos&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.cs.princeton.edu/courses/archive/spring13/cos598C/atc12-final181.pdf&#34;&gt;Erasure Coding in Windows&lt;/a&gt; &lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://webcourse.cs.technion.ac.il/236802/Spring2018/ho/WCFiles/Azure_Cloud_Storage.pdf&#34;&gt;Windows Azure Storage: A Highly Available Cloud Storage Service with Strong&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</description>
      <pubDate>Mon, 17 Feb 2020 23:45:14 +0000</pubDate>
    </item>
    <item>
      <title>再见,2017</title>
      <link>http://openhex.cn/2018/1/20/再见-2017.html</link>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;如果提前了解了你所要面对的人生，你是否还会有勇气前来？&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;2017年的帝都似乎很少看到雾霾，不知道是因为空气确实变好了，还是由于这一年过于疲惫，极少抬头的缘故。2017，对我来说无疑是最艰难的一年，找工作，毕业论文，更多的是对自我的思索，一切似乎都陷入了一种无解的状态。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;正像无问西东里开场的独白“如果提前了解了你所要面对的人生，你是否还会有勇气前来？”，一年多前意识到来到这里并不是自己想象中的那样，一度想过休学，但是最后鼓起了勇气继续面对这一切，以为自己可以不被现实羁绊，走出不一样的道路。现在回头看，也许用力过猛，如此地天真，终究还是太世俗，以致于自己都感觉自己是一个不折不扣的loser. 不过正是鼓起勇气面对了这一切，对自我的认识也更加的深刻。通过近一年的经历和思索，对自己的缺点认识更加的清楚了：&#xA;- 过于纠结。归根到底还是没有想清楚自己到底在追求什么？面对一个一个的选择的时候，总是过早的去想选择这个会怎么样，选择那个会怎么样，是不是要放弃什么。其实并没有那么多怎么样，选一个喜欢的就行了，不喜欢就换呗！这点胡女神说的很对，人活着也就是为了多巴胺而已，开心就好。也渐渐的明白了自己的“兴奋点”，只要做的事情有挑战，可以带来成就感即可！&#xA;- 过于理想化，太过追求完美。《无问西东》里说的很有道理，“这个世界从不缺完美的人，缺的是从心底给出的真心、无畏、正义和同情”，同样道理，做事追求极致固然很好，但是很容易陷入死胡同，有时候也许应该把精力适当的放在其他地方，用心去感受，任何一件艺术品都是有瑕疵的，合适的或许实用的才是最好的。&#xA;- 情绪控制待加强(情商太低？？)。容易陷入情绪旋涡，对不喜欢的事情过多的情绪化表现，对“朋友”(也许只是单方面的朋友)过于在乎，因为个别小事就会导致情绪波动。总之，要更加关注自我，是朋友就用心对待即可，不要计较过多得失。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;不管怎么样，2017已经过去，也许再走一遍，还是会走同样的路，虽然技术上没有长进，但是依然收获颇丰。希望2018更加开心，也能遇到更多良师益友！&lt;/p&gt;&#xA;&#xA;&lt;p&gt;作为总结似乎少了点什么，翻一下去年的总结，打打脸。&lt;/p&gt;&#xA;&#xA;&lt;table&gt;&#xA;&lt;thead&gt;&#xA;&lt;tr&gt;&#xA;&lt;th align=&#34;left&#34;&gt;计划&lt;/th&gt;&#xA;&lt;th align=&#34;left&#34;&gt;现实&lt;/th&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/thead&gt;&#xA;&#xA;&lt;tbody&gt;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;left&#34;&gt;基于大数据技术的安全防御系统&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;夭折&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;left&#34;&gt;MIT6.824 Spring 2016&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;实验完成，论文大部分没看&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&#xA;&lt;p&gt;现实总是骨感的，不过2018还是要计划一下的，方便明年验证，列个表&lt;/p&gt;&#xA;&#xA;&lt;table&gt;&#xA;&lt;thead&gt;&#xA;&lt;tr&gt;&#xA;&lt;th&gt;打脸计划&lt;/th&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/thead&gt;&#xA;&#xA;&lt;tbody&gt;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;每个月至少读一本书(上帝掷骰子吗?,别逗了，费曼先生,巨婴国,菊与刀)&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;每天坚持背单词&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;关注SIGMOD, VLDB, FAST, ATC, ICDE等相关会议paper&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;每周坚持打羽毛球&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;每天坚持看科技新闻&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;掌握一个领域的新技术(暂定数据库，根据入职工作调整)&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;关注研究一两个开源项目最好能有所贡献(如TiDB, etcd)&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;</description>
      <pubDate>Sat, 20 Jan 2018 21:33:14 +0000</pubDate>
    </item>
    <item>
      <title>分布式存储系统对比</title>
      <link>http://openhex.cn/2017/11/14/分布式存储系统对比.html</link>
      <description>&lt;h3 id=&#34;ceph&#34;&gt;ceph&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Ceph是一个统一的分布式存储系统，提供了对象存储，块存储和文件存储三种模式，在架构上没有中心节点，提供了无限的扩展性。&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Ceph is a unified, distributed storage system designed for excellent performance, reliability and scalability&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Ceph核心是使用CRUSH算法通过计算获取数据存储的位置，无需中心节点，提供了高度的可靠性和可扩展性。&#xA;&lt;img src=&#34;http://docs.ceph.com/docs/master/_images/stack.png&#34; alt=&#34;&#34; /&gt;&#xA;ceph的的块存储，文件存储都是基于其实现的对象存储系统RADOS。下面是RADOS的系统架构&#xA;&lt;img src=&#34;/media/archive/blog/images/876E9FA342C9C0341021EDC0E7611B22.jpg?imageView2/0/w/600&#34; alt=&#34;1510306287044.jpg&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;OSD是存储对象的设备，Monotors提供整个OSDs布局的监控，也就是记录clusterMap, 然后client使用crush算法对输入的对象id,和monotors提供的clustermap映射到osd组，然后直接对对象进行存取。整个映射过程如下图:&#xA;&lt;img src=&#34;/media/archive/blog/images/DEA1B39C1679E4428E577CD8555A0AD4.jpg?imageView2/0/w/600&#34; alt=&#34;1510306450158.jpg&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;参考论文：&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.ssrc.ucsc.edu/Papers/weil-osdi06.pdf&#34;&gt;Ceph: A Scalable, High-Performance Distributed File System&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ceph.com/wp-content/uploads/2016/08/weil-rados-pdsw07.pdf&#34;&gt;RADOS: A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://ceph.com/wp-content/uploads/2016/08/weil-crush-sc06.pdf&#34;&gt;CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h3 id=&#34;swift&#34;&gt;swift&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;swift是openstack项目的一个开源的分布式对象存储系统，使用一致性哈希算法确保真个系统没有中心节点，并且使用Quorum协议保证系统的最终一致性。下面是整个系统的架构：&#xA;&lt;img src=&#34;/media/archive/blog/images/AC2D020F9C2A8DF9C6866217D265E57D.png?imageView2/0/w/600&#34; alt=&#34;image005.png&#34; /&gt;&#xA;最核心的就是如何对一个对象进行寻址，swift抽象除了虚拟节点的概念，先讲对象使用一致性哈希算法映射到虚拟节点，然后利用虚拟节点到物理设备的映射表找到实际的存储设备。&#xA;&lt;img src=&#34;/media/archive/blog/images/B222FF85DEDC4427DE45D2390B9BA1A8.png?imageView2/0/w/600&#34; alt=&#34;image003.png&#34; /&gt;&#xA;上图中的分区P0-p7就是一致性哈希算法中的虚拟节点&#xA;&lt;img src=&#34;/media/archive/blog/images/C5C82FF2AC2BD1C6E5087E44066B7E75.png?imageView2/0/w/600&#34; alt=&#34;image001.png&#34; /&gt;&#xA;以逆时针方向递增的散列空间有 4 个字节长共 32 位，整数范围是[0~232-1]；将散列结果右移 m 位，可产生 232-m个虚拟节点，例如 m=29 时可产生 8 个虚拟节点。在实际部署的时候需要经过仔细计算得到合适的虚拟节点数，以达到存储空间和工作负载之间的平衡&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;glusterfs&#34;&gt;GlusterFS&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;GlusterFS(GNUF ClusterFile System)是一个开源的分布式文件系统，使用分布式哈希(DHT)技术去中心化存储，采用模块化的扩展架构，支持IP/RDMA等传输协议，提供Block, File, Object统一存储，下面是整体架构：&#xA;&lt;img src=&#34;/media/archive/blog/images/D8673B563AC1F588B76B2AAD28F3999E.jpg?imageView2/0/w/600&#34; alt=&#34;WechatIMG248.jpeg&#34; /&gt;&#xA;参考: &lt;a href=&#34;http://www.chinacloud.cn/upload/2015-01/15011009251270.pdf&#34;&gt;GlusterFS分布式文件系统&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;fastdfs-tfs&#34;&gt;fastdfs/tfs&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;fastdfs是一个国人开源的分布式文件系统，解决了大容量存储和负载均衡的问题，结构如下:&#xA;&lt;img src=&#34;http://static.oschina.net/uploads/img/201204/20230218_pNXn.jpg&#34; alt=&#34;&#34; /&gt;&#xA;tracker负责系统的调度工作，访问上起到负载均衡的作用，storage节点会定期发送心跳给每个tracker节点，告诉其所属group等信息，每个tracker是对等的，因此可以无限扩展，但是因为每个storage节点要发送心跳消息给所有tracker，所以也不能扩展很多。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;每个storage节点在初始化是，会生成一个二级目录，每一级含有256个目录，也就是一共65525个文件目，在存储文件时使用round robin(轮询)、load balance(选择最大剩余空 间的组上传文件)、specify group(指定group上传)中的一种方式存储文件。每一个group的数据间异步进行复制(tfs是同步进行复制)。对于小文件的存储没有做特殊的优化处理。&#xA;参考: &lt;a src=&#34;/media/archive/blog/images/FastDFS.pdf&#34;&gt;FastDFS 分布式存储实战&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;hdfs&#34;&gt;HDFS&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;HDFS是专门为GB级别的数据存储优化的存储系统，namenode负责元数据的存储，datanode负责实际文件的存储，大文件分成64m的chunck进行存储，使用chain replication的方式对数据进行复制。&#xA;&lt;img src=&#34;/media/archive/hdfs_arch.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;参考：&lt;a href=&#34;http://blog.csdn.net/suifeng3051/article/details/48548341&#34;&gt;http://blog.csdn.net/suifeng3051/article/details/48548341&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;haystack&#34;&gt;Haystack&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;WebServer + Haystack Directory + CDN + Cache(Internal CDN) + Haystack Store&lt;/li&gt;&#xA;&lt;li&gt;多级缓存，映射关系集中管理&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;小文件合并，内存索引，多副本&#xA;&lt;img src=&#34;/media/archive/blog/images/22537BD9E2E878215589256DE92F4DF7.jpg?imageView2/0/w/600&#34; alt=&#34;haystack_arch.jpeg&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/blog/images/8BCBA869054800DD24B9D624136E0051.png?imageView2/0/w/600&#34; alt=&#34;图片 1.png&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;ambry&#34;&gt;Ambry&lt;/h3&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;系统架构和haystack很像，核心思想也是小文件合并&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;各种大小文件，大文件切分&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;数据负载均衡&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;文件索引分段，冷数据持久化，布隆过滤快速定位&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;跨多个数据中心&#xA;&lt;img src=&#34;/media/archive/blog/images/64943ED7F96443B5505E254ABC345E7B.png?imageView2/0/w/600&#34; alt=&#34;图片 1.png&#34; /&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
      <pubDate>Tue, 14 Nov 2017 22:56:55 +0000</pubDate>
    </item>
    <item>
      <title>Go Channels</title>
      <link>http://openhex.cn/2017/7/16/Go-Channels.html</link>
      <description>&lt;p&gt;Golang使用Groutine和channels实现了CSP(Communicating Sequential Processes)模型，channles在goroutine的通信和同步中承担着重要的角色。在GopherCon 2017中，Golang专家Kavya深入介绍了 Go Channels 的内部机制，以及运行时调度器和内存管理系统是如何支持Channel的，本文根据Kavya的ppt学习和分析一下go channels的原理，希望能够对以后正确高效使用golang的并发带来一些启发。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;以一个简单的channel应用开始，使用goroutine和channel实现一个任务队列，并行处理多个任务。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main(){&#xA;    //带缓冲的channel&#xA;    ch := make(chan Task, 3)&#xA;    &#xA;    //启动固定数量的worker&#xA;    for i := 0; i&amp;lt; numWorkers; i++ {&#xA;        go worker(ch)&#xA;    }&#xA;    &#xA;    //发送任务给worker&#xA;    hellaTasks := getTaks()&#xA;    &#xA;    for _, task := range hellaTasks {&#xA;        ch &amp;lt;- task&#xA;    }&#xA;    &#xA;    ...&#xA;}&#xA;&#xA;func worker(ch chan Task){&#xA;    for {&#xA;       //接受任务&#xA;       task := &amp;lt;- ch&#xA;       process(task)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;从上面的代码可以看出，使用golang的goroutine和channel可以很容易的实现一个生产者-消费者模式的任务队列，相比java, c++简洁了很多。channel可以天然的实现了下面四个特性：&#xA;- goroutine安全&#xA;- 在不同的goroutine之间存储和传输值&#xA;- 提供FIFO语义(buffered channel提供)&#xA;- 可以让goroutine block/unblock&lt;/p&gt;&#xA;&#xA;&lt;p&gt;那么channel是怎么实现这些特性的呢？下面我们看看当我们调用make来生成一个channel的时候都做了些什么。&lt;/p&gt;&#xA;&#xA;&lt;h2 id=&#34;make-chan&#34;&gt;make chan&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;上述任务队列的例子第三行，使用make创建了一个长度为3的带缓冲的channel，channel在底层是一个hchan结构体，位于&lt;code&gt;src/runtime/chan.go&lt;/code&gt;里。其定义如下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type hchan struct {&#xA;    qcount   uint           // total data in the queue&#xA;    dataqsiz uint           // size of the circular queue&#xA;    buf      unsafe.Pointer // points to an array of dataqsiz elements&#xA;    elemsize uint16&#xA;    closed   uint32&#xA;    elemtype *_type // element type&#xA;    sendx    uint   // send index&#xA;    recvx    uint   // receive index&#xA;    recvq    waitq  // list of recv waiters&#xA;    sendq    waitq  // list of send waiters&#xA;&#xA;    // lock protects all fields in hchan, as well as several&#xA;    // fields in sudogs blocked on this channel.&#xA;    //&#xA;    // Do not change another G&#39;s status while holding this lock&#xA;    // (in particular, do not ready a G), as this can deadlock&#xA;    // with stack shrinking.&#xA;    lock mutex&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;make函数在创建channel的时候会在该进程的heap区申请一块内存，创建一个hchan结构体，返回执行该内存的指针，所以获取的的ch变量本身就是一个指针，在函数之间传递的时候是同一个channel。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;hchan结构体使&lt;strong&gt;用一个环形队列&lt;/strong&gt;来保存groutine之间传递的数据(如果是缓存channel的话)，使用&lt;strong&gt;两个list&lt;/strong&gt;保存像该chan发送和从改chan接收数据的goroutine，还有一个mutex来保证操作这些结构的安全。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;发送和接收&#34;&gt;发送和接收&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;向channel发送和从channel接收数据主要涉及hchan里的四个成员变量，借用Kavya ppt里的图示，来分析发送和接收的过程。&#xA;&lt;img src=&#34;/media/archive/blog/images/hchan_white_gif.gif?imageView/0/w/600/&#34; alt=&#34;hchan white &#34; /&gt;&#xA;还是以前面的任务队列为例:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;//G1&#xA;func main(){&#xA;    ...&#xA;    &#xA;    for _, task := range hellaTasks {&#xA;        ch &amp;lt;- task    //sender&#xA;    }&#xA;    &#xA;    ...&#xA;}&#xA;&#xA;//G2&#xA;func worker(ch chan Task){&#xA;    for {&#xA;       //接受任务&#xA;       task := &amp;lt;- ch  //recevier&#xA;       process(task)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;其中G1是发送者，G2是接收，因为ch是长度为3的带缓冲channel，初始的时候hchan结构体的buf为空，sendx和recvx都为0，当G1向ch里发送数据的时候，会首先对buf加锁，然后将要发送的&lt;strong&gt;数据copy到buf里&lt;/strong&gt;，并增加sendx的值，最后释放buf的锁。然后G2消费的时候首先对buf加锁，然后将buf里的&lt;strong&gt;数据copy到task变量对应的内存里&lt;/strong&gt;，增加recvx，最后释放锁。整个过程，G1和G2没有共享的内存，底层通过hchan结构体的buf，使用copy内存的方式进行通信，最后达到了共享内存的目的，这完全符合CSP的设计理念&#xA;&amp;gt; Do not comminute by sharing memory;instead, share memory by communicating&lt;/p&gt;&#xA;&#xA;&lt;p&gt;一般情况下，G2的消费速度应该是慢于G1的，所以buf的数据会越来越多，这个时候G1再向ch里发送数据，这个时候G1就会阻塞，那么阻塞到底是发生了什么呢？&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;goroutine-pause-resume&#34;&gt;Goroutine Pause/Resume&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;goroutine是Golang实现的用户空间的轻量级的线程，有runtime调度器调度，与操作系统的thread有多对一的关系，相关的数据结构如下图:&#xA;&lt;img src=&#34;/media/archive/blog/images/schedule.png?imageView/0/w/600/&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;其中M是操作系统的线程，G是用户启动的goroutine，P是与调度相关的context，每个M都拥有一个P，P维护了一个能够运行的goutine队列，用于该线程执行。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;当G1向buf已经满了的ch发送数据的时候，当runtine检测到对应的hchan的buf已经满了，会通知调度器，调度器会将G1的状态设置为waiting, 移除与线程M的联系，然后从P的runqueue中选择一个goroutine在线程M中执行，此时G1就是阻塞状态，但是不是操作系统的线程阻塞，所以这个时候只用消耗少量的资源。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;那么G1设置为waiting状态后去哪了？怎们去resume呢？我们再回到hchan结构体，注意到hchan有个sendq的成员，其类型是waitq，查看源码如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type hchan struct {&#xA;    ...&#xA;    recvq    waitq  // list of recv waiters&#xA;    sendq    waitq  // list of send waiters&#xA;    ...&#xA;}&#xA;//&#xA;type waitq struct {&#xA;    first *sudog&#xA;    last  *sudog&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;实际上，当G1变为waiting状态后，会创建一个代表自己的sudog的结构，然后放到sendq这个list中，sudog结构中保存了channel相关的变量的指针(如果该Goroutine是sender，那么保存的是待发送数据的变量的地址，如果是receiver则为接收数据的变量的地址，之所以是地址，前面我们提到在传输数据的时候使用的是copy的方式)&#xA; &lt;img src=&#34;/media/archive/blog/images/sendq.png?imageView/0/w/600/&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;当G2从ch中接收一个数据时，会通知调度器，设置G1的状态为runnable，然后将加入P的runqueue里，等待线程执行.&#xA; &lt;img src=&#34;/media/archive/blog/images/G12Runnable.png?imageView/0/w/600/&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;### wait empty channel&#xA; 前面我们是假设G1先运行，如果G2先运行会怎么样呢？如果G2先运行，那么G2会从一个empty的channel里取数据，这个时候G2就会阻塞，和前面介绍的G1阻塞一样，G2也会创建一个sudog结构体，保存接收数据的变量的地址，但是该sudog结构体是放到了recvq列表里，当G1向ch发送数据的时候，&lt;strong&gt;runtime并没有对hchan结构体题的buf进行加锁，而是直接将G1里的发送到ch的数据copy到了G2 sudog里对应的elem指向的内存地址！&lt;/strong&gt;&#xA; &lt;img src=&#34;/media/archive/blog/images/G2wait.png?imageView/0/w/600/&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;## 总结&#xA;Golang的一大特色就是其简单搞笑的天然并发机制，使用goroutine和channel实现了CSP模型。理解channel的底层运行机制对灵活运用golang开发并发程序有很大的帮助，看了Kavya的分享，然后结合golang runtime相关的源码(源码开源并且也是golang实现简直良心！),对channel的认识更加的深刻，当然还有一些地方存在一些疑问，比如goroutine的调度实现相关的，还是要潜心膜拜大神们的源码！&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;h3 id=&#34;参考资料&#34;&gt;参考资料&lt;/h3&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://speakerdeck.com/kavya719/understanding-channels&#34;&gt;https://speakerdeck.com/kavya719/understanding-channels&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://about.sourcegraph.com/go/understanding-channels-kavya-joshi&#34;&gt;https://about.sourcegraph.com/go/understanding-channels-kavya-joshi&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/golang/go/blob/master/src/runtime/chan.go&#34;&gt;https://github.com/golang/go/blob/master/src/runtime/chan.go&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/golang/go/blob/master/src/runtime/runtime2.go&#34;&gt;https://github.com/golang/go/blob/master/src/runtime/runtime2.go&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</description>
      <pubDate>Sun, 16 Jul 2017 18:51:47 +0000</pubDate>
    </item>
    <item>
      <title>Raft论文重读</title>
      <link>http://openhex.cn/2017/7/13/Raft论文重读.html</link>
      <description>&lt;p&gt;Raft是一个管理日志副本一致性的算法。相比Paxos结果一样，并且一样高效，但是理解起来更加的容易。Raft将一致性的主要元素分离开来，比如leader选举，log 复制，安全等。同时，也提供了一个新的机制实现cluster membership改变，其使用多数的原则来保证安全性。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;一致性算法&#34;&gt;一致性算法&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;一致性算法的意义就是保证一致性的一组机器能够在其部分成员出现故障的时候依然能够存活下来(提供服务)。在Raft之前所有的一致性算法可以认为都是Paxos或者其变种的实现，但是Paxos难以理解，其工程实现往往都需要改变Paxos的架构。因此为了便于理解，以及工程上的实现，开发了Raft算法，该算法使用一些技术从而使其在保证正确性和性能的前提下，更加的容易理解和实现，这些技术主要有过程分解(领导选举，日志复制，安全)和状态空间减少。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;相比其他一致性算法，Raft有几个明显的特点:&#xA;- strong leader: Raft使用很强的leadership，比所有的Log entries都是通过leader发送到所有其他的服务器。这样简化了管理也更加容易理解。&#xA;- leader election：采用随机化的定时器去选举leader，简单快速的解决了leader选举中的冲突。&#xA;- 成员变更: 使用一个joint consensus机制，保证在变更成员的时候依然能够正常的操作。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;replicated-state-machines&#34;&gt;Replicated state machines&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;(Replicated State Machine)状态机&lt;/strong&gt;：一致性group的节点的某个时刻的状态(比如数据库里x=1,y=1是一个状态)转移可以看成自动机里的一个状态，所以叫状态机。&#xA;&lt;strong&gt;Replicated Log&lt;/strong&gt;: 包含了来自客户端的用于执行在状态机上的命令(操作)。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;replicate group里的每一个节点都包含一个&lt;strong&gt;相同序列的&lt;/strong&gt;log,也就是相同序列的操作，因此每一个节点的state machine都是执行相同序列的操作，所以其结果也是相同。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;一致性算法的主要目的就是保证每个节点上log的一致，其核心功能就是接收客户端的命令，并添加到log里，同时一致性模块与其他节点通信，保证他们的log都是&lt;strong&gt;相同顺序的序列&lt;/strong&gt;，即使一些节点出现了故障，当log成功的复制到一定数量(通常是多数)的节点后，每个节点的状态机就执行该条命令并将结果返回给客户端。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;总的来说一致性算法有下面一下特性:&#xA;- 安全。这里是指即使再机器出现故障(比如网络延迟，隔离，丢包，重复，重排等情况)的情况下绝不会返回错误的结果&#xA;- 可用性。在多数机器可用的情况下，整个集群依然总是可以对外服务。出现故障的机器后续依然可以通过持久化的数据恢复并且重新加入集群&#xA;- 不依赖时钟。一致性算法不依赖时钟就能保证log的一致性&#xA;- Single-round RPC. 通常情况下，一个命令只要集群中多数机器响应，就认为是成功了。即使一小部分机器出现了故障也不会影响整个系统的性能，而这个过程通常只需要一轮的RPC调用。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;paxos的缺陷&#34;&gt;Paxos的缺陷&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Paxos几乎成为了分布式一致性算法的代名词，基本成为了一致性算法的教学典范，以及工业实现的参考。Paxos分为单命令的(single decree)*single-decree Paxos*和多命令的&lt;em&gt;multi-Paxos&lt;/em&gt;. Paxos的正确性和性能都已经得到理论上的证明。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;但是Paxos有两个主要的缺陷:&#xA;- 难以理解&#xA;- 工程实现困难&lt;/p&gt;&#xA;&#xA;&lt;p&gt;而且Paxos的架构实际系统的实现中并不实用，比如其收集日志的一个集合然后将其重排成有序的日志的做法并没有任何好处，相反直接append到一个有序的log更加的高效。还有其实用同等地位的点对点，没有leader的方式，对于一个确定一个决策没有问题，但是对于一些序列决策就存在一定的问题，虽然其最后建议使用一个弱的leader，但是还是显得很复杂。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;raft如何做到可理解性&#34;&gt;Raft如何做到可理解性？&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;为了是一致性算法更加容易理解，Raft主要使用了一下两个技术:&#xA;- 问题分解(Decomposition)&#xA;将一致性问题分解成几个容易理解和实现的子问题，Raft讲算法分为：领导选举(Leader Election), 日志复制(Log Replication)，安全(Safety)和成员变更(Membership Change)四个部分&#xA;- 减少状态空间&#xA;尽可能的减少需要考虑的状态和不确定性，让系统看起来更加的清晰。和Paxos一个很不同的地方就是Raft不允许log存储漏洞(Holes)，并且限制了不同节点之间log不一致的情况。通过减少状态空间和一些限制，大大地增加了算法的可理解性。除此之外，Raft使用随机的方式简化Raft的领导选举，这样虽然增加了算法的不确定性，但是随机化通过对于所有的状态都是用同样的处理方式可以简化状态空间。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;raft一致性算法&#34;&gt;Raft一致性算法&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Raft算法的一个核心思路是在一组机器中间选出一个Leader, 然后又Leader去管理日志，通过选举一个leader大大地简化了管理的复杂度，客户端需要发送新的命令，通过leader将log复制到所有其他节点，当大多数节点都复制了log后，leader通知所有节点，可以将日志包含的命令应用在本地的状态机。基于leader机制，Raft讲算法分为三个部分:&#xA;- leader选举&#xA;当一个集群中没有leadr或者存在的leader出现故障时重新选举一个leader&#xA;- 日志复制&#xA;当前的leader接受客户端发送的日志(命令)，然后复制到集群的其他节点。&#xA;- 安全(Safety)&#xA;安全主要指状态机的安全属性。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;raft基本概念&#34;&gt;Raft基本概念&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Role:&lt;/strong&gt; 在一个Raft集群中(通常五个节点，容忍两个节点故障), 每个节点或者说(Raft实例)都有三种角色，Follwer, Candidate, Leader，每个实例初始化都是Follower状态，设置一个定时器，当一定时间内没有发现leader，则会发起leader选举的请求，其状态变为candidate，当其他节点投票满足一定的条件后成为Leader, 其他所有的节点都转变或者维持Follower状态，具体的状态装换图如下:&#xA;&lt;img src=&#34;/media/archive/blog/images/state_transition.png?imageView/0/w/500/&#34; alt=&#34;state transition.png&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Trem:&lt;/strong&gt; Raft算法将时间分成不同长度的term，每个一个term都是从选举一个leader开始，换句话说就是质只要出现新的leader选举那么就是进入了新的term，一旦选出leader后，该term后续的所有时间都有该leader负责与客户端交互，管理日志的复制。Term更像是一个逻辑时钟，每个server都会维护一个currentTerm，这么在server通信的过程中便能识别出每个server的term是高于还是低于自己的term，从而进行状态的转换和term的更新。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;leader选举&#34;&gt;Leader选举&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Raft使用心跳机制触发leader选举，所有服务器启动的时候都是follower，规定在一定时间间隔内如果能够收到leader的心跳或者candidate的投票消息，则一直保持follower，否则触发leader选举过程。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;leader选举有两个阶段：首先增加currentTerm，转为candidate状态，然后&lt;strong&gt;并行&lt;/strong&gt;的向其他服务器发送投票RPC(RequestVote)。candidate状态结束的条件有如下三个:&#xA;- &lt;strong&gt;赢得了选举，成为leader&lt;/strong&gt;&#xA;如果candidate状态的节点收到了多数节点的投票那么就会赢的选举，成为leader, 然后向其他节点发送心跳，告诉他们自己是leader。每一个节点在投票的时候，至多投一次，并且按照先到先得的原则，同时请求投票的节点的term必须大于等于自身的term。&lt;strong&gt;多数的规则&lt;/strong&gt;能够保证再一个选举周期保证最多一个节点赢得选举成为leader。&#xA;- &lt;strong&gt;收到了自称leader的节点的心跳&lt;/strong&gt;&#xA;如果candidate状态的节点在等待投票的过程中收到了某个节点的心跳自称自己是leader，如果心跳里包含的term大于等于自己的currentTerm，那么就说明该leader是合法的，自己转为follower，否则拒绝RPC,并返回自己的term.&#xA;- &lt;strong&gt;既没有赢得选举，也没有失败(没有其他leader产生)&lt;/strong&gt;&#xA;如果同一个时刻有多个candidate状态的机器，那么就会产生votes split，这种情况就不会满足多数的规则，所以不会产生leader，这个时候每个candidate增加当前term，重置election timeout，开始新一轮的选举。这个时候为了防止每个candidate的election timeout相同导致无休止的选举失败，Raft采用了一个简单但是非常有效的方法，&lt;strong&gt;随机生成&lt;/strong&gt;election timeout，通常是一个范围比如150ms~300ms。这个&lt;strong&gt;随机化&lt;/strong&gt;也是Raft的一个重要的技术点，很多地方都用到了随机化从而简化了系统同时，也保证了正确性。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;为了防止不包含之前leader已经commit的log entry，在投票的时候有一个限制，candidate在发送RequestVote RPC的时候会附带其最后一个log的index和term，只有candidate的term大于等于(等于时候比较index)自身的term才投票给candidate，否则拒绝投票。这样能够保证按照这种规则投票 选出的leader包含了所有之前leader已经committed的log entry。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;log-replication&#34;&gt;Log Replication&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;当一个raft实例当选为leader后，该实例就开始负责与客户端交互，并将客户端的请求作为一个新的log entry保存到本地logs内，然后并行的发给其他的follower，如果收到多数follower保存该entry成功的相应，那么就commit该log entry，并apply到本地的state machine, 其他follower也会在&lt;del&gt;收到commit的请求后或者在&lt;/del&gt;后续的一致性检查的时候apply改entry到本地的state machine，最终实现所有实例的一致性。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;正常的情况下，按照上述的流程是不会出错的。但是往往follower, candidate, leader都有可能出现宕机或者其他故障导致log的不一致，如下图所示，leader和follower都有可能缺少log或者有多余的Log.&#xA;&lt;img src=&#34;/media/archive/blog/images/Log_inconsistent.png?imageView/0/w/500/&#34; alt=&#34;Log inconsistent.png&#34; /&gt;&#xA;最上面的log是当前的leader，a和b两个follower的log属于丢失log，c，d, e和f都含有没有提交的log，f相对于现在的leader少了index 4之后所有的log entry，但是多了term 2和term3的log, 这种情况可能就是由于f在term2的时候是leader，生成了三个log，但是在没有提交的的时候就挂了，迅速重启后又称为了term 3的leader,同样生成了5个log entry，但是也是没提交就挂了直到term 8的时候才启动起来。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;由于leader选举时候的限制，当前的leader包含了之前leader已经commit的所有log, 对于follower缺失log的情况，leader在当选为leader的时候初始化每个follower已经匹配到的index为其log的长度，也就是最有一个log entry的下一个index，然后在appendEntries的时候，在follower查看其log对应的该index时候和leader对应的index对应的log相同，如果相同则删除其后所有的log，强制用leader的log覆盖，否则index减一向前查找，知道找到第一个匹配的log，然后删除后面的用leader的log覆盖，这样就达到了follower的log和leader的一致。对于leader缺失的log，由于选举时候的限制，说明该确实的log是没有commit的，所以可以忽略，直接覆盖即可。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;那么如果leader在replicate log entry的时候，如果leader挂了，然后重新选出的leader可能不是之前的实例，那么对于之前的leader已经commit的log entry，或者可能没有commit的log entry怎么处理呢？leader出现故障可能发生在下面三个阶段：&#xA;- log entry没有replicate在多数的follower，然后crash&#xA;- log entry已经replicate在多数的follower，没有commit，然后crash&#xA;- log entry已经replicate在多数的follower，并且已经commit，然后crash&lt;/p&gt;&#xA;&#xA;&lt;p&gt;还是以论文中的例子看一下怎么处理之前term的log entry:&#xA;&lt;img src=&#34;/media/archive/blog/images/commit_previous_log.png?imageView/0/w/500/&#34; alt=&#34;commit previous log.png&#34; /&gt;&#xA;我们用(termID,indexID)表示图中的term为termID，index是indexID的log entry.图中(a)对应第一种情况，此时S1是leader，(2,2)对应的log entry只在S1,S2完成了复制，然后S1挂了，这个时候根据leader选举的约束条件，S5可以当选为leader，然后接收了一个新的log entry，对应(3,2)，这个时候还没有在其他节点完成复制，就挂了，接着S1又成为了term 4阶段的新的leader, 此时只有接收新的log entry，复制的到其他follower时候，完成之前没有完成的复制，也就是图中S3对应(2,2)的entry,这个时候完成多数的复制后，即图中的&amp;copy;。这个时候(2,2)处于复制了多数但是没有commit的状态，(4,2)只有S1的logs里有，这个时候(2,2)虽然是多数但是是不会提交的，原因后面会解释。所以这个时候就有两种情况，分别对应上面的第二种和第三种情况，对于第二种情况对应(d)，S1中的(2,2)和(4,3)都没有提交，然后挂了，这个时候S5当选为leader(term 5, S2, S3, S4，都可能投票给S5)，然后强制用S5的(3,2)覆盖S1，S2, S3, S4未提交的日志。第三种情况对应(e), (2,2)和(4,3)已经提交，然后S1挂了，这个时候因为S1，S2，S3最后一个log的term是4，index是3，而S5最后一个log的term是3，不满足leader选举的限制条件，所以不能当选leader，新的leader只能从S1, S2,S3中选举，所以之前提交的log都是没有丢失。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;font color=red&gt;对于&amp;copy;中的(2,2)为什么虽然已经是多数，但是还是不会提交呢?&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;因为Raft为了简化提交旧term的log entry的过程，Raft不根据旧的log entry已经完成的副本的数量进行commit，只有当前term的log entry根据多数原则提交，但是因为在完成当前term Log entry复制的过程中，会强制复制leader的log 到确实相应log entry的follower，所以当commit当前某个index对应的log的时候，会将leader该index之前的log都进行提交，而且存在多数的follower的log在改index之前的log entry是一致的，然后在appendEntrye(心跳也是这个方法)的时候会将leader已经commit的最大index发送给follower, follower会根据该index提交本地的log。&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;font color=red&gt;会不会出现(b)中S1的(2,2)已经是多数，也就是S3也已经存在(2,2)，然后已经提交然后S1挂了，这个时候S5当选为leader，强制覆盖了S1，S2,S3里的(2,2)？&lt;/font&gt;&lt;/strong&gt;&#xA;&amp;gt; 这种情况是不会出现的，因为(2,2)是term 2阶段生成的日志，如果(2,2)已经提交，然后leader挂了，这个时候S5是不会当选为leader的，也就是说不会出现图(b)中S5里的(3,2)这个log entry.&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;cluster-membership-changes&#34;&gt;Cluster Membership Changes&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;当集群成员发生改变的时候，通常最直接的方法就是每个节点的配置直接更新成新的配置，但是这种方法会导致在切换的过程中，由于每个节点切换成功的时刻不一致，所以导致新旧配置产生两个Leader.还有一种方法是，将切换过程分两阶段，首先让集群暂时不可用，然后切换成新的配置，最后使用新的配置启用集群，这样显然会造成集群短暂的不可用。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;为了确保在变更配置的时候，集群依然能够提供该服务，Raft提供了一种Joint Consesus的机制。其核心想法是讲配置作为一个特殊的Log Entry，使用上面的replication算法分发到每个节点。Joint Consesus将新旧配置组合到一起：&#xA;- Log Entries会复制到新旧配置中的所有节点&#xA;- 任何一个server，可能是新配置的也可能是旧配置的，都能够当选为leader&#xA;- 选举或者提交entry需要新配置节点的多数节点同意，也需要旧配置多数节点同意&lt;/p&gt;&#xA;&#xA;&lt;p&gt;下图是配置变更的一个过程:&#xA;&lt;img src=&#34;/media/archive/blog/images/membership_change.png?imageView/0/w/500/&#34; alt=&#34;membership change.png&#34; /&gt;&#xA;当当前集群leader收到请求需要将配置从$C&lt;em&gt;{old}$变更到$C&lt;/em&gt;{new}$时，leader创建一个新的用于Joint Consesus的配置$C&lt;em&gt;{old,new}$，改配置作为一个log entry使用Raft的replicate算法复制到新旧配置里的所有节点，&lt;font color=red&gt;一旦节点接收到新的配置Log, 不管其是否已经提交都会使用该新的配置&lt;/font&gt;，也就是说当前集群的leader会使用$C&lt;/em&gt;{old,new}$来决策什么时候提交该配置，如果这个时候leader宕机，那么新选举出来的leader可能是使用old配置的节点，也可能使用$C&lt;em&gt;{old,new}$的节点。一旦$C&lt;/em&gt;{old,new}$提交，这个时候有多数的节点已经有$C&lt;em&gt;{old,new}$配置，这个时候如果leader宕机，只有有$C&lt;/em&gt;{old,new}$的节点才会成为leader。所以这个时候leader就会创建一个$C&lt;em&gt;{new}$log进行replicate，一旦$C&lt;/em&gt;{new}$已经commit，那么旧配置不相关的节点就可以关掉了。从图中可以看出不存在一个时刻$C&lt;em&gt;{old}$和$C&lt;/em&gt;{new}$同时作用的时刻，这就能保证系统配置变更的正确性。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;问:&lt;/strong&gt; &lt;font color=red&gt;新加入的节点没有任何的log entry，将会导致其短期能不能commit 新的log entries，怎么解决?&lt;/font&gt;&#xA;&amp;gt; 因为新加入的节点，其log可能需要一段时间才能接收完leader之前提交的log，所以会导致需要一段时间才能接受提交的新的log，Raft新增加了一个阶段，对于新加入的节点，在其log复制完也就是跟上集群内其他节点的日志前，不参与多数节点的行列，只有接受完旧的日志，才能够正常参与表决(投票和commit表决)。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;问:&lt;/strong&gt; &lt;font color=red&gt;如果leader不在新的配置里怎么办？&lt;/font&gt;&#xA;&amp;gt; 这种情况下，leader在提交$C_{new}$后再关掉，也就是说在提交新配置前，leader管理了一个不包含自己的集群，这个时候replicate log的时候，计算多数的时候不算他自己在内。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;问:&lt;/strong&gt; &lt;font color=red&gt;移除的节点在没有关掉的情况下，因为这些节点收不到leader的心跳，所以会重新发起选举，这个时候会像新集群的节点发送rpc，这个时候可能影响新节点的状态&lt;/font&gt;&#xA;&amp;gt; Raft规定，如果新配置的集群能够收到leader的心跳，即使收到了选举的RPC，也会拒绝掉，不会给他投票。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;log-compaction&#34;&gt;Log Compaction&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;随着请求的增多，Raft每个实例的log不断的增加，现实中是不可能任其无限增长的，因此Raft也提供了快照的方法来压缩日志。每个Raft实例都单独执行快照算法，当日志大小达到一定大小的时候触发快照操作，保存最后提交的日志时刻状态机的状态，以及最后提交的一个日志的index和term。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;当一个节点的日志远落后于leader节点，比如新加入节点，这个时候就需要将leader的快照发送给该节点，因此Raft也提供了一个InstallSnapshot的RPC接口用来发送快照。当follower收到leader的快照时候，根据快照里包含的LastIncludeIndex和LastIncludeTerm以及自身log的index和term来确定如果处理，如果当前节点包含一些没有提交但是和快照冲突的日志，那么清楚有冲突的日志，保留快照后面的日志(正常情况不会出现，有可能快照是重传或者产生了错误，所以收到一个自身日志之前某个index前的快照，这个时候覆盖该index之前的日志保留后续日志即可)。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;client-interaction&#34;&gt;Client Interaction&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;客户端与Raft集群进行交互主要存在两个问题：第一，客户端如何确定leader的位置？第二，如何保证Raft的线性一致性(Linearizable)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;客户端如何确定leader的位置？当客户端启动的时候会随机的选择一个节点请求，如果这个节点不是leader，那么会返回该节点最近知道的leader的位置(可能leader已经切换，该节点还没感知)。如果leader宕机了，那么客户端随机选择一个节点请求。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;第二个问题，Raft提供线性一致性，也就是说对于一个操作，Raft只执行一次，并且立即执行，后面的读一定读到最新的数据。但是如果不加任何的限制，目前描述的Raft算法是可能出现同一个命令执行多次的情况，比如如果leader在提交了log entry后但是没有返回给客户端的时候宕机了，那么客户端超时后会重新请求改命令。这样就可能提交两次操作。针对这种情况，客户端给每个操作都分配一个递增的序列号，Raft集群每个状态机都记录一下当前每个客户端已经执行的操作的最新的序列号，如果客户端请求了同一个序列号的操作，一点状态机发现已经执行过了，就直接返回上次的结果给客户端。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Raft在保证正确性的前提下实现一个了容易理解和实现的一致性算法，相比Paxos简化了很多，与Paxos最大的不同就是Raft是一个强leader的协议，所以的操作都依赖于leader，操作流的方向也只能从Leader发往其他节点，所以Raft整个协议分为两阶段，每个term首先进行选主，然后后续leader掌权接受所有客户端的请求，这样大大的简化了协议的复杂度，但是也存在leader负载过高的问题，不过通常实现的时候用的都是multi-raft，在接受请求前已经进行了负载的均衡，有时候还会使用旁路的监控，动态调整leader的位置达到更好的性能。&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;附录&lt;/strong&gt;&#xA;Raft正确性原则：&#xA;- Election Safety&#xA;一个term只能选出一个leader&#xA;- Leader Append-Only&#xA;leader绝不覆盖或者删除日志，只会增加新的日志&#xA;- Log Matching&#xA;如果两个logs的包含一个entry其term和index都相同，那么该entry之前所有的log entry都相同&#xA;- Leader Completeness&#xA;一个选举出来的leader包含了当前term之前所有term已经提交的log entry&#xA;- State Machine Safety&#xA;如果一个节点的状态机已经应用了某个index的操作，那么其他节点的状态机在改index也是执行同样的操作，不会是其他操作&lt;/p&gt;&#xA;</description>
      <pubDate>Thu, 13 Jul 2017 17:16:40 +0000</pubDate>
    </item>
    <item>
      <title>分布式对象存储比较-Ambry</title>
      <link>http://openhex.cn/2017/7/2/分布式对象存储比较-Ambry.html</link>
      <description>&lt;p&gt;在前面的文章&lt;a href=&#34;http://kdf5000.com/2017/06/25/对象存储比较-Haystack/&#34;&gt;对象存储比较-Haystack&lt;/a&gt;中详细介绍了对象存储的经典之作Haystack，该系统合并小文件，元数据分开管理的思想到现在依然是很多系统的基石，想TFS, FastDFS多少都借用了这个思想。去年(2016)Linkedid在数据管理会议SIGMOD发表了自己内部使用的对象存储系统&lt;a href=&#34;http://dprg.cs.uiuc.edu/docs/SIGMOD2016-a/ambry.pdf&#34;&gt;Ambry&lt;/a&gt;，能发到 SIGMOD，想必也不是等闲之物，因此就拜读了一下。整体感觉相比Haystack考虑的比较全面，诸如数据的负载均衡，partition的异步同步，甚至数据的冷热全都考虑了进来，但是依然能看到Haystack的影子，只能说是在基础上更加的完善，可用性，维护性也更强了。&lt;/p&gt;&#xA;&#xA;&lt;h2 id=&#34;动机&#34;&gt;动机&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;在介绍Ambry之前，先看一下为什么要重新自己设计一套对象存储系统，Linkedin的工程师想必也都是有经验的工程师，他们在分析了一下自己的业务场景，已经现在的解决方案，最后得出了一下的结论：&#xA;* 存储各种各样大小的对象(图片，文档，音视频)&#xA;* 各种不同的负载&#xA;* 现有系统大多针对小文件(100k一下)的文件存储进行了优化，但是不能适应各种大小的对象&#xA;* 现有系统不能很好的解决，数据访问倾斜，新加入节点导致数据分配不均衡等问题&lt;/p&gt;&#xA;&#xA;&lt;p&gt;因此他们决定在前人的肩膀上，重新设计一套适合自己的解决方案，他们认为他们的系统应该有如下的特点：&#xA;* 低延迟，高吞吐&#xA;* 跨多个数据中心&#xA;* 扩展性要强&#xA;* 能够很好的解决负载均衡的问题&lt;/p&gt;&#xA;&#xA;&lt;h2 id=&#34;架构&#34;&gt;架构&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/blog/images/ambry-arch.png&#34; alt=&#34;ambry-arch.png&#34; /&gt;&#xA;Ambry主要有Cluster Manager，Datanode，Frontend三个组件。在介绍每个组件负责的职责前，先介绍Ambry里的一个重要概念: partition.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Partition:&lt;/strong&gt; Partition是Ambry里的最小的存储单元，是一个虚拟的概念，相当于haystack里的Logical Volume,  Twitter BlobStore里的Virtual Bucket，Partition在实现的时候实际上是一个大的文件，存放着成千上万的小文件，并且作为replication的最小单位。因为存在replication，所以每个parition对应着多个位于不同节点，甚至不同机架和数据中心的物理文件，每个文件的大小通常100G，尽可能大，但是不能太大，因为移动数据的时候可能会消耗太多的时间。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Cluster Manager:&lt;/strong&gt; Cluster Manager负责管理整个集群的节点，磁盘等布局信息，跟踪partition的状态，以及映射的位置。不同的数据中心都有自己的cluster Manager，通过zookeeper保持状态的同步。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Datanode:&lt;/strong&gt; Datanaode是一个实际的物理节点，管理多个磁盘，存储真实的数据，每个datanode有多个partition，并且每个parition会在不同的节点有相应的副本。同时Datanode为每个partition都维护一个常驻内存的索引，Journals和布隆过滤，索引后面会详细介绍。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; Frontend是直接客户端交互的组件，所有的请求都是发送给Frontend然后再有其转发到相应的Datanode节点，然后接受数据返回给客户端。Frontend主要有下面三个功能：&#xA;- 请求处理转发&#xA;- 安全检查。Frontend在接收到客户端的请求的时候会根据设置进行一点的权限之类的安全检查。&#xA;- 操作拦截，push到Kafaka。这个功能主要用于监控集群的请求信息，用于分析请求的模式等信息。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Frontend有一个重要的模块叫Router Library，主要用于请求的转发，以及执行相关的逻辑。该模块的的请求操作行为是基于规则的。用户可以设置，每个写入或者get特定partition的请求是写入或者读取几个datanode才算是成功。除了转发请求，该模块还负责将大对象分成固定大小的chunck，然后作为新的blob分发到不同的partition，同时使用一个额外的保存blob的编号等信息，保存在某个paitition，并将partition返回给客户端。Router Library同时还兼顾故障检测，请求代理的功能。&lt;/p&gt;&#xA;&#xA;&lt;h2 id=&#34;索引-journal和bloom-filter&#34;&gt;索引、Journal和Bloom Filter&lt;/h2&gt;&#xA;&#xA;&lt;h3 id=&#34;索引&#34;&gt;索引&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;前面提到Datanode的主要作用就是存储不同的paitition，每个partition内保存了成千上万的小blob，每个blob在Partition的布局如下图：&#xA;&lt;img src=&#34;/media/archive/blog/images/ambry-blob_layout.png&#34; alt=&#34;ambry-blob_layout.png&#34; /&gt;&#xA;当增加一个blob的时候直接append到特定的partition即可。那么当获取一个blob的时候怎么才能够快速地找到相应的数据呢？为此Ambry为每一个Partition创建了一个索引，保存了每个Blob在paitition文件中的偏移位置，大小等信息，不过建索引不是什么新鲜的东西，Haystack也有。但是Linkedin的工程师并没有就此止步，他们为了节约成本，减少索引占用的内存，Ambry讲索引分成不同的segment，然后只有经常访问的blob，也就是热索引才会常驻内存，然后那些冷数据通过在内存维护一个布隆过滤器，当访问冷数据的时候现在布隆过滤器里查找是否存在其对应的索引段，然后再决定加载那段索引 ，从而在保证用少量内存的情况下依然能够有较高的访问速度。&#xA;&lt;img src=&#34;/media/archive/blog/images/ambry-index.png&#34; alt=&#34;ambry-index.png&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;replication和journal&#34;&gt;replication和Journal&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;replication是保证数据可靠性的一个有效方法，通常在保证数据的一致性上最经典的做法就是使用Paxos等协议，但是在对象存储的场景下，直接向所有或者多数的副本节点发送同步或者异步的请求就可以了，大大的简化了实现难度，并且保证了延迟，如果使用一致性协议则会大大的增加延迟(个人的猜测，也许不是这个原因)。Ambry的replication发生在put操作时候，客户端向Frontend发送put请求，然后根据贪心的策略选择replication的位置，选择的时候根据下面两个原则：&#xA;- 不能再同一个Datanode&#xA;- 处于多个不同的数据中心&lt;/p&gt;&#xA;&#xA;&lt;p&gt;副本的数量有管理员自己配置，选择好replicatio的位置后，异步的像这些节点发送请求，但是会根据管理员的配置等待指定的k(1，多数，all)个节点成功返回才向客户端返回成功的响应。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;因为replication的操作是异步的，因此失败的情况是可能发生的。为了保证不同副本的一致性，Ambry使用了一个replication算法，周期性的在后台执行，每个replica都作为一个master从其他节点拉取丢失的blob，为了加快检测的速度，使用journal维护每个节点上每个parititon的lastOffset，然后分下面两个阶段同步副本的blob&#xA;- 请求lastOffset后的所有blob id，然后过滤掉本地已经有的blob&#xA;- 请求missing的blob&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/blog/images/ambry-replication.png&#34; alt=&#34;ambry-replication.png&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h2 id=&#34;负载均衡&#34;&gt;负载均衡&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;负载均衡是Ambry相对于Haystack的一个重要的不同点，针对业务请求负载可能的倾斜，大量大的blobs以及集群节点的变化导致的负载不均衡，Ambry提出了分别针对静态集群，以及动态添加节点时候两种情况提出了自己的均衡算法。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;静态集群&#34;&gt;静态集群&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;针对静态集群，导致其负载均衡主要的原因是大量的大Blob导致数据分布的均衡，已经数据冷热不同从而导致访问的不均衡。对于大的blob，ambry将其分成固定大小(一般4M-8M)的chunk，分发到不同的partition；对于热门数据通过CDN缓存，从而可以减少冷热数据访问不均的问题。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;动态集群&#34;&gt;动态集群&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;对于新增节点的情况，因为新增节点，那么其上面的partition的状态都是可写的，因为新写入的数据往往有更大可能性被访问，因此新节点的partition往往承担大量的读和写，所以导致访问严重不均。Ambry使用了一个rebanlance的算法，该算法定义了一个理想下的读写，只读，磁盘使用率，计算方法都是根据整个集群当前读写状态的，只读磁盘，以及所有使用的磁盘除以整个集群所有的磁盘数得到；然后将每个磁盘上RW, RO状态的partition放入一个pool，最后将pool里的partition分发给低于理想状态的磁盘。整个算法的伪代码如下:&#xA;&lt;img src=&#34;/media/archive/blog/images/ambry-reblance.png&#34; alt=&#34;ambry-reblance.png&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;那么如何移动一个partition到另一个节点呢？Ambry分为三个阶段去安全的移动Partition&#xA;- 在新的datanode创键一个replica&#xA;- 同步旧的replica所有的Blob到新的replica，同步的时候如果有新的blob写入旧的replica，那么两个replica都要执行append操作&#xA;- 同步完成后，删除旧的replica，然后更新映射信息&lt;/p&gt;&#xA;&#xA;&lt;h2 id=&#34;操作&#34;&gt;操作&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Ambry主要提供了Put, Get, Delete三种操作，这三种操作都是客户端直接和Frontend交互，然后Frontend分发请求到相应的partition对应的Datanode，最后将数据或者操作状态返回给客户端。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Delete和Put操作相同，同步写本地数据中心的replica，然后异步写不同数据中心的replica, 只有返回管理员设置的k个成功响应即可。后续通过前面描述的replication算法达到副本的一致性。Put操作在Frontend层的时候会随机选择Partition希尔如，并且声称一个PartitionId + UUID的fid作为blob的唯一id，返回给客户端。对于Delete操作会置delete flag位，并且在后续周期性的compact操作中将其删除。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Get操作请求在Router模块，会提取请求的fid找到对应partitionId，向对应的replica节点发送请求，然后根据设置的成功请求数判断是否成功。如果get时候replica没有完成，则proxy到其他已经存在的数据中心。&lt;/p&gt;&#xA;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;前面介绍了Ambry的整体架构以及核心组件的实现，中间也多多少少和Haystack进行了对比。总的来说Ambry的设计是在Haystack基础上又前进了一步，主要是为了能够适应不同大小的对象，以及解决负载均衡的问题。整个系统印象最深的有三点：&#xA;- 将大对象分成多个固定大小的chunck，并用一个新的Blob记录了不同chunck的信息，虽然多消耗了一些空间，但是提高了get数据的并行度，从而能够减低延迟。&#xA;- 对Haystack存在的多个副本同步失败的问题，通过引入异步复制，按需设置一致性，后台定期执行replication算法实现最终一致性巧妙的解决了。&#xA;- 对于Haystack没有解决的负载均衡的问题，创新性的提出了负载 均衡算法，后台动态调整不同状态partition的分布，实现动态的reblance。&lt;/p&gt;&#xA;</description>
      <pubDate>Sun, 02 Jul 2017 21:07:36 +0000</pubDate>
    </item>
    <item>
      <title>对象存储比较 - Haystack</title>
      <link>http://openhex.cn/2017/6/25/对象存储比较-Haystack.html</link>
      <description>&lt;p&gt;经过一段时间的调研，对分布式对象系统有了一点浅显的认识，暂不谈和文件系统的区别(其实还是有点傻傻分不清)，姑且认为对象存储和文件系统最大的区别就是API，文件系统提供了完整POSIX语义，往往具有层次化的目录结构，对文件可以进行精细的操作(open, read, write, seek, delete等)，相对会复杂一些。相比之下，对象存储就简单的多，对象本义是&lt;strong&gt;B&lt;/strong&gt;inary &lt;strong&gt;L&lt;/strong&gt;arge &lt;strong&gt;OB&lt;/strong&gt;ject(BLOB), 是一个大的二进制文件，是作为一个整体出现，不能对其进行修改，因此对象存储系统有一个显著的特点就是Immutable，即不变性，正是因为这个特点对象存储系统一般只提供put, get, 和delete的操作，并且都是以key/val的形式，val一般是整个blob. 具体到实际的资源，主要指的是像图片，视频，文档，源码，二进制程序等这样的文件，这些文件往往都很小，一般在100k左右，视频可能会大一些，能达到几十兆甚至好几个G。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;综合上面的一些特点学术界和工业界对对象存储系统也区别于文件系统做了很多优化，其中比较有名的当属10年Facebook在OSDI公开的一个分布式对象存储系统Haystack，针对其内部的业务场景(大量的小图片存储)，创新的提出了一个小文件合并成大文件的方法去存储海量图片的需求，并且得到了很好的实践考研。除了facebook之外，Amazon, twitter， linkdin等知名厂商也都提出了自己的对象存储，其中Amazon的S3作为一个商业服务也取得了巨大的成功。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;本文主要调研了Facebook， twitter, linkdin内部使用的对象存储系统，企图从其系统的构建初衷到设计进行一次全面的探索，最后再进行一个对比，方便有需求的同学选择适合自己的系统。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h2 id=&#34;fackbook&#34;&gt;Fackbook&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Facebook作为全球最大的图片分享社区，其图片，视频的存储量用海量形容当之无愧，2010年其第一次将内部使用的海量图片存储系统Haystack发表在了系统界的顶级会议OSDI，虽然现在看整个系统设计非常简单，但是已经发表便引起了工业界和学术界的顶礼膜拜，从此各种海量小文件存储系统无不拿其说事，其提出的通过文件合并从而减少元数据，最终减少磁盘io的思想经久不衰，认真研读了当年的文章，整篇文件架构清晰，丝毫不拖泥带水。继Haystack之后，2014年经过四年的发展，其内部系统又做了大量的改进，并且新增加了一个专门用于存储冷数据的存储系统，大大的提高了存储成本，同样也是发表在了OSDI(怎么感觉OSDI是他们家开的，想发就发，当然也可能是这些系统都是经过实践的考研，这也是系统界比较看重的)。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;haystack&#34;&gt;Haystack&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;在介绍Haystack之前，先看一下Facebook面临着什么样的问题，从论文里的数据可以得知，Facebook在2010年，其内部已经存储2600亿张图片，并且每周还持续增加100万，这样的体量，恐怕一般的公司是早就那以应对了。那么在开发Haystack之前Facebook内部使用的系统是什么样的呢？当时他们使用的还是基于NAS的存储系统，存储服务器讲NAS的Volumes挂载在本地，然后每个文件放在不同的目录，每个目录存放上千张图片，这样会有什么问题呢？在传统的POSIX语义的文件系统下，每个目录对应的元数据会非常大，并且每个文件都有自己的元数据，因此元数据的访问变成了主要的瓶颈，为此他们也尝试了一些解决方案，甚至修改了操作源码，增加了一个&lt;code&gt;open_by_filehandler&lt;/code&gt;的方法，将每个文件的handler保存在memcached，这样确实能够减少磁盘操作，减少文件的响应时间。但是很快他们发现，被访问的图片很多都是以前没有访问过的，也就是说很多时候文件handler都是不在缓存中的，除非所有文件handler都在缓存里，这就需要大量的内存，显然不靠谱。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;所以，现在Facebook面临的主要问题是：&#xA;- 海量图片&#xA;- 元数据量大，访问元数据成为瓶颈&#xA;- 如何在低成本，高可用的情况下，保证系统的性能&lt;/p&gt;&#xA;&#xA;&lt;p&gt;但是Facebook的大神们并没有屈服，经过两年时间的研发上线运行，Haystack诞生了！Haystack解决方案的核心点就是通过减少文件系统元数据的大小，保证其能够常驻内存，从而减少磁盘的操作次数。工程师们分析了Facebook的图片数据，图片有个显著的特征：一次上传，经常读，永不修改，很少删除。针对这个特点，他们认为文件系统的元数据里有很多属性，比如权限等都是多余的，因此尽可能的减少元数据的大小，并将其保存到内存，这样可以只用一次的磁盘操作就从文件里读出图片数据，并且他们讲多个图片追加写入到一个大文件里，从而进一步的减少了元数据的数量。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Haystack的整体架构如图：&#xA;&lt;img src=&#34;/media/archive/blog/images/haystack_arch.jpeg?imageView/0/w/500/&#34; alt=&#34;haystack_arch.jpeg&#34; /&gt;&#xA;从架构图可以看出Haystack一共包含了四个核心组件：CDN, Haystack Directory, Haystack Cache, Haystack Store. 下面简略介绍一下每个组件的主要功能，最后我们再给出读写操作的详细流程。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;haystack-directory&#34;&gt;Haystack Directory&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;前面我们提到，Haystack有一个很重要的优化就是将很多图片合并到一个大文件里，论文里讲一个大文件成为Physical Volume， 并且每一个Physical Volume都有多个副本(一般是3份)，因此在这之上有抽象出了一个Logical Volume，一个Logical Volume对应几个Physical Volume，这个对应关系以及其他一些状态信息都有Directory保存。废话不说，Haystack Directory主要有下面几个功能：&#xA;- 逻辑卷标到物理卷标的映射&#xA;- 读写负载均衡&#xA;- 决定一个读请求是通过CDN还是Cache&#xA;- 记录Logical volumes是否只读(只读的粒度是机器层次)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;所有这些信息全部都保存在Memcached里，从而减少了访问延迟。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;cdn&#34;&gt;CDN&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;CDN就是普通的CDN，接受请求，缓存数据，如果没有命中则访问Haystack Cache。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;haystack-cache&#34;&gt;Haystack Cache&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Cache相当于一个内部的CDN，因为有两层的CDN所以Haystack Cache的缓存策略就不太一样，并不是来一个请求就缓存，Haystack Cache只有下面两种情况才进行缓存:&#xA;- 当请求直接来自浏览器，而不是CDN&#xA;- 图片来自write-enable的存储机器&lt;/p&gt;&#xA;&#xA;&lt;p&gt;对于第一种情况，如果一个请求来自CDN说明CDN没有缓存，那么返回给CDN后，cdn会缓存，所以Cache无需缓存；对于第二种情况，基于一种假设：新上传的图片有很大可能被读取。并且当前的设计只读或者只写的性能好，但是同时读写的性能不太好，所以最好将新写的数据缓存。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;haystack-store&#34;&gt;Haystack Store&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Haystack Store是最核心的一部分，所有的图片文件全部都保存在Store Server里。其主要有下面的特点：&#xA;- 保存实际的Physical Volume(大文件)，路径可能是&lt;code&gt;/hay/haystack_&amp;lt;logical volume id&amp;gt;&lt;/code&gt;&#xA;- 所有的物理卷标文件描述符都保存在内存&#xA;- 对一个图片的读取，photoid, offset, size都在内存读取，不需要磁盘操作&#xA;维护photoid -&amp;gt; (fd, flag, offset, size)的映射&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Haystak Store除了实际的Phystical Volume外，还有一个重要的数据文件：索引文件。索引文件主要作用是可以在机器重启的时候帮助快速的重建内存mapping。索引文件在写入一个needle时候，同步的写Physical Volume和内存map， 异步写索引文件；同时，删除一个文件的时候，同步置内存map对应的flag和Pyhsical Volume的flag，但是&lt;strong&gt;不跟新&lt;/strong&gt;索引文件信息。因此可能存在两个问题：&#xA;- needle可能不在索引文件中&#xA;- 索引文件不能反映删除了的needle&lt;/p&gt;&#xA;&#xA;&lt;p&gt;针对第一个问题，在重建mapping的时候检查不在索引文件中的needle并append到索引文件索引文件的最后一个needle记录，对应volume中的needle后的所有needle都要重新append index；针对第二个问题，从物理needle中读取needle后，检查flag，如果是删除的话更新in-memory map。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;读写删操作&#34;&gt;读写删操作&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;读操作&#xA;先根据上传时候保存的需要读的文件的(logicalVolume id, key, alternate key, cookie)等信息请求Directory获取实际的请求URL，获取的格式如下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;http://&amp;lt;CDN&amp;gt;/&amp;lt;Cache&amp;gt;/&amp;lt;Machine Id&amp;gt;/&amp;lt;Logical volume Id, PhotoId&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;然后根据该URL请求CDN服务器，服务器如果没有命中则会请求Cache服务器, 同样Cache服务器没有命中则会请求URL中的Machine节点，该节点在内存内查找相关映射，如果文件没有被删除(flag)，则从磁盘读取，然后验证cookie，checksum等，如果成功则返回相应数据，否则返回读取错误。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;写操作&#xA;同样从从Directory获取可以写的logical volume id，以及key, alternate key ，cookie&#xA;然后同步写所有logical volume 对应的physical volume。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;删除操作&#xA;删除操作会同步置map和volume里相应的flag，volume的数据后续compact的时候会进行删除。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;更新操作&#xA;不允许更新，只能append相同的key的文件。如果新的needle append到不同的logical volume，则更新Directory的application metadata，保证下次请求不会读取到旧版本数据；如果append到相同的logical volume，则根据offset来更新map，最大的是最新的。&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;到此为止Haystack比较核心的组件以及操作都进行了有一个回顾，除了这些之外，Haystack也提供了一系列的故障恢复机制，主要包括故障检测和修复技术。同时也做了一些内存以及上传等的优化。整体来说Haystack简单有效，能够有些的解决当前的场景的问题。当然也存在一些问题，比如Directory服务器存在单点问题，以及Storage Server的内存映射如果很大的时候怎么办，总不能全部放内存吧，当然可以限制每个节点的Physical Volume节点的数量限制，不过看来就就没有那么优美了。下一篇会介绍一些2016年Facebook在OSDI发表的另一个和Haystack相辅相成的存储系统，解决了Haystack的一些问题。&lt;/p&gt;&#xA;</description>
      <pubDate>Sun, 25 Jun 2017 20:54:24 +0000</pubDate>
    </item>
    <item>
      <title>SeaweedFs索引-CompactMap</title>
      <link>http://openhex.cn/2017/6/18/SeaweedFs索引-CompactMap.html</link>
      <description>&lt;p&gt;SeaweedFS提供了几种不同的needle索引策略，包括memory, btree, blotdb, leveldb四种，其中默认的是memory，也是其内部唯一自己实现的一种索引，btree使用google的btree开源实现，boltdb和leveldb都依赖一个db.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;memory的索引实现，使用了一个叫CompactMap(这是作者自己起的名)的数据结构。本文后面将会重点介绍compactMap是一个怎样的数据结构，以及如何使用这个数据结构简历needle的索引。&lt;/p&gt;&#xA;&#xA;&lt;!--more --&gt;&#xA;&#xA;&lt;h2 id=&#34;compactmap&#34;&gt;CompactMap&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;compactMap本质是一个数组(Golang中可以动态扩展的数组)，其元素类型是CompactSection，定义如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;//This map assumes mostly inserting increasing keys&#xA;type CompactMap struct {&#xA;    list []*CompactSection&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;CompactMap结构体内部只有一个CompactSection的结构体的指针数组，那么为什么作者假设插入的key是增序的呢？我们先看CompactSection是一个什么样的结构，答案后面就会很清楚。&#xA;下面是CompactSection的结构：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type CompactSection struct {&#xA;    sync.RWMutex                 //读写锁&#xA;    values   []NeedleValue       //NeedleValue数组，保存了Needle的key, offset, size&#xA;    overflow map[Key]NeedleValue //用于保存超出batch，但是在(start, end)范围的needleValue&#xA;    start    Key                 //该section保存的key最小值&#xA;    end      Key                 //该section保存的key最大值&#xA;    counter  int                 //当前该section的needleValue的数量&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;CompactSection保存了key值在[start, end]范围内的所有needleValue，每个section都会记录当前处于该段的所有needleValue数量，并且有一个固定的容量限制，目前版本的限制是100000，如果该段已经存储的needleValue已经达到了最大容量，但是其key那么就会将当前[start,end]范围，那么needleValue保存到map里，也就是说values数组最大长度为100000， 主要是因为当前使用二分搜索，查找key对应的needleValue, 所以数组还是不能太大。使用map也能够快速的定位。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;上面简单介绍了compactSection的结构，先不管怎么向某个section插入一个needleValue, 我们只用知道一个compactSection保存从[start,end]范围内的所有needleValue。所以CompactMap的list就是将key分成一段一段管理，至于每一段的key是怎么管理的可以不用管，只用知道每个段的范围即可，当要插入一个key的时候，使用二分查找的方法找到该key所处的section, 然后插入到该section。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;下面是二分查找的代码：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (cm *CompactMap) binarySearchCompactSection(key Key) int {&#xA;    l, h := 0, len(cm.list)-1&#xA;    if h &amp;lt; 0 {&#xA;        return -5&#xA;    }&#xA;    if cm.list[h].start &amp;lt;= key {&#xA;        if cm.list[h].counter &amp;lt; batch || key &amp;lt;= cm.list[h].end {&#xA;            return h&#xA;        }&#xA;        return -4&#xA;    }&#xA;    for l &amp;lt;= h {&#xA;        m := (l + h) / 2&#xA;        if key &amp;lt; cm.list[m].start {&#xA;            h = m - 1&#xA;        } else { // cm.list[m].start &amp;lt;= key&#xA;            if cm.list[m+1].start &amp;lt;= key {&#xA;                l = m + 1&#xA;            } else {&#xA;                //貌似这里有bug, 如果处于(end, $$start_{m+1}$$), 则同样要插入一个新的section&#xA;                /*if(key &amp;gt; cm.list[m].end){&#xA;                    return -5&#xA;                }*/&#xA;                return m&#xA;            }&#xA;        }&#xA;    }&#xA;    return -3&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;下面以上面的代码分析怎么找到一个NeedleValue应该插入的section:&#xA;* 如果list为空则直接返回一个负值&#xA;* 先判断是否能够插入到最后一个section, 能够插入的条件是该key大于section的最小key值，如果当前section没有超过容量，或者超过了但是处于[start, end]范围，则返回当前section, 否则返回一个负值。&#xA;* 前两个条件都不满足的话，就二分搜索前面所有section能够插入的section, 如果到第一个section都没有找到，即key&amp;lt; list[0].start， 说明需要插入一个新的section, 返回一个负值&lt;/p&gt;&#xA;&#xA;&lt;p&gt;总的来说，搜索有两个结果，返回一个可以插入的section和返回一个负值，当返回一个负值的时候，就需要重新生成一个compactSection，使用插入排序的方法，将其插入到list&#xA;代码如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (cm *CompactMap) Set(key Key, offset, size uint32) (oldOffset, oldSize uint32) {&#xA;    x := cm.binarySearchCompactSection(key)&#xA;    if x &amp;lt; 0 {&#xA;        //println(x, &amp;quot;creating&amp;quot;, len(cm.list), &amp;quot;section, starting&amp;quot;, key)&#xA;        cm.list = append(cm.list, NewCompactSection(key))&#xA;        x = len(cm.list) - 1&#xA;        //keep compact section sorted by start&#xA;        for x &amp;gt; 0 {&#xA;            //插入排序，保证start有序&#xA;            if cm.list[x-1].start &amp;gt; cm.list[x].start {&#xA;                cm.list[x-1], cm.list[x] = cm.list[x], cm.list[x-1]&#xA;                x = x - 1&#xA;            } else {&#xA;                break&#xA;            }&#xA;        }&#xA;    }&#xA;    return cm.list[x].Set(key, offset, size)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;del&gt;感觉这里存在一个bug, 因为二分搜索的时候是根据start来查找能够插入的section， 但是存在一种情况，比如假设list[i].start &amp;lt;= key, 且list[i+1].start &amp;gt; key， 那么上面的程序就会返回插入到第i个section, 但是可能list[i].end &amp;lt; key， 也就是说实际上这个时候应该生成一个新的setction插入到除以i和i+1之间。当然作者假设key是递增的（并不一定要连续），如果key是递增的，这样就能能够保证不会出现出现插入一个中间断节的list.但是如果假设是递增的，何必使用二分查找合适的section呢？肯定是最后一个或者需要重新在后面追加一个，所以目前感觉这个逻辑还是有问题的。考虑的非递增的情况，但是非递增的情况下，在二分查找section又没有考虑断节的情况，所以这里应该有问题，二分查找的代码里21-23行是我添加的，应该可以解决这个bug。后分析发现，因为插入除了最后一个section时候，同样也更新了end所以就不存在这个问题&lt;/del&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h3 id=&#34;compactsectionmap&#34;&gt;CompactSectionMap&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;前面已经简单的介绍了CompactSectionMap的结构，主要有一个数组和一个map组成，并且每个section的数组都有一个容量，下面主要分析怎么set和get一个needleValue。&#xA;下面是set的核心代码：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;//return old entry size&#xA;func (cs *CompactSection) Set(key Key, offset, size uint32) (oldOffset, oldSize uint32) {&#xA;    cs.Lock()&#xA;    if key &amp;gt; cs.end {&#xA;        cs.end = key&#xA;    }&#xA;    if i := cs.binarySearchValues(key); i &amp;gt;= 0 {&#xA;        oldOffset, oldSize = cs.values[i].Offset, cs.values[i].Size&#xA;        //println(&amp;quot;key&amp;quot;, key, &amp;quot;old size&amp;quot;, ret)&#xA;        cs.values[i].Offset, cs.values[i].Size = offset, size&#xA;    } else {&#xA;        needOverflow := cs.counter &amp;gt;= batch&#xA;        needOverflow = needOverflow || cs.counter &amp;gt; 0 &amp;amp;&amp;amp; cs.values[cs.counter-1].Key &amp;gt; key&#xA;        if needOverflow {&#xA;            //println(&amp;quot;start&amp;quot;, cs.start, &amp;quot;counter&amp;quot;, cs.counter, &amp;quot;key&amp;quot;, key)&#xA;            if oldValue, found := cs.overflow[key]; found {&#xA;                oldOffset, oldSize = oldValue.Offset, oldValue.Size&#xA;            }&#xA;            cs.overflow[key] = NeedleValue{Key: key, Offset: offset, Size: size}&#xA;        } else {&#xA;            p := &amp;amp;cs.values[cs.counter]&#xA;            p.Key, p.Offset, p.Size = key, offset, size&#xA;            //println(&amp;quot;added index&amp;quot;, cs.counter, &amp;quot;key&amp;quot;, key, cs.values[cs.counter].Key)&#xA;            cs.counter++&#xA;        }&#xA;    }&#xA;    cs.Unlock()&#xA;    return&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;下面是set的过程：&#xA;* 在插入前，首先会进行加锁的操作&#xA;* 然后如果当前的key大于end, 那么更新end(这里优于，在执行set之前，compactMap的set里有个判断，如果key大于最后一个section时，如果小于当前容量，则直接返回最后一个section，如果大于当前容量，那么要求要小于最后一个sectiond的end, 这样的结果就是最后一个section的end只有在小于当前容量的时候才会更新。低于其他的section就没有这个限制，这样可以防止中间出现断节从而需要插入新的section)&#xA;* 使用二分查找在values数组中查找是否含有该key, 如果含有那么就更新对应的needleValue，返回旧的needleValue&#xA;* 如果没有找到，则判断是否需要插入到overflow map中，这里判断是否需要插入到溢出map中并不单单，根据overflow来判断，还有一种情况是，没有大于数组的容量，但是该key小于values数组中最后一个needleValue的key, 这样能够保证values数组的key是有序的&#xA;* 如果不需要溢出，那么久直接追加在values数组的最后面&lt;/p&gt;&#xA;&#xA;&lt;p&gt;从compactSectionMap中获取一个needleValue的操作比较简单，下面是核心代码：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (cs *CompactSection) Get(key Key) (*NeedleValue, bool) {&#xA;    cs.RLock()&#xA;    if v, ok := cs.overflow[key]; ok {&#xA;        cs.RUnlock()&#xA;        return &amp;amp;v, true&#xA;    }&#xA;    if i := cs.binarySearchValues(key); i &amp;gt;= 0 {&#xA;        cs.RUnlock()&#xA;        return &amp;amp;cs.values[i], true&#xA;    }&#xA;    cs.RUnlock()&#xA;    return nil, false&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;get的时候也同样要先加锁，然后先判断overflow map中 是否存在改key, 如果不存在二分搜索values数组，如果都不存在，则返回没有找到即可。&lt;/p&gt;&#xA;&#xA;&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;整个CompactMap的设计思路还是比较清晰的，很像skiplist，通过分段(section)，可以大大的加快查找速度，不过如果key比较随机，那么就会带来很多的二分查找，插入排序的操作，性能就不是那么理想，不过总的来说这个数据结构还是比较高效的。&lt;/p&gt;&#xA;</description>
      <pubDate>Sun, 18 Jun 2017 23:39:23 +0000</pubDate>
    </item>
    <item>
      <title>分布式对象存储系统SeaweedFS</title>
      <link>http://openhex.cn/2017/6/13/分布式对象存储系统SeaweedFS.html</link>
      <description>&lt;p&gt;SeaweedFS是Facebook的海量图片存储系统Haystack的一个开源实现，其目标是：&#xA;- 存储数亿张图片&#xA;- 快速响应的文件服务&lt;/p&gt;&#xA;&#xA;&lt;p&gt;seaweedfs提供一个Key-&amp;gt; file的k/v服务，不支持POSIX的文件语义。虽然是Haystack的开源实现，但是也有所不同，比如Haystack论文里提到的使用一个中心节点保存所以文件的metadate。Seaweedfs实现中，中心master是负责文件卷标的管理，具体文件的metadata有相应的valume服务器负责。这样可以大大减缓中心节点的并发的压力，并且提供了快速的文件访问&lt;strong&gt;(一次磁盘操作)&lt;/strong&gt;。&#xA;&lt;!--more --&gt;&#xA;&lt;strong&gt;每个文件的metadata只有50个字节，因此可以在O(1)的时间内从磁盘读取出来。&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;额外的功能:&#xA;* 可以选择不同层次的副本策略：不备份，不同机架，不同数据中心&#xA;* 中心节点自动故障恢复 - 无单点问题&#xA;* 根据文件mime类型自动选择Gzip压缩&#xA;* 更新或者删除文件之后自动compaction&#xA;* 同一个集群的服务器可以有不同的磁盘大小，操作系统，文件系统&#xA;* 增加或者移除节点不会导致任何数据的re-balance&#xA;* 可选的Filer Server提供&amp;rdquo;normal&amp;rdquo;的目录和文件服务&#xA;* 可选的文件方向调整修复&#xA;* 支持Etag, Accept-Range, Last-Modified等&#xA;* 支持in-memory/leveldb/boltdb/btree 模式，以便调整性能/内存的平衡&lt;/p&gt;&#xA;&#xA;&lt;h2 id=&#34;相关术语&#34;&gt;相关术语&lt;/h2&gt;&#xA;&#xA;&lt;h3 id=&#34;save-file-id&#34;&gt;Save File Id&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;fid的格式是&lt;code&gt;volumeId, filekey+fileCookie&lt;/code&gt;, 其中volumeId是一个32位的无符号整数。fileKey是一个64位的无符号整数。fileCookie是一个64位的无符号整数，用来防止url暴力破解。volumeID, fileKey和fileCookie被编码成十六进制，&lt;strong&gt;获得的fid需要保存到应用服务器&lt;/strong&gt;。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;writefile&#34;&gt;WriteFile&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;写文件分为两步：&#xA;* 从master获取可以保存的volume信息&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -X POST http://localhost:9333/dir/assign&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;可以返回volume node的相关信息，以及相应的fid&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;{&amp;quot;fid&amp;quot;:&amp;quot;11,026bfba733&amp;quot;,&amp;quot;url&amp;quot;:&amp;quot;127.0.0.1:8080&amp;quot;,&amp;quot;publicUrl&amp;quot;:&amp;quot;127.0.0.1:8080&amp;quot;,&amp;quot;count&amp;quot;:1}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;保存文件到相应的volume server&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -X PUT -F file=@/Users/KDF5000/Documents/2017/Coding/ObjectStorage/SeaweedFS/seaweedfs/weed/vim-key.png http://127.0.0.1:8080/11,026bfba733 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;成功的话返回上传的文件名和文件大小&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;{&amp;quot;name&amp;quot;:&amp;quot;vim-key.png&amp;quot;,&amp;quot;size&amp;quot;:236702}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;readfile&#34;&gt;ReadFile&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;读文件可以根据fid到任何一个节点(master,volumeserver)去读，他会自动的跳转到实际存储的节点。&#xA;请求的格式可以至此下面的几种格式：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; http://localhost:8080/3/01637037d6/my_preferred_name.jpg&#xA; http://localhost:8080/3/01637037d6.jpg&#xA; http://localhost:8080/3,01637037d6.jpg&#xA; http://localhost:8080/3/01637037d6&#xA; http://localhost:8080/3,01637037d6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;也可以对图片进行伸缩&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;http://localhost:8080/3/01637037d6.jpg?height=200&amp;amp;width=200&#xA;http://localhost:8080/3/01637037d6.jpg?height=200&amp;amp;width=200&amp;amp;mode=fit&#xA;http://localhost:8080/3/01637037d6.jpg?height=200&amp;amp;width=200&amp;amp;mode=fill&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;架构&#34;&gt;架构&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;seaweedfs主要有两种服务类型，master server和volume server。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;master server可以部署多个节点，使用raft维护一致性，主要负责记录volumeId到volume server的映射关系。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Volume server 是主要的存储节点，有一系列的volume文件(默认32G),每个volume存储很多的小文件，与之对应还有有一个所以文件，记录volume中文件的编号，偏移，大小等信息。Volume server维护本地所有volume的元信息，初始化后保存在内存中，这样当读一个文件的时候只用一次磁盘操作即读出文件的实际内容。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;读写文件&#34;&gt;读写文件&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;写文件的时候需要首先与master server通信，获得可以保存文件的volume信息，master server会返回(volume id, file key, file cookie, volume node url)格式的信息用于保存文件。然后client在请求返回的volume node url以及文件路径保存文件到volume server。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;读文件时候client可以根据之前记录的volume server信息，根据(volumeId, file key, file cookie)直接读取文件信息，也可以向master节点查询volume server的url, 然后去读取文件信息。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;replication&#34;&gt;Replication&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;在上传图片的时候，获取fid时候可以指定是否需要保存副本，保存的类型是什么&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;curl -X POST http://localhost:9333/dir/assign?replication=001&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;seaweedfs一共有6种副本类型：&#xA;* 000: 没有副本(默认)&#xA;* 001: 在同一个机架保存一份副本&#xA;* 010: 在同一个数据中心的不同机架保存一份副本&#xA;* 100：在不同数据中心保存一份副本&#xA;* 200：在两个不同的数据中心保存两个副本&#xA;* 110：一个副本在不同机架，一个在不同数据中心&lt;/p&gt;&#xA;</description>
      <pubDate>Tue, 13 Jun 2017 19:39:31 +0000</pubDate>
    </item>
    <item>
      <title>Golang第八章: Goroutine和Channel</title>
      <link>http://openhex.cn/2017/5/7/Golang第八章-Goroutine和Channel.html</link>
      <description>&lt;p&gt;并发编程模型&#xA; - 顺序通信进程(Communicating Sequential Processes) CSP&#xA;  值会在不同的实例(goroutine)中传递&#xA; - 多线程共享内存&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;!--more --&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;goroutines&#34;&gt;Goroutines&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Go语言中每一个并发的执行单元叫做一个goroutine&lt;/li&gt;&#xA;&lt;li&gt;Go语言中主函数即在一个单独的goroutine中运行&lt;/li&gt;&#xA;&lt;li&gt;所有的goroutine在主函数返回时，都会直接打断，程序退出&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;简单的clock程序&lt;a href=&#34;ch8/clock.go&#34;&gt;clock.go&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;    &amp;quot;flag&amp;quot;&#xA;    &amp;quot;fmt&amp;quot;&#xA;    &amp;quot;io&amp;quot;&#xA;    &amp;quot;log&amp;quot;&#xA;    &amp;quot;net&amp;quot;&#xA;    &amp;quot;time&amp;quot;&#xA;)&#xA;&#xA;var (&#xA;    host string&#xA;    port int&#xA;)&#xA;&#xA;func init() {&#xA;    flag.StringVar(&amp;amp;host, &amp;quot;host&amp;quot;, &amp;quot;localhost&amp;quot;, &amp;quot;clock server host&amp;quot;)&#xA;    flag.IntVar(&amp;amp;port, &amp;quot;port&amp;quot;, 8000, &amp;quot;clock server port&amp;quot;)&#xA;    flag.Parse()&#xA;}&#xA;&#xA;func handleCon(c net.Conn) {&#xA;    defer c.Close()&#xA;    // for {&#xA;    _, err := io.WriteString(c, time.Now().Format(&amp;quot;15:04:05\n&amp;quot;))&#xA;    if err != nil {&#xA;        return&#xA;    }&#xA;    time.Sleep(1 * time.Second)&#xA;    // }&#xA;}&#xA;&#xA;func main() {&#xA;    server := fmt.Sprintf(&amp;quot;%s:%d&amp;quot;, host, port)&#xA;    listener, err := net.Listen(&amp;quot;tcp&amp;quot;, server)&#xA;    if err != nil {&#xA;        log.Fatal(err)&#xA;    }&#xA;    for {&#xA;        conn, err := listener.Accept() // 会阻塞&#xA;        if err != nil {&#xA;            log.Print(err)&#xA;            continue&#xA;        }&#xA;        go handleCon(conn)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;ch8/netcat.go&#34;&gt;client.go&lt;/a&gt;：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;    &amp;quot;io&amp;quot;&#xA;    &amp;quot;log&amp;quot;&#xA;    &amp;quot;net&amp;quot;&#xA;    &amp;quot;os&amp;quot;&#xA;    &amp;quot;sync&amp;quot;&#xA;)&#xA;&#xA;func main() {&#xA;    servers := []string{&amp;quot;localhost:8001&amp;quot;, &amp;quot;localhost:8002&amp;quot;, &amp;quot;localhost:8003&amp;quot;}&#xA;    var wg sync.WaitGroup&#xA;    for _, s := range servers {&#xA;        wg.Add(1)&#xA;        go func(serv string) {&#xA;            conn, err := net.Dial(&amp;quot;tcp&amp;quot;, serv)&#xA;            if err != nil {&#xA;                log.Fatal(err)&#xA;            }&#xA;&#xA;            if _, err := io.Copy(os.Stdout, conn); err != nil {&#xA;                log.Fatal(err)&#xA;            }&#xA;            conn.Close()&#xA;            wg.Done()&#xA;        }(s)&#xA;    }&#xA;    wg.Wait()&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;channels&#34;&gt;Channels&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;goroutine是并发单元，可以通channels实现他们之间的通信&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;创建一个channel&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;ch := make(chan int)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;channel是一个底层数据结构的引用，当作为参数传递是，是传递的一个引用&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;两个channel可以用==运算比较，如果两个channel引用的是相同的对象，那么比较的结果为真，channel的零值是nil&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;关闭一个channel后，继续发送会产生panic，但是可以继续接收，此时将不会阻塞，立即返回一个零值&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;没有办法直接测试一个channel是否被关闭，但是接收操作有一个变体形式，它可以多接收一个结果，多接收的第二个结果是一个布尔值Ok, true表示成功从channel接收到了值，false便是已经关闭 并且里面没有值可以接收&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;x,ok := &amp;lt;- ch&#xA;if !ok{&#xA;    //do something&#xA;}&#xA;//使用range简化&#xA;for x := range ch{&#xA;    //do something&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;channel 可以不用显式的关闭，当他没有被引用时go语言的垃圾回收会自动回收&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;单向channel&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;chan&amp;lt;- int 只用来发送。 向channel发送数据&lt;/li&gt;&#xA;&lt;li&gt;&amp;lt;- chan int 只接收。接收channel的数据&lt;/li&gt;&#xA;&lt;li&gt;关闭channel用于断言不再向channel发送数据，因此对于一个只接收的channel调用close将是一个编译错误&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;可以将一个双向的channel赋值给单向channel变量，会做隐式的转换。但是不能将一个单向的channel转换为双向的channel&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;基于select的多路复用&#34;&gt;基于select的多路复用&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;select会等待case中能够执行的case时去执行，当条件满足时，才会去通信并执行case之后的语句&lt;/li&gt;&#xA;&lt;li&gt;一个没有任何case的select语句写作select{}，会永远等待下去&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果多个case同时就绪，select会随机选择一个执行，这样来保证每一个channel都有相等的被select执行的机会,下面的例子中channel的缓冲设为1，这样每次循环的时候ch的状态为空或者满，偶数的时候恰好为空，奇数时候为满，所以输出0，2，4，6，8。如果缓冲区设为大于1的数，那么select就会随机选择，结果就不确定了。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {&#xA;    ch := make(chan int, 1)&#xA;    for i := 0; i &amp;lt; 10; i++ {&#xA;        select {&#xA;        case x := &amp;lt;-ch:&#xA;            fmt.Println(x)&#xA;        case ch &amp;lt;- i:&#xA;            //&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;Time.Tick函数表现的像是它创建一个在循环中调用time.Sleep的goroutine每次被唤醒时发送一个事件，依然会不断的尝试向channel中发送值，如果没有接受方去接受，那么就会造成goroutine泄露，因此只有当程序整个生命周期都需要这个时间时我们使用它才比较合适，否则建议使用下面的模式：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;ticker := time.NewTicker(1 * time.Second)&#xA;&amp;lt;-ticker.C    // receive from the ticker&#39;s channel&#xA;ticker.Stop() // cause the ticker&#39;s goroutine to terminate&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;对一个nil的channel发送和接受操作会永远阻塞，在select语句中操作nil的channel永远都不会被select到&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;实例-并发的目录遍历&#34;&gt;实例 - 并发的目录遍历&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;下面是一个并发遍历目录的实例&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;&#xA;import (&#xA;    &amp;quot;flag&amp;quot;&#xA;    &amp;quot;fmt&amp;quot;&#xA;    &amp;quot;io/ioutil&amp;quot;&#xA;    &amp;quot;os&amp;quot;&#xA;    &amp;quot;path/filepath&amp;quot;&#xA;    &amp;quot;sync&amp;quot;&#xA;    &amp;quot;time&amp;quot;&#xA;)&#xA;&#xA;func walkDir(dir string, wg *sync.WaitGroup, filesize chan&amp;lt;- int64) {&#xA;    defer wg.Done()&#xA;    for _, entry := range dirents(dir) {&#xA;        if entry.IsDir() {&#xA;            wg.Add(1)&#xA;            subdir := filepath.Join(dir, entry.Name())&#xA;            go walkDir(subdir, wg, filesize)&#xA;        } else {&#xA;            filesize &amp;lt;- entry.Size()&#xA;        }&#xA;    }&#xA;}&#xA;&#xA;var sema = make(chan struct{}, 20) //最多打开20个目录&#xA;&#xA;func dirents(dir string) []os.FileInfo {&#xA;    sema &amp;lt;- struct{}{}&#xA;    defer func() { &amp;lt;-sema }()&#xA;    entries, err := ioutil.ReadDir(dir)&#xA;    if err != nil {&#xA;        fmt.Fprintf(os.Stderr, &amp;quot;du1: %v\n&amp;quot;, err)&#xA;        return nil&#xA;    }&#xA;    return entries&#xA;}&#xA;&#xA;var verbose = flag.Bool(&amp;quot;v&amp;quot;, false, &amp;quot;show verbose progress messages&amp;quot;)&#xA;&#xA;func main() {&#xA;    flag.Parse()&#xA;    roots := flag.Args()&#xA;    if len(roots) == 0 {&#xA;        roots = []string{&amp;quot;.&amp;quot;}&#xA;    }&#xA;    filesizes := make(chan int64)&#xA;    var wg sync.WaitGroup&#xA;    for _, dir := range roots {&#xA;        wg.Add(1)&#xA;        go walkDir(dir, &amp;amp;wg, filesizes)&#xA;    }&#xA;&#xA;    go func() {&#xA;        wg.Wait()&#xA;        close(filesizes)&#xA;    }()&#xA;    var tick &amp;lt;-chan time.Time&#xA;    if *verbose {&#xA;        tick = time.Tick(500 * time.Millisecond)&#xA;    }&#xA;    var nfiles, nsize int64&#xA;loop:&#xA;    for {&#xA;        select {&#xA;        case size, ok := &amp;lt;-filesizes:&#xA;            if !ok {&#xA;                break loop&#xA;            }&#xA;            nfiles++&#xA;            nsize += size&#xA;        case &amp;lt;-tick:&#xA;            fmt.Printf(&amp;quot;%d files %1.fG\n&amp;quot;, nfiles, float64(nsize)/1e9)&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;并发的退出&#34;&gt;并发的退出&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Go语言不提供在一个goroutine里终止另一个goroutine的方法，由于这样会导致goroutine之间共享变量落在未定义的状态上。&lt;/li&gt;&#xA;&lt;li&gt;可以通过关闭一个channel进行广播关闭所有goroutine&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;一个简单的聊天程序&#34;&gt;一个简单的聊天程序&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KDF5000/gopl/blob/master/src/ch8/chat.go&#34;&gt;聊天服务器&lt;/a&gt;+&lt;a href=&#34;https://github.com/KDF5000/gopl/blob/master/src/ch8/echoclient.go&#34;&gt;客户端&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sun, 07 May 2017 20:36:18 +0000</pubDate>
    </item>
    <item>
      <title>一致性哈希算法</title>
      <link>http://openhex.cn/2017/4/30/一致性哈希算法.html</link>
      <description>&lt;p&gt;前面专门介绍过几种&lt;a href=&#34;http://kdf5000.com/2017/04/17/常见的几种Sharding策略/&#34;&gt;常见的数据分片(Sharding)&lt;/a&gt;,主要包括范围分片(Range)和哈希(Hash)分片两种大的策略，其中哈希分片最简单的Round Robin方法(直接按照机器数取模)存在一个明显的缺点，当机器数增加或者减少的时候，所有的数据都要进行重新的哈希分配。这个问题的本质原因是因为机器和数据哈希分布之间强耦合，因此针对这个缺陷主要有两个解决方案一种是引入虚拟桶的概念，分两步映射(key-partition和partition-machine)，另一种方案就是一致性哈希，引入哈希空间(将key-partition和partition-machine映射到同一个哈希空间)，解除了机器和数据分布的耦合关系。这两天看了《大数据日知录》第一章的数据分片与路由，这里主要讲一下一致性哈希的实现原理。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;整体架构&#34;&gt;整体架构&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;一致性哈希是分布式哈希表的一种实现，主要有Chord(和弦)系统提出，其变种也广泛应用于诸如Dynamo和Cassandra等分布式存储系统。与虚拟桶的初衷相同，都是为了解决机器和数据分布的耦合关系，但是一致性哈希则对key-partition和partition-machine使用同一个哈希函数，这样就可以将机器和数据分片都在同一个空间，每一个机器负责规定范围的数据分片。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;一致性哈希算法将哈希空间抽象成一个环状结构，比如一个长度为5的二进制空间(可以表示0-31的空间)，在空间结构上首尾相连，组成一个环状序列。对于数据根据Key值哈希到这个空间，对于机器同样根据根据一定的key(比如ip+port)映射到同一个空间，这样机器就相当于将整个空间截成了几段，每个机器节点负责一段，&lt;strong&gt;同时每个节点记录其前驱节点和后继节点&lt;/strong&gt;。下图就是一个长度为5的哈希空间，有五个节点，五个节点的哈希值分别是5,14,20,25,29，每个节点负责($N_p$,$N_n$]，其中$N_p$是前一个节点的哈希值，$N_n$ 是当前节点的哈希值。比如对于节点 $N_5$其哈希值是5，其前一个节点是$N_{29}$，其哈希值是29，所以节点$N_5$负责的哈希空间是(29,5]，也就是30-5,所有哈希值在这个范围的key都归节点$N_5$管理。由于节点的哈希值比较随机，可能映射到哈希空间排列比较密集，导致负载不均衡，后面会讨论解决方法。&#xA;&lt;img src=&#34;/media/archive/blog/images/一致性哈希算法.png&#34; alt=&#34;一致性哈希算法.png&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;查找key&#34;&gt;查找key&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;上一节可以看到一致性哈希算法没有统一的控制节点，也就是说对于客户端来说并不知道需要请求那个节点去获得某个key的值，也不能通过一个主控节点去获得某个key的存储节点。这也正是一致性哈希广泛用于分布式系统的重要原因，不需要主控节点，也就不存在单点问题，同时又有良好的可扩展性。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;虽然没有主控节点，但是客户端知道所有节点，可以向任何一台节点发送请求，前面我们说过每个节点都知道其前驱节点和后继节点，所以任何一个收到请求的节点会首先通过哈希函数获得请求key的哈希值，假设为j，判断是否在其负责的范围如果没有则将其转发给后继节点查找，如此循环直到找到一个节点，其负责范围包括j(即上图中节点下角大于等于j的最小编号节点)。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;如果按照这样的方法查找显然效率很低，一个请求如果查找这么久才返回，延迟也太高了。所以为了加快速度，一致性哈希算法在每个节点配置了一个路由表，存储了m条(前面提到的二进制哈希空间长度，比如前面的5)路由信息。分别是比当前节点多$2^i$($0\leq i \leq (m-1)$)的五个哈希值所在的节点编号，还是前面例子为例说明,下面是节点$N_{14}$存储的路由表:&lt;/p&gt;&#xA;&#xA;&lt;table&gt;&#xA;&lt;thead&gt;&#xA;&lt;tr&gt;&#xA;&lt;th align=&#34;left&#34;&gt;距离&lt;/th&gt;&#xA;&lt;th align=&#34;left&#34;&gt;1($2^0$)&lt;/th&gt;&#xA;&lt;th align=&#34;left&#34;&gt;2($2^1$)&lt;/th&gt;&#xA;&lt;th align=&#34;left&#34;&gt;4($2^2$)&lt;/th&gt;&#xA;&lt;th align=&#34;left&#34;&gt;8($2^3$)&lt;/th&gt;&#xA;&lt;th align=&#34;left&#34;&gt;16($2^4$)&lt;/th&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/thead&gt;&#xA;&#xA;&lt;tbody&gt;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;left&#34;&gt;节点编号&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;$N_{20}$&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;$N_{20}$&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;$N_{20}$&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;$N_{25}$&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;$N_{5}$&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&#xA;&lt;p&gt;有了路由表，如果客户端要请求的key不在改节点，怎么可以直接根据该表尽可能快的找到目标节点。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;一致性哈希路由算法&#34;&gt;一致性哈希路由算法&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;假设要查找key的哈希值为hash(key)=j，初始请求的节点为$N_c$，其后继节点为$N_s$ ，$N_c$节点负责的哈希空间范围是(i,c]，按照下面的算法查找：&#xA;1. 首先$N_c$判断是否$j\epsilon(i,c]$，如果为真，说明key确实存在$N_c$节点，查询key的value，直接返回&#xA;2. 如果key不在$N_c$节点，则判断j是否属于(c,s]，如果存在，则说明key在$N_c$的后继节点$N_s$上，$N_c$向$N_s$发送消息，查找key的值，然后返回给$N_c$(每个消息都包含$N_c$的相关信息)。&#xA;3. 如果key不在后继节点，这时就需要查找路由表(&lt;strong&gt;这里的路由表示距离为1开始所以存储了后继节点的信息，和第二步有点重复，个人感觉也许可以不用存储后继节点的路由信息进行优化&lt;/strong&gt;)，找到小于j的最大编号节点(如果所有路由表都大于j，则选择最后一项路由信息作为下一个查询节点)作为查询节点，$N_c$向其发送消息查询key的值，该节点成为新的$N_c$节点，继续按照上述步骤查找。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;还是以上面的例子为例，假设初始查询的节点是$N_5$，想要查询的key的哈希值为27，按照步骤一$N&lt;em&gt;5$的负责哈希空间为[30,5]，27不在这个范围；进入步骤二，查看其后继节点N14的范围位[6,14]，也不在这个范围；进入步骤三查询路由表，找到小于27的最大编号节点是$N&lt;/em&gt;{25}$(14+8=22&amp;lt;27&amp;lt;14+16=30),于是发送该请求到$N&lt;em&gt;{25}$，$N&lt;/em&gt;{25}$进入步骤一，发现不在其负责范围，然后进入步骤二发现在其后继节点$N&lt;em&gt;{29}$的负责范围[26,29]，所以将请求再次转发到$N&lt;/em&gt;{29}$，$N&lt;em&gt;{29}$查询结果返回给$N&lt;/em&gt;{5}$.&#xA;&lt;img src=&#34;/media/archive/blog/images/一致性哈希路由.png&#34; alt=&#34;一致性哈希路由.png&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;节点变更&#34;&gt;节点变更&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;一致性哈希算法一般应用于分布式的环境，避免不了新增加节点，以及节点宕机的故障。一致性哈希算法也同样设计了一套算法用于增加，减少节点。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;增加节点&#34;&gt;增加节点&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;如果现有的哈希空间中新增加一个节点，假设为$N&lt;em&gt;{new}$，则$N&lt;/em&gt;{new}$必须能够被安插到合适的位置，并且能够和其他节点建立联系。通过上一节介绍的路由查找算法，拿着新节点的哈希值new, 可以很容易地找到新节点哈希值所在的节点，假设为$N_s$, 其前驱节点为$N_p$, 则新节点就是要插入到节点$N_p$和$N_s$之间，$N&lt;em&gt;s$的前驱节点重新指向$N&lt;/em&gt;{new}$，$N_{new}$的后继节点指向$N_s$, $N&lt;em&gt;p$的后继节点指向$N&lt;/em&gt;{new}$，并完成数据片的转移，这里就是将$N&lt;em&gt;s$多余的数据转移到$N&lt;/em&gt;{new}$上。&#xA;为了保证多个节点加入的时候不出现问题，一致性哈希提供了一套稳定性检测的算法去保证新节点的加入和离开。每个节点都会周期性的执行稳定性检测算法，保证目前节点的连接关系的正确性。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;稳定性检测算法&#34;&gt;稳定性检测算法&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;在新加入节点的时候，首先安装前面说的找到新节点的后继节点，也就是当前新节点哈希值所在的节点$N_s$，然后将新节点的后继节点指向$N_s$。然后在执行稳定性检测算法的时候自动更新节点的关系，新加入的节点也就进入了新的网络结构中。&#xA;稳定性检测算法的具体流程如下:&#xA;1. 假设$N_c$的后继节点为$N_s$，$N_c$向$N_s$询问其前驱节点，假设收到的回复是$N_p$&#xA;2. 如果$N_p$位于$N_c$和$N_s$之间，$N_c$记录下$N_p$为其后继节点&#xA;3. 假设$N_c$的后继节点为$N_x$,这里$N_x$可能是$N_s$和$N_p$。如果$N_x$的前驱节点为空，或者$N_c$位于$N_x$和它的前驱节点之间，那么$N_c$给$N_x$发送消息告诉$N_x$说$N_c$就是它的前驱节点，$N_x$将其前驱节点置为$N_c$。&#xA;4. $N_x$把部分数据迁移到$N_c$，即将$N_x$上哈希值小于c的记录迁移到$N_c$上。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;还是以前面的例子为例，加入一个新的节点$N&lt;em&gt;8$，其哈希值是8，在当前的哈希空间中落到了节点$N&lt;/em&gt;{14}$，则此时将节点$N&lt;em&gt;8$的后继节点指向$N&lt;/em&gt;{14}$，前驱节点置位空，这个时候是下面的状态:&#xA;&lt;img src=&#34;/media/archive/blog/images/加入新节点.png&#34; alt=&#34;加入新节点.png&#34; /&gt;&#xA;之后节点$N_8$进行稳定性检测的时候，这个时候$N_c=N_8, N_s=N_14, N_p=N_5$，此时$N&lt;em&gt;8$向$N&lt;/em&gt;{14}$询问其前驱节点，此处为$N_5$，不满足步骤二，因此$N&lt;em&gt;x$为$N&lt;/em&gt;{14}$，按照步骤三$N_8$位于$N&lt;em&gt;5$和$N&lt;/em&gt;{14}$之间，因此将$N_{14}$的前驱节点设置为$N_8$，并将[[6,8]的数据迁移到$N_8$。自此$N_8$稳定性检测完成，此时的状态如下:&#xA;&lt;img src=&#34;/media/archive/blog/images/加入新节点中.png&#34; alt=&#34;加入新节点中.png&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;之后当节点$N&lt;em&gt;5$进行稳定检测的时候，发现其后继节点$N&lt;/em&gt;{14}$的前驱节点为$N_8$，满足步骤二位于节点$N&lt;em&gt;5$和$N&lt;/em&gt;{14}$之间。因此将$N_5$的后继节点指向$N_8$，到步骤三的时候$N_x$此时为$N_8$，其前驱节点为空，因此$N_5$给$N_8$发消息告诉其$N_8$其前驱节点为$N_5$，这时$N_8$的前驱节点指向$N_5$。这个时候$N_8$上的数据全都大于5，所以不需要进行数据迁移。这个时候已经完成了节点的加入工作，最后的状态如下:&#xA;&lt;img src=&#34;/media/archive/blog/images/加入新节点下.png&#34; alt=&#34;加入新节点下.png&#34; /&gt;&#xA;当然这个时候还不算完成，加入节点后，部分节点的路由表已经不再适用，除了稳定性检测，每个节点也会周期性进行路由表的更新检测，用来更新节点的路由表。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;节点的离开&#34;&gt;节点的离开&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;节点的离开分为两种，正常离开和异常离开，对于正常离开的节点，可以自动更新前驱和后继节点，并转移数据。对于异常离开的节点，通常都是由于机器的故障，这时就会导致数据的丢失，因此通常会在节点的后继节点中保留多份副本保证数据的安全。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;虚拟节点&#34;&gt;虚拟节点&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;为了解决刚开始提到哈希的随机导致的节点负载不均衡，可以使用虚拟节点的方法解决，具体稍微复杂一点可以参考&lt;a href=&#34;http://blog.csdn.net/bluishglc/article/details/52847591&#34;&gt; Cassandra的一致性哈希(Consistent Hashing)和虚拟节点（Virtual Nodes）的关系&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;参考资料&#34;&gt;参考资料&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;《大数据日知录》第一章数据分片与路由&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.cnblogs.com/yanghuahui/p/3755460.html&#34;&gt;分布式缓存负载均衡的规则处理：虚拟节点对一致性哈希的改进&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://blog.csdn.net/bluishglc/article/details/52847591&#34;&gt; Cassandra的一致性哈希(Consistent Hashing)和虚拟节点（Virtual Nodes）的关系&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
      <pubDate>Sun, 30 Apr 2017 21:34:43 +0000</pubDate>
    </item>
    <item>
      <title>Golang第七章:接口</title>
      <link>http://openhex.cn/2017/4/24/Golang第七章-接口.html</link>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;接口更像是一种约定，范式满足接口约定的形式的类型都可以作为该接口的实例&lt;/li&gt;&#xA;&lt;li&gt;接口类型具体描述了一系列方法的集合，一个实现了这些方法的具体类型是这个接口类型的实例&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;接口类型可以进行组合成新的接口类型，这种也叫接口内嵌&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package io&#xA;type Reader interface {&#xA;Read(p []byte) (n int, err error)&#xA;}&#xA;type Closer interface {&#xA;Close() error&#xA;}&#xA;//&#xA;type ReadWriter interface {&#xA;Reader&#xA;Writer }&#xA;type ReadWriteCloser interface {&#xA;Reader&#xA;Writer&#xA;Closer }&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个类型如果拥有一个接口需要的所有方法，那么这个类型就实现了这个接口&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;表达一个类型属于某个接口只要这个类型实现了这个接口&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var w io.Writer&#xA;w = os.Stdout&#xA;w = new(bytes.Buffer)&#xA;w = time.Second&#xA;// OK: *os.File has Write method&#xA;// OK: *bytes.Buffer has Write method&#xA;// compile error: time.Duration lacks Write method&#xA;var rwc io.ReadWriteCloser&#xA;rwc = os.Stdout         // OK: *os.File has Read, Write, Close methods&#xA;rwc = new(bytes.Buffer) // compile error: *bytes.Buffer lacks Close method&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;一个具体类型赋值给一个接口类型的变量后，即使该具体类型有其他方法也不能调用&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;interface{}空类型对实现它的类型没有任何要求，所以可以将任意一个值赋给空接口类型&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;flag.Value可以为自己的数据类型位flag添加新的标记符号&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;sort-interface接口&#34;&gt;sort.Interface接口&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;go语言的sort函数不会对具体的序列和它的元素做任何的假设&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;排序算法通常需要知道的三个要素&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;序列的长度 --Len() int&lt;/li&gt;&#xA;&lt;li&gt;两个元素比较的结果 - Less(i int, j int)bool&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;交换两个元素的方式 -- Swap(i,j int)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;///&#xA;package sort&#xA;type Interface interface {&#xA;Len() int&#xA;Less(i, j int) bool // i, j are indices of sequence elements&#xA;Swap(i, j int)&#xA;}&#xA;///////&#xA;type StringSlice []string&#xA;func (p StringSlice) Len() int           { return len(p) }&#xA;func (p StringSlice) Less(i, j int) bool { return p[i] &amp;lt; p[j] }&#xA;func (p StringSlice) Swap(i, j int)      { p[i], p[j] = p[j], p[i] &#xA;//调用&#xA;sort.Sort(StringSlice(names))&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;### http.Handler接口&#xA;* 在package http里&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package http&#xA;type Handler interface {&#xA;    ServeHTTP(w ResponseWriter, r *Request)&#xA;}&#xA;func ListenAndServe(address string, h Handler) error&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;一个最简单的例子&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {&#xA;db := database{&amp;quot;shoes&amp;quot;: 50, &amp;quot;socks&amp;quot;: 5}&#xA;log.Fatal(http.ListenAndServe(&amp;quot;localhost:8000&amp;quot;, db))&#xA;}&#xA;type dollars float32&#xA;func (d dollars) String() string { return fmt.Sprintf(&amp;quot;$%.2f&amp;quot;, d) }&#xA;type database map[string]dollars&#xA;func (db database) ServeHTTP(w http.ResponseWriter, req *http.Request) { for item, price := range db {&#xA;    fmt.Fprintf(w, &amp;quot;%s: %s\n&amp;quot;, item, price)&#xA;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果要想对不同url执行不同的操作，则可以在ServeHttp里通过req.URL.Path获得请url然后通过switch等方式判断执行不同的操作&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;go也提供了请求多路ServeMux来简化URL和handler的对应&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {&#xA;db := database{&amp;quot;shoes&amp;quot;: 50, &amp;quot;socks&amp;quot;: 5}&#xA;mux := http.NewServeMux()&#xA;mux.Handle(&amp;quot;/list&amp;quot;, http.HandlerFunc(db.list))&#xA;mux.Handle(&amp;quot;/price&amp;quot;, http.HandlerFunc(db.price))&#xA;log.Fatal(http.ListenAndServe(&amp;quot;localhost:8000&amp;quot;, mux))&#xA;}&#xA;type database map[string]dollars&#xA;func (db database) list(w http.ResponseWriter, req *http.Request) {&#xA;for item, price := range db {&#xA;    fmt.Fprintf(w, &amp;quot;%s: %s\n&amp;quot;, item, price)&#xA;}&#xA;}&#xA;func (db database) price(w http.ResponseWriter, req *http.Request) {&#xA;item := req.URL.Query().Get(&amp;quot;item&amp;quot;)&#xA;price, ok := db[item]&#xA;if !ok {&#xA;    w.WriteHeader(http.StatusNotFound) // 404&#xA;    fmt.Fprintf(w, &amp;quot;no such item: %q\n&amp;quot;, item)&#xA;    return&#xA;}&#xA;fmt.Fprintf(w, &amp;quot;%s\n&amp;quot;, price)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;其中http.HandlerFunc其实是一个实现了http.Handler ServeHttp方法的函数类型&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package http&#xA;type HandlerFunc func(w ResponseWriter, r *Request)&#xA;func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) {&#xA;f(w, r)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果有很多handler，有可能需要放在不同的文件中，这个时候可以使用http包提供的默认的ServeMux示例DefaultServeMux和包级http.Handle和http.HandleFunc函数， 这个时候ListenAndServe函数的handler值为空就可以&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func main() {&#xA;db := database{&amp;quot;shoes&amp;quot;: 50, &amp;quot;socks&amp;quot;: 5}&#xA;http.HandleFunc(&amp;quot;/list&amp;quot;, db.list)&#xA;http.HandleFunc(&amp;quot;/price&amp;quot;, db.price)&#xA;log.Fatal(http.ListenAndServe(&amp;quot;localhost:8000&amp;quot;, nil))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;error接口&#34;&gt;error接口&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;error接口&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type error interface {&#xA;Error() string&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;errors包的error的一个实现&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package errors&#xA;func New(text string) error { return &amp;amp;errorString{text} }&#xA;type errorString struct { text string }&#xA;func (e *errorString) Error() string { return e.text }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;类型断言&#34;&gt;类型断言&lt;/h3&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;x.(T)判断x是否是T类型，如果成功则返回x的的动态值，如果失败则跑出panic&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
      <pubDate>Mon, 24 Apr 2017 16:34:48 +0000</pubDate>
    </item>
    <item>
      <title>常见的几种Sharding策略</title>
      <link>http://openhex.cn/2017/4/17/常见的几种Sharding策略.html</link>
      <description>&lt;h3 id=&#34;range&#34;&gt;Range&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;range假设key有序，好处是临近的key经常在一起，比如共同前缀的key,可以很好的支持scan操作，hbase的region就是range策略。缺点是对压力较大的顺序写不太友好，比如日志类型的写入，一般日志的key都是和时间相关的，时间是单调递增的，因此写入的热点永远在最后一个region。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;但是对于关系型的数据库，因为经常性的需要&lt;strong&gt;表扫描或者索引扫描&lt;/strong&gt;，基本上都会使用range的shard策略。&#xA;&lt;img src=&#34;http://static.zybuluo.com/zyytop/5h4vfs0g6t7y3609lbuslozw/屏幕快照_2016-10-13_下午4.40.22.png&#34; alt=&#34;&#34; /&gt;&#xA;&lt;em&gt;图片来自PingCAP博客&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;hash&#34;&gt;Hash&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Hash的策略是将key先做一个hash,然后得到的hash值作为sharding ID, 这样每一个Key的分布都是随机的，因此可以任务是均匀分布的，这样对于写压力较大的系统非常友好，同事随机的读因为会将读的压力均匀分布在不同的节点，因此也是非常友好的。但是对于需要scan的操作几乎是不可能的。&#xA;&lt;img src=&#34;http://static.zybuluo.com/zyytop/8kaltq5ww337kgxq63asdbnz/屏幕快照_2016-10-19_下午6.14.37.png&#34; alt=&#34;&#34; /&gt;&#xA;&lt;em&gt;图片来自PingCAP博客&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;round-robin&#34;&gt;Round Robin&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;这种hash是最简单的其实也是上面描述的Hash的最朴素的实现方式，Hash首先一般是讲数据进行分片，如果分片后每一片直接对应一个物理节点，一般的实现是直接在hash函数设计的时候将物理节点个数考虑进去，比如对物理节点的个数取余。&#xA;$$H(key) = hash(key) \%  k$$&#xA;这样H(key)的值即为物理节点的编号，这种方式因为讲数据分片和分片与物理节点映射的功能合二为一，因此新增加或者减少一个物理节点re-hash的代价非常高。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;对于这个问题有多种解决方案，一种就是想办法解除合二为一的哈希功能，虚拟桶就是这种思路，另一种是一致性哈希所采用的方法。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;虚拟通-virtual-buckets&#34;&gt;虚拟通(Virtual Buckets)&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;虚拟桶的方法就是为了解决上面提到的Round Robin的哈希方法的扩展性问题，其实也是很简单的，就是讲hash的映射分为两部分，首先 根据key做个hash映射到一个虚拟桶(数据分片)，然后维护一个从虚拟桶到物理节点的映射表。当新增加一个节点的时候，将某些虚拟桶从原先的物理节点移动到新的物理节点，然后更新映射表即可，不需要重新对所有的key进行hash.&#xA;&lt;img src=&#34;/media/archive/blog/images/sharding.png&#34; alt=&#34;sharding.png&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;一致性哈希&#34;&gt;一致性哈希&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Cassandra和Twemproxy都是使用了一致性哈希的方法，这个讲起来有点复杂，详情看[待定]()&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;pre-sharding&#34;&gt;Pre-sharding&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Pre-sharding其实就是前面提到的虚拟桶的方法，只要经过两次hash都可以认为是pre-sharding的方式。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;动态扩展&#34;&gt;动态扩展&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;range在动态扩展方面可以通过分裂的方式将一个大的shard分裂成不同大小的shard然后做shard的迁移，但是对于hash来说就必须来进行re-hash，这样的代价是非常大的，比如添加一个物理节点，此时hash的模如果是3，则添加后变成了4, 对于已有系统的抖动非常大。 虽然一致性hash可以一定程度降低系统的抖动，但是并不能彻底避免。&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;参考资料&lt;/strong&gt;&#xA;1. &lt;a href=&#34;https://pingcap.com/blog-building-distributed-db-with-raft-zh&#34;&gt;基于 Raft 构建弹性伸缩的存储系统的一些实践&lt;/a&gt;&#xA;2. &lt;a href=&#34;http://engineering.xueqiu.com/blog/2014/12/26/redis-capacity-planning/&#34;&gt;Redis集群的数据划分与扩容探讨&lt;/a&gt;&#xA;3. 《大数据日知录》第一章：数据分片与路由&lt;/p&gt;&#xA;</description>
      <pubDate>Mon, 17 Apr 2017 00:00:38 +0000</pubDate>
    </item>
    <item>
      <title>Golang RPC 性能测试</title>
      <link>http://openhex.cn/2017/3/28/Golang-RPC-性能测试.html</link>
      <description>&lt;p&gt;最近刚好要使用Golang的RPC，因此对Golang标准库的RPC进行了一下测试，看看其性能到底如何。RPC服务端和客户端的实现完全使用RPC的&lt;code&gt;net/rpc&lt;/code&gt;标准库，没有经过特殊的优化，主要针对下面三个场景进行测试。测试之前需要先说明一下，Go的rpc连接是支持并发请求的，就是说一个一个连接可以并发的发送很多个请求，不像http协议一问一答的模式。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;测试环境&#34;&gt;测试环境&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;操作系统：Centos 6.8 (Linux 2.6.32)&#xA;内存：32G&#xA;核数：双CPU, 一共12核&#xA;CPU型号：Intel&amp;reg; Xeon&amp;reg; CPU E5645  @ 2.40GHz&#xA;Golang: 1.7.4&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;场景&#34;&gt;场景&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;测试的场景主要是下面两个指标&#xA;* QPS指标&#xA; * 单个个连接保证一个并发，随着该并发请求数增加，QPS的变化&#xA; * 单个连接(Client), 单个并发请求10w, 随着并发数的增加，QPS的变化&#xA; * 单个连接并发数固定(第一个测试的最优值)，增加连接数，QPS的变化&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;单机Server的并发数(同时连接数)&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;单机Server, 测试所能接收的连接数&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;QPS指标测试中，第一个设置是为了测试单个连接的并发数&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;实现&#34;&gt;实现&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Server端的实现使用tcp协议，监听4200端口，循环等待连接，每当检测到请求时，启动一个goroutine去处理该连接，注册的服务执行一个简单的乘法操作。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;//Service&#xA;type Args struct {&#xA;    A, B int&#xA;}&#xA;&#xA;type Quotient struct {&#xA;    Quo, Rem int&#xA;}&#xA;&#xA;type Arith int&#xA;&#xA;func (t *Arith) Multiply(args Args, reply *int) error {&#xA;    *reply = args.A * args.B&#xA;    return nil&#xA;}&#xA;&#xA;func (t *Arith) Divide(args Args, quo *Quotient) error {&#xA;    if args.B == 0 {&#xA;        return errors.New(&amp;quot;Divided by zero!&amp;quot;)&#xA;    }&#xA;    quo.Quo = args.A / args.B&#xA;    quo.Rem = args.A % args.B&#xA;    return nil&#xA;}&#xA;//////////////////////////////////////////////////&#xA;//Server&#xA;runtime.GOMAXPROCS(4)&#xA;arith := new(service.Arith)&#xA;server := rpc.NewServer()&#xA;log.Printf(&amp;quot;Register service:%v\n&amp;quot;, arith)&#xA;server.Register(arith)&#xA;&#xA;log.Printf(&amp;quot;Listen tcp on port %d\n&amp;quot;, 4200)&#xA;l, e := net.Listen(&amp;quot;tcp&amp;quot;, &amp;quot;:4200&amp;quot;)&#xA;&#xA;if e != nil {&#xA;    log.Fatal(&amp;quot;Listen error:&amp;quot;, e)&#xA;}&#xA;log.Println(&amp;quot;Ready to accept connection...&amp;quot;)&#xA;conCount := 0&#xA;go func() {&#xA;    for {&#xA;        conn, err := l.Accept()&#xA;        if err != nil {&#xA;            log.Fatal(&amp;quot;Accept Error:,&amp;quot;, err)&#xA;            continue&#xA;        }&#xA;        conCount++&#xA;        log.Printf(&amp;quot;Receive Client Connection %d\n&amp;quot;, conCount)&#xA;        go server.ServeConn(conn)&#xA;    }&#xA;}()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Client端的创建多个连接(Client), 然后每个连接指定不同的并发数，每个并发启动一个goroutine发送指定数量的request，该请求执行一个简单的乘法操作，最后统计整个过程的QPS。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;部署&#34;&gt;部署&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;QPS指标的测试使用两台上述配置的服务器，然后设置相关的内核参数，主要是允许最大打开的文件数，可使用的端口范围，tcp缓存大小等,操作如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sysctl -w fs.file-max=10485760 #系统允许的文件描述符数量10m&#xA;sysctl -w net.ipv4.tcp_rmem=1024 #每个tcp连接的读取缓冲区1k，一个连接1k&#xA;sysctl -w net.ipv4.tcp_wmem=1024 #每个tcp连接的写入缓冲区1k&#xA;&#xA;sysctl -w net.ipv4.ip_local_port_range=&#39;1024 65535&#39; #修改默认的本地端口范围&#xA;sysctl -w net.ipv4.tcp_tw_recycle=1  #快速回收time_wait的连接&#xA;sysctl -w net.ipv4.tcp_tw_reuse=1&#xA;sysctl -w net.ipv4.tcp_timestamps=1&#xA;&#xA;#用户单进程的最大文件数，用户登录时生效&#xA;echo &#39;* soft nofile 1048576&#39; &amp;gt;&amp;gt; /etc/security/limits.conf &#xA;echo &#39;* hard nofile 1048576&#39; &amp;gt;&amp;gt; /etc/security/limits.conf &#xA;ulimit -n 1048576 #用户单进程的最大文件数 当前会话生效&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;对于Server连接数的测试，使用多台机器，尽量保证同时进行连接同一个server&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;测试结果&#34;&gt;测试结果&lt;/h3&gt;&#xA;&#xA;&lt;h4 id=&#34;qps指标&#34;&gt;QPS指标&lt;/h4&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;单个连接保证一个并发，随着该一个并发请求数增加，QPS的变化&#xA;&lt;img src=&#34;/media/archive/blog/images/单个连接请求数.png&#34; alt=&#34;单个连接请求数.png&#34; /&gt;&#xA;这个测试可以评估单个连接单并发的情况下，能过处理的最多请求数。随着请求数的增加，QPS一直下降，但是下降到6k左右的时候讲保持这个值不再改变，从图中可以大概在请求量大概在10k以后qps将不再改变，也就是说按照现在这个频率(无休止持续请求),client的处理能力大概就是6k/s，可以根据这个指标控制请求的频率。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;单个连接(Client), 单个并发请求10w, 随着并发数的增加，QPS的变化&#xA;&lt;img src=&#34;/media/archive/blog/images/单个连接并发数.png&#34; alt=&#34;单个连接并发数.png&#34; /&gt;&#xA;这个测试可以指导客户端在设计单个连接的并发请求数时候怎么选择最佳的并发数。从测试结果可以看出，单个连接随着并发数的增加，QPS(RPS)并不是线性增长的，基本上增加到100个并发数就基本不再增加了，说明对于单个连接最大的QPS大概是6w多点。而且对于单个连接来说，并发数最好不要超过150。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;单个连接并发数固定(第一个测试的最优值)，增加连接数，QPS的变化&#xA;&lt;img src=&#34;/media/archive/blog/images/固定并发连接数.png&#34; alt=&#34;固定并发连接数.png&#34; /&gt;&#xA;这里设置单个连接数的并发数为100，每个并发的请求量位10w, 从结果可以看出随着连接数的增加，Qps也是在不断增加的,但是当增加到100个连接数后，Qps基本不再变化，维持在47w左右，这个值相对还是比较大的，这说明Go的RPC client和Server的性能还是不错的。&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h4 id=&#34;单机server的并发数-同时连接数&#34;&gt;单机Server的并发数(同时连接数)&lt;/h4&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;单机Server, 测试同时能接收的连接数&#xA;因为这个涉及到多台部署了Client的服务器进行同时请求，暂时手里服务器资源还不够，过段时间腾出来一些机器了补充起来，&lt;strong&gt;如果有单机可以模拟(单机局限于端口数)的方案望转告&lt;/strong&gt;。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;上面的前三个实验主要关注Client端的性能，因为自己对服务端的压测指标，业务场景还不是非常明确，所以对Server端的压测试还不够充分，以后会逐步的补充起来。从测试结果来看，Go RPC的Client的性能还是不错的。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;单连接的满负载Qps大概是6k&lt;/li&gt;&#xA;&lt;li&gt;单连接的并发数最好不要超过150&lt;/li&gt;&#xA;&lt;li&gt;单个机器的的连接数最好不要超过100&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
      <pubDate>Tue, 28 Mar 2017 16:44:27 +0000</pubDate>
    </item>
    <item>
      <title>Golang第十一章-测试</title>
      <link>http://openhex.cn/2017/3/28/Golang第十一章-测试.html</link>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;命令&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;go test&lt;/li&gt;&#xA;&lt;li&gt;遍历*_test.go 文件中复合规则的函数&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;类型&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;测试函数：Test开头用于测试程序逻辑行为的正确性&lt;/li&gt;&#xA;&lt;li&gt;基准测试函数：Benchmark开头，用于衡量函数的性能，go test会运行多次取平均的执行时间&lt;/li&gt;&#xA;&lt;li&gt;示例函数：Examole开头的函数，提供一个有编译器保证正确性的示例文档&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2 id=&#34;测试函数&#34;&gt;测试函数&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;导入的包和形式如下&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import &amp;quot;testing&amp;quot;&#xA;func TestName(t *testing){&#xA;//...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;go test -v参数可以打印每个测试函数的名字和运行时间&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;go test -run 对应一个正则表达式，只有测试函数名被它正确匹配的测试函数才会被go test测试命令运行。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;t.Error不能终止测试，t.Fatal可以终止测试&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;t.Fatal必须和测试函数在同一个groutine里调用才能终止测试&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;一般测试失败的信息形式为&lt;code&gt;f(x)=y, want z&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;随机测试&#34;&gt;随机测试&lt;/h3&gt;&#xA;&#xA;&lt;h3 id=&#34;白盒测试&#34;&gt;白盒测试&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;在测试一些诸如邮件发送函数的时候，我们并不想真正的发送邮件，因此可以将发送函数作为一个包级的私有函数值，然后在测试代码里先用修改该函数值，进行同等效果的测试&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;//fakefunc.go&#xA;package ch11&#xA;import (&#xA;&amp;quot;fmt&amp;quot;&#xA;)&#xA;var realFunc = func(data string) {&#xA;fmt.Printf(&amp;quot;Real Func:%s\n&amp;quot;, data)&#xA;}&#xA;func CheckInfo(data string) {&#xA;realFunc(data)&#xA;}&#xA;//&#xA;//fakefunc_test&#xA;package ch11&#xA;import (&#xA;&amp;quot;testing&amp;quot;&#xA;)&#xA;func TestFakeFunc(t *testing.T) {&#xA;var data string&#xA;saved := realFunc //保存原来的realfunc&#xA;defer func() { realFunc = saved }()&#xA;realFunc = func(d string) {&#xA;    data = d + &amp;quot;demo&amp;quot;&#xA;}&#xA;CheckInfo(&amp;quot;demo&amp;quot;)&#xA;if data != &amp;quot;demo&amp;quot; {&#xA;    t.Errorf(`data=%s want &amp;quot;demo&amp;quot;`, data)&#xA;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;测试扩展包&#34;&gt;测试扩展包&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;一个测试包想要使用，调用了该包的函数，这样讲形成一个包循环，go是不允许的，因此可以在被调用包的目录创建一个&lt;code&gt;&amp;lt;path&amp;gt;/&amp;lt;package&amp;gt;_test&lt;/code&gt;的目录。告诉go test工具应该建立一个额外的包运行测试。这样就可以在该包内倒入测试代码依赖的其他的辅助包&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;go list 可以查看哪些是Go源文件产品代码，哪些是包测试，哪些是测试扩展包。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;go list -f={ {.GoFiles} } fmt。GoFiles表示产品代码对应的Go源文件列表；也就是go build命令要编译的部分&lt;/li&gt;&#xA;&lt;li&gt;go list -f={ {.TestGoFiles} } fmt fmt。TestGoFiles表示包内部测试代码，以_test.go为后缀文件名，不过只是在测试时被构建&lt;/li&gt;&#xA;&lt;li&gt;go list -f={ {.XTestGoFiles} } fmt。XTestGoFiles表示属于测试扩展包的测试代码，也就是fmt_test&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;测试覆盖率&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;go test -coverprofile=c.out&lt;/li&gt;&#xA;&lt;li&gt;coverprofile参数通过在测试代码中插入生成钩子来统计覆盖率数据。也就是说在运行每个测试前。他会修改测试代码的副本，在每个词法块都会设置一个布尔标志变量。当被修改后的的被测试代码运行退出时，讲统计日志数据写入到c.out文件，并打印一部分执行的语句的一个总结。如果你需要的是摘要使用go test -cover&lt;/li&gt;&#xA;&lt;li&gt;-covermod=count可以在每个代码块插入计数器。统计每个块的执行次数，从而可以知道执行的热点代码&lt;/li&gt;&#xA;&lt;li&gt;go tool cover -html=c.out可以生成一个HTML报告&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2 id=&#34;基准测试&#34;&gt;基准测试&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Benchmark开头，参数是b *testing.B,其中b有一个变量N，用于指定操作循环的次数。系统自己指定,会自己调整&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import &amp;quot;testing&amp;quot;&#xA;func BenchmarkIsPalindrome(b *testing.B) {&#xA;for i := 0; i &amp;lt; b.N; i++ {&#xA;    IsPalindrome(&amp;quot;A man, a plan, a canal: Panama&amp;quot;)&#xA;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;循环放在测试函数内部，而不是放在测试框架内实现，这样可以让每个基准测试函数有机会在循环启动前执行初始化代码，这样并不会显著影响每次迭代的平均运行时间&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;go test -bench=. 需要通过bench标志指定要运行的基准测试函数。支持正则，默认是空。&lt;code&gt;.&lt;/code&gt;表示匹配所有基准测试函数&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;-benchmem命令行标志参数可以在报告中包含内存的分配数据统计&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;可以通过go tool pprof对操作系统信息进行采样，比如cpu,内存占用.可以参考&lt;a href=&#34;https://blog.golang.org/profiling-go-programs&#34;&gt;Profiling Go Programs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2 id=&#34;示例函数&#34;&gt;示例函数&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;以Example开头，没有参数&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;func ExampleIsPalindrome() {&#xA;fmt.Println(IsPalindrome(&amp;quot;A man, a plan, a canal: Panama&amp;quot;))&#xA;fmt.Println(IsPalindrome(&amp;quot;palindrome&amp;quot;))&#xA;// Output:&#xA;// true&#xA;// false&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;作为文档，godoc会根据示例函数的后缀名部分，将一个示例函数关联到某个具体函数或包本身&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;go test会运行示例函数，如果含有上面Output的注释，那么测试工具会比较输出结果和注释是否一致&lt;/li&gt;&#xA;&lt;li&gt;提供一个真实的演练场，像&lt;a href=&#34;http://golang.org&#34;&gt;http://golang.org&lt;/a&gt;一样就是有godoc提供的文档服务，他使用了Go Playground提高的技术让用户可以在浏览器中在线编辑和运行每个示例函数&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
      <pubDate>Tue, 28 Mar 2017 00:39:43 +0000</pubDate>
    </item>
    <item>
      <title>Golang第六章:方法</title>
      <link>http://openhex.cn/2017/3/15/Golang第六章-方法.html</link>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;对象其实就是属性和方法的集合，可以认为方法就是意义和特殊类型关联的函数&lt;/li&gt;&#xA;&lt;li&gt;在函数声明时，在其名字之前放上一个变量，即是一个方法。这个附加的参数会将该函数附加到这种类型上，即相当于这种类型定义了一个独占的方法。这个附加的参数叫接收器(Receiver)&lt;/li&gt;&#xA;&lt;li&gt;方法名不能和对象的字段名一样，编译会报错&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;除了常见的struct类型可以定义方法，slice类型也可以，其实任何类型都可以。这个和很多其他面向对象的语言不同&lt;/strong&gt;，看个例子&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main&#xA;import &amp;quot;math&amp;quot;&#xA;import &amp;quot;fmt&amp;quot;&#xA;type Point struct{ X, Y float64 }&#xA;// traditional function&#xA;// same thing, but as a method of the Point type&#xA;func (p Point) Distance(q Point) float64 {&#xA;return math.Hypot(q.X-p.X, q.Y-p.Y)&#xA;}&#xA;type Path []Point&#xA;func (p Path) Distance() float64 {&#xA;sum := 0.0&#xA;for i := range p {&#xA;    if i &amp;gt; 0 {&#xA;        sum += p[i-1].Distance(p[i])&#xA;    }&#xA;}&#xA;return sum&#xA;}&#xA;func main() {&#xA;perim := Path{&#xA;    {1, 1},&#xA;    {5, 1},&#xA;    {5, 4},&#xA;    {1, 1},&#xA;}&#xA;fmt.Println(perim.Distance()) // &amp;quot;12&amp;quot;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;方法相比函数有一个明显的好处就是可以很简短，尤其是在外部包使用的时候，方法可以省略包名，直接通过对象变量调用&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import &amp;quot;gopl.io/ch6/geometry&amp;quot;&#xA;perim := geometry.Path{{1, 1}, {5, 1}, {5, 4}, {1, 1}}&#xA;fmt.Println(geometry.PathDistance(perim)) // &amp;quot;12&amp;quot;, standalone function&#xA;fmt.Println(perim.Distance())             // &amp;quot;12&amp;quot;, method of geometry.Path&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;基于指针对象的方法&#34;&gt;基于指针对象的方法&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;如果一个函数需要更新一个变量，或者函数的其中一个参数实在太大我们希望能够避免进行这种默认的拷贝，这种情况下就需要用到指针了。对应到我们用来更新接收器的对象的方法，当这个接受者变量本身较大，我们就可以用其指针而不是用对象来声明方法。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (p *Point) ScaleBy(factor float64) {&#xA;    p.X *= factor&#xA;    p.Y *= factor&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一般约定如果某个类有一个指针作为接收器的方法，那么所有其他的方法都必须有一个指针接收器。&lt;/li&gt;&#xA;&lt;li&gt;只有类型和指向他们的指针，才能做接收器。&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果一个类型名本身就是一个指针的话，是不允许出现在接收器中的。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type P *int&#xA;func (P) f() { /* ... */ } // compile error: invalid receiver type&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;要想调用指针类型的方法，只要提供一个该类型的指针即可&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;r := &amp;amp;Point{1, 2}&#xA;r.ScaleBy(2)&#xA;fmt.Println(*r) // &amp;quot;{2, 4}&amp;quot;&#xA;//&#xA;p := Point{1, 2}&#xA;pptr := &amp;amp;p&#xA;pptr.ScaleBy(2)&#xA;fmt.Println(p) // &amp;quot;{2, 4}&amp;quot;&#xA;//&#xA;p := Point{1, 2}&#xA;(&amp;amp;p).ScaleBy(2)&#xA;fmt.Println(p) // &amp;quot;{2, 4}&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;但是go语言也是允许直接使用该类型的变量调用方法,编译器会隐式的帮我们用&amp;amp;p去调用, 但是这种简写也是有限制的,这种方法只适用于变量，也就是说能够保证编译器能够取到地址。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;p := Point{1, 2}&#xA;p.ScaleBy(2)&#xA;fmt.Println(p) // &amp;quot;{2, 4}&amp;quot;&#xA;//&#xA;Point{1, 2}.ScaleBy(2) // compile error: can&#39;t take address of Point literal&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;在每一个合法的方法调用表达式中，下面三种任意一种都是可以的&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;接收器的实际参数和其接受器的形式参数相同，比如两者都是类型T或者类型*T&lt;/li&gt;&#xA;&lt;li&gt;接收器的形参是T，实参是*T, 编译器会隐式的位我们解引用，取到指针指向的实际变量&lt;/li&gt;&#xA;&lt;li&gt;接收器的形参是*T, 实参是T，编译器会隐式的取变量的地址&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果类型T的所有方法都是类型T，而不是*T作为接收器，那么拷贝这种类型的实例就是安全的，调用他的任何一个方法也就会产生一个值得拷贝，如果是*T就要避免对其进行拷贝，这样可能会破坏该类型内部的不变性&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;总结&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不管你的方法的接收器是指针类型还是非指针类型，都是可以通过指针/非指针类型进行调用，编译器会帮你做类型转换&lt;/li&gt;&#xA;&lt;li&gt;在声明一个方法的接收器是指针还是非指针类型时， 需要考虑：第一，这个对象是不是特别大，如果声明为非知怎类型，调用就会产生一次拷贝，；第二，如果选择指针类型作为接收器，那么这种类型时钟执行同一块内存地址，拷贝只是一个别名&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;nil作为接收器&#34;&gt;nil作为接收器&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;对类型是map, slice这样的nil也可以作为接收器,比如标准库&lt;code&gt;net/url&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Values maps a string key to a list of values.&#xA;type Values map[string][]string&#xA;// Get returns the first value associated with the given key,&#xA;// or &amp;quot;&amp;quot; if there are none.&#xA;func (v Values) Get(key string) string {&#xA;    if vs := v[key]; len(vs) &amp;gt; 0 {&#xA;        return vs[0]&#xA;    }&#xA;    return &amp;quot;&amp;quot; &#xA;}&#xA;// Add adds the value to key.&#xA;// It appends to any existing values associated with key.&#xA;func (v Values) Add(key, value string) {&#xA;    v[key] = append(v[key], value)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;通过嵌入结构体来扩展类型&#34;&gt;通过嵌入结构体来扩展类型&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;一个结构体嵌套一个结构体，是对结构体的扩展，可以认为结构体拥有了嵌入结构体的属性和方法，可以当成自己的成员一样直接调用但是必须是作为匿名字段嵌入的类型，否则必须制定变量名调用。通过这种内嵌可以使我们定义字段特别多的复杂类型，我们可以将字段先按小类型分组，然后定义小类型的方法，之后再把他们组合起来。&lt;strong&gt;很像继承&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import &amp;quot;image/color&amp;quot;&#xA;type Point struct{ X, Y float64 }&#xA;type ColoredPoint struct {&#xA;Point //必须匿名，这样下面才可以直接想自己的成员一样调用&#xA;Color color.RGBA&#xA;}&#xA;var cp ColoredPoint&#xA;cp.X = 1&#xA;fmt.Println(cp.Point.X) // &amp;quot;1&amp;quot;&#xA;cp.Point.Y = 2&#xA;fmt.Println(cp.Y) // &amp;quot;2&amp;quot;&#xA;//&#xA;ed := color.RGBA{255, 0, 0, 255}&#xA;blue := color.RGBA{0, 0, 255, 255}&#xA;var p = ColoredPoint{Point{1, 1}, red}&#xA;var q = ColoredPoint{Point{5, 4}, blue}&#xA;fmt.Println(p.Distance(q.Point)) // &amp;quot;5&amp;quot; //必须显示的知名Point&#xA;p.ScaleBy(2)&#xA;q.ScaleBy(2)&#xA;fmt.Println(p.Distance(q.Point)) // &amp;quot;10&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;从实现的角度看，内嵌字段会指导编译器去生成额外的包装方法来委托已经声明好的方法&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func (p ColoredPoint) Distance(q Point) float64 {&#xA;return p.Point.Distance(q)&#xA;}&#xA;func (p *ColoredPoint) ScaleBy(factor float64) {&#xA;p.Point.ScaleBy(factor)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;内嵌的字段可以是指针类型的，这种情况下字段和方法会被间接地引入到当前的类型中，访问通过该指针指向的对象。这样就可以在不同的&lt;strong&gt;对象之间共享通用的数据结构&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;当解析器解析一个选择器方法时，，首先找直接定义在这个类型的方法，然后内嵌字段引入的方法，然后去找匿名的内嵌字段引入的方法，一直这么递归向下找，如果选择器有二义性的话就会报错，也就是不能再同一级里有两个同名的方法。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果两个匿名字段含有相同的方法名，不管其参数类型是否相同都是不能编译通过的，编译器无法确定是哪个方法&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;匿名struct&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var cache = struct {&#xA;sync.Mutex&#xA;mapping map[string]string&#xA;}{&#xA;mapping: make(map[string]string),&#xA;}&#xA;func Lookup(key string) string {&#xA;cache.Lock() //因为Mutex是匿名的&#xA;v := cache.mapping[key]&#xA;cache.Unlock()&#xA;return v&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;方法值和方法表达式&#34;&gt;方法值和方法表达式&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;方法值是绑定了&lt;strong&gt;特定对象的方法&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;p := Point{1, 2}&#xA;q := Point{4, 6}&#xA;distanceFromP := p.Distance&#xA;fmt.Println(distanceFromP(q)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;方法表达式：当T表示一个类型，方法表达式可能会写作T.f或者(*T).f, 会返回一个函数值，这种函数会将其第一个参数用作接收器&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;p := Point{1, 2}&#xA;q := Point{4, 6}&#xA;distance := Point.Distance   // method expression&#xA;fmt.Println(distance(p, q))  // &amp;quot;5&amp;quot;&#xA;fmt.Printf(&amp;quot;%T\n&amp;quot;, distance) // &amp;quot;func(Point, Point) float64&amp;quot;&#xA;//&#xA;scale := (*Point).ScaleBy&#xA;scale(&amp;amp;p, 2)&#xA;fmt.Println(p)            // &amp;quot;{2 4}&amp;quot;&#xA;fmt.Printf(&amp;quot;%T\n&amp;quot;, scale) // &amp;quot;func(*Point, float64)&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;bit数组&#34;&gt;Bit数组&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;使用bit数组数显的非负整数集合: &lt;a href=&#34;https://github.com/KDF5000/gopl/blob/master/src/ch6/intset.go&#34;&gt;intset.go&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;封装&#34;&gt;封装&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;这里的封装是指一个对象的变量或者方法对调用方是不可见的，也叫信息隐藏。&lt;/li&gt;&#xA;&lt;li&gt;大写字母的标识符从定义他们的包中导出，小写不会导出&lt;/li&gt;&#xA;&lt;li&gt;封装的好处&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;调用方不能直接修改对象的变量值，其只需关注少量的语句并且只要弄懂少量变量的可能的值即可&lt;/li&gt;&#xA;&lt;li&gt;隐藏实现细节，可以防止调用方不能直接修改对象的具体实现，这样可以保值不破坏api的情况下灵活的改变实现方式&lt;/li&gt;&#xA;&lt;li&gt;阻止外部调用方对对象内部的值任意的修改&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
      <pubDate>Wed, 15 Mar 2017 20:58:12 +0000</pubDate>
    </item>
    <item>
      <title>操作系统页表的管理</title>
      <link>http://openhex.cn/2017/3/12/操作系统页表的管理.html</link>
      <description>&lt;p&gt;学过操作系统的都知道，在操作系统中存在一个虚拟内存的概念，它用于内存的管理，使得应用程序认为它有一段连续的内存，大大地简化了程序员码代码的难度。程序员只用关注在这个连续的虚拟内存段中怎么使用内存，不用关心在物理内存中到底用那一段内存，进程运行的时候操作系统会自动进行映射。操作系统是怎么做到的呢？实际上操作系统为每一个进程维护了一个从虚拟地址到物理地址的映射关系的数据结构，叫页表，页表的内容就是该进程的虚拟地址到物理地址的一个映射。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;那么什么样的数据结构能够表示这个映射关系呢？最容易想到的方法就是用数组保存，虚拟空间中的每一个页都分配一个数组项。该数组指向与之关联的页帧, 但这会引发一个问题, 例如, IA-32体系结构使用4KB大小的页,在虚拟地址空间为4GB的前提下,则需要包含1048576&#xA;(4G/4K=1024*1024=1048576)项的页表，要寻址4G的空间，需要32位,也就是4Bytes, 1048576项需要4M(4 * 1024 * 1024 Bytes=4M)的内存.这个问题在64位体系结构下, 情况会更加糟糕. 而每个进程都需要自身的页表,这会导致系统中大量的内存都用来保存页表.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;为了减少页表占用空间，我们可以考虑一个实际情况，对于一个进程的虚拟空间，其实大部分的内存都是没有使用的，因此在页表中其实是不需要这部分的映射的，因此前辈们设计出了多级页表的结构。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;那么到底多少级才是最合适的呢？&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;页表级数的选择和发展&#34;&gt;页表级数的选择和发展&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;多级页表的设计思想其实就是现实生活中分组的概念，举个例子，我们研究中心有1000个人，现在秘书准备组织一次春游活动，需要统计一下有多少人去春游，如果秘书每个人都问一下记录一下每个人的信息，那么就需要1000行去记录这个信息，但是这样效率就很低而且浪费空间，所以秘书就让每个实验室单独统计，秘书只用要记录每个实验室去春游的人数以及负责人的联系方式即可，假设有五个实验室，则只需要五行即可，然后每个实验室的负责人统计一下自己实验室需要去的人员信息，那么这样效率就比较高了，而且因为只是记录了那些去春游的的人员信息，因此这样既提高了效率，也大大的也省了存储空间，当然你可能回发现一个小问题，如果10000个人都去那么实际上需要10005行统计信息，但是考虑到一些可能也是经常发生的情况，比如我们软件系统实验室春游那天需要加班赶项目，那么实际上我们实验室负责人就不用进行记录了，秘书那里直接记录软件实验室0个人去，甚至不记录也可以，所以一般情况下这样是可以提高效率，并且能够节省空间的。分级的本质其实也是将页表项进行分组管理，考虑到进程内存大部分空闲以及局部性，因此实际记录的时候大部分的记录是没有的，所以相比上面提到的直接使用一个数组（也就是一级页表）大大地节省了存储需要的空间。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我们再来算一笔帐，从Intel 80386开始，默认的页大小是4k, 提供32位的寻址空间。一页4k需要12位进行寻址，因此对于32的寻址空间，还有32-12=20位可以用来页表的索引。因为页表项存储的是物理内存一页的物理地址，因此一个页表项只能指向一页，而硬件的实现是使用CR3寄存器保存了页表的起始地址，因此对于页表我们应该使其只占用一页的内存。4k能够存储的页表项个数为4k/4B=1024, 寻址1024个索引只需要10位即可，因此在这个前提下，用于页表索引的20位自然而然的就分为了两段，也就是页表被分成了两级，十位用来表示一级页表内部偏移，十位用来表示二级页表的内部偏移，一级页表的页表项存储二级页表的物理地址。其实这也是Linux最初采用的两级页表分页机制。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;linux的两级分页机制&#34;&gt;Linux的两级分页机制&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;两级分页机制将32位的虚拟空间分成三段，对就是三段，低十二位表示页内偏移，高20分成两段分别表示两级页表的偏移。&#xA;&lt;img src=&#34;/media/archive/blog/images/二级页表.png&#34; alt=&#34;二级页表.png&#34; /&gt;&#xA;* PGD(Page Global Directory): 最高10位，全局页目录表索引&#xA;* PTE(Page Table Entry)：中间10位，页表入口索引&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在进行地址转换时，结合CR3寄存器中存放的全局页表物理地址，然后加上虚拟地址的高10位也就是PGD段，作为页表偏移找到相应的页表项，从改页表项得到二级页表的物理地址，然后结合中间十位即PTE段，作为偏移找到PTE项，即获得了要访问的内存地址所在的物理页，然后结合Page Offset即定位到了需要访问的物理内存，这样就完成了从虚拟地址到物理地址的转换。如果没有缓存的情况下，整个过程需要三次内存访问。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;linux的三级页表&#34;&gt;Linux的三级页表&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Intel通过在处理器上把管脚数从32增加到36，以提高处理器的寻址能力，使其达到$2^{36}=64GB$，但是linux依然使用32位的虚拟寻址空间，为此，需引入一种新的分页机制。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;如果页的大小依然是4k, 64G($2^{34}$)的内存可以分为$2^{24}$个页框，然后页表项的物理地址字段从20位扩展到24位，每个页表项必须包含12个标志位（固定）和24个物理地址位（36-12），共36位，因此，每个页表项须从32位扩展到64位（36位&amp;gt;32位，考虑到对齐，因此应将页表项扩大一倍到64位)。所以原来32位的页表项现在变成了64位后，一页可以存储的页表项从1024变成了512，需要9位进行寻址，高一级的页表也需要9位，加上页内偏移12位，现在32位的地址还剩余2位，因此又引入一级页表，该页表只含有4个页表项。这样就解决了PAE情况下的分页。&#xA;下图是PAE情况下的三级页表情况,图片来自&lt;a href=&#34;https://zh.wikipedia.org/wiki/物理地址扩展&#34;&gt;维基百科&lt;/a&gt;&#xA;&lt;img src=&#34;/media/archive/blog/images/三级页表.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;注意(个人观点)：&lt;/strong&gt;虽然页表项扩展到了64位，选址达到了36位，即64G的空间，但是实际上一个进程可以使用的虚拟空间大小仍然是4G，因为虚拟地址依然只有32位，即使每个页表项都填满也只能有4G的空间。唯一不同的是物理内存进行分配时候就可以使用超过4G的空间了。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;linux的四级页表&#34;&gt;Linux的四级页表&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;随着硬件的发展，64位的CPU出现了，这个时候基于32位处理器的三级页表又不能使用了，不过现在的硬件已经可以支持四级页表了，可以使用48位的虚拟空间了(虚拟空间的位数和硬件的对应关系理的还不是很清楚)，经过前辈们的&lt;a href=&#34;https://github.com/gatieme/LDD-LinuxDeviceDrivers/blob/master/study/kernel/02-memory/02-pagetable/01-develop/README.md&#34;&gt;争论&lt;/a&gt;，最后确定了下面的四级页表形式:&#xA;&lt;img src=&#34;/media/archive/blog/images/四级页表.png&#34; alt=&#34;四级页表.png&#34; /&gt;&#xA;从图中可以看出，64位的虚拟地址只使用了低48位，每一级的页表项都有64位，其中低12位作为标志位，然后紧接着的40位作为物理寻址使用，加上页内偏移一共52位用于物理地址，但是实际上CPU的引脚并没有52位，一般是40位，46位等。比如我们实验室使用的Intel Xeon E5645 就是40位的物理地址，所以52位只使用了40位($2^{40}=1T$)。&#xA;下面是IA32-e模式下的线性地址映射&#xA;&lt;img src=&#34;/media/archive/blog/images/线性地址映射.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;参考资料&#34;&gt;参考资料&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;[1] &lt;a href=&#34;https://github.com/gatieme/LDD-LinuxDeviceDrivers/blob/master/study/kernel/02-memory/02-pagetable/01-develop/README.md&#34;&gt;Linux分页机制之概述&lt;/a&gt;&#xA;[2] &lt;a href=&#34;http://blog.csdn.net/trochiluses/article/details/12853027&#34;&gt;物理地址扩展(PAE)的分页机制&lt;/a&gt;&#xA;[3] &lt;a href=&#34;https://zh.wikipedia.org/wiki/物理地址扩展&#34;&gt;物理地址扩展&lt;/a&gt;&#xA;[4] &lt;a href=&#34;https://book.douban.com/subject/5333562/&#34;&gt;《深入理解计算机系统》第九章：虚拟存储器&lt;/a&gt;&#xA;[5] &lt;a href=&#34;https://book.douban.com/subject/24708145/&#34;&gt;《Linux内核设计的艺术》第六章：用户进程与内存管理&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sun, 12 Mar 2017 20:09:39 +0000</pubDate>
    </item>
    <item>
      <title>Golang第五章:函数</title>
      <link>http://openhex.cn/2017/3/11/Golang第五章-函数.html</link>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;函数的类型被称为函数的标识符，如果两个函数的形参列表和返回值列表中的变量类型一一对应，那么这个函数被认为有相同的类型和标识符。形参和返回值的变量名不影响含糊是标识符，也不影响他们是否可以省略参数类型的形式表示。&lt;/li&gt;&#xA;&lt;li&gt;Go语言没有默认参数值&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;Go可以有有名返回值，和形参一样作为局部变量，被存储在相同的词法块，一般使用带有变量名的返回值主要为了表示返回值的含义,也可以直接在函数体对这些变量赋值，返回时候可以省略return的操作数&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func Size(rect image.Rectangle) (width, height int)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;Go的参数传递都是值传递，因此对形参的修改不会影响实参，但是如果实参是指针，slice,map, function,channel等类型，则可以间接的影响实参值。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;下面没有函数体的函数声明，表明该函数不是Go事先的，这样的声明定义了函数标识符&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package math&#xA;func Sin(x float64) float //implemented in assembly language&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2 id=&#34;错误处理&#34;&gt;错误处理&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;error类，是个接口类型，有nil和non-nil两种&lt;/li&gt;&#xA;&lt;li&gt;错误处理策略&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;传播错误&lt;/li&gt;&#xA;&lt;li&gt;将错误err传递给调用者，然后决定怎么处理&lt;/li&gt;&#xA;&lt;li&gt;错误信息里最好包含上下文信息，通常是连式组合的形式，所以也要避免大写和换行符&lt;/li&gt;&#xA;&lt;li&gt;重试&#xA;设置超时时间，重新尝试操作&lt;/li&gt;&#xA;&lt;li&gt;结束进程&#xA;如果错误会导致程序无法继续运行，则输出错误结束程序,os.Exit(1)&lt;/li&gt;&#xA;&lt;li&gt;Log记录错误信息，继续运行&lt;/li&gt;&#xA;&lt;li&gt;忽略错误&lt;br /&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;io包提供一个错误io.EOF,保证任何文件结束引起的读取失败错误。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2 id=&#34;函数值&#34;&gt;函数值&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Go中函数被看做第一类值(first-class values)： 函数像其他值一样，拥有类型，可以赋值给其他变量，传递给函数，从函数返回&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func square(n int) int { return n * n  }&#xA;func negative(n int) int { return -n  }&#xA;func product(m, n int) int { return m * n  }&#xA;f := square&#xA;fmt.Println(f(3)) // &amp;quot;9&amp;quot;&#xA;f = negative&#xA;fmt.Println(f(3))     // &amp;quot;-3&amp;quot;&#xA;fmt.Printf(&amp;quot;%T\n&amp;quot;, f) // &amp;quot;func(int) int&amp;quot;&#xA;f = product // compile error: can&#39;t assign func(int, int) int to func(int) int&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;函数类型的零值为nil, 调用值为nil的函数值会引起panic错误&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;函数值不能进行比较，不能作为map的key&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;函数可以作为函数的参数&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func add(a int , b int) int{return a+b}&#xA;func sub(a int , b int) int {return a-b}&#xA;func compute(a, b int, fun func(int,int)int){&#xA;return fun(a,b)&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h2 id=&#34;匿名函数&#34;&gt;匿名函数&lt;/h2&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;匿名函数在Go中称为Function literal, 声明方式和函数类似，但是没有函数名，是一种表达式，他的值被称为匿名函数&lt;/li&gt;&#xA;&lt;li&gt;匿名函数可以在使用时定义&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;匿名函数可以访问完整的词法环境，也就是说在一个函数内部定义匿名函数，在匿名函数内部可以访问函数的变量&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func squares() func() int {&#xA;var x int&#xA;  return func() int {&#xA;          x++&#xA;                  return x * x&#xA;                          &#xA;  }&#xA;&#xA;}&#xA;func main() {&#xA;f := squares()           //f指向一个匿名函数，该匿名函数里的X是一个变量&#xA;    fmt.Println(f())         //1&#xA;        fmt.Println(f())         //4&#xA;            fmt.Println(f())         //9&#xA;                fmt.Println(squares()()) //1&#xA;                    fmt.Println(squares()()) //1&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;对squares的一次调用会生成一个局部变量x并返回一个匿名函数，每次调用改匿名函数，该函数都会使x得值加1， 上面f指向匿名函数，是同一个局部变量x。第一次调用squares（）,会生成第二个x变量。并返回一个新的匿名函数&lt;/p&gt;&#xA;&#xA;&lt;p&gt;这个例子证明，函数值不仅仅是一串代码，还记录了状态。在square中定义的匿名内部函数可以访问和更新square中的局部变量，这意味着匿名函数和square中，存在变量引用。这就是&lt;strong&gt;函数值属于引用类型和函数值不可比较&lt;/strong&gt;的原因。Go使用闭包技术实现函数值，Go程序员也把函数值叫做闭包。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;捕获迭代变量&#34;&gt;捕获迭代变量&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;先看下面的程序&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var rmdirs []func()&#xA;    for _, d := range tempDirs() {&#xA;        os.MkdirAll(dir, 0755) // creates parent directories too&#xA;            rmdirs = append(rmdirs, func() {&#xA;                    os.RemoveAll(d)&#xA;                        &#xA;                    })&#xA;&#xA;    }&#xA;    // ...do some work...&#xA;for _, rmdir := range rmdirs {&#xA;    rmdir() // clean up&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;上面程序的执行能否达到预期的结果呢？答案是否定的。for循环内部创建一个新的目录的时候，我们使用一个匿名的函数，在循环内部引用了变量d, d其实只是指向变量的内存地址，每一次循环的过程都在发生改变，因此for循环结束后指向的是最后一个变量的地址，所以后面for循环的时候删除的都是同一个目录。解决方法就是在for循环内部对循环变量进行一个copy即可，见下面代码&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var rmdirs []func()&#xA;    for _, d := range tempDirs() {&#xA;        dir := d // NOTE: necessary!&#xA;            os.MkdirAll(dir, 0755) // creates parent directories too&#xA;            rmdirs = append(rmdirs, func() {&#xA;                    os.RemoveAll(dir)&#xA;                        &#xA;                    })&#xA;&#xA;    }&#xA;    // ...do some work...&#xA;for _, rmdir := range rmdirs {&#xA;    rmdir() // clean up&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;可变参数&#34;&gt;可变参数&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;声明可变参数函数时，需要在参数列表的最后一个参数类型之前加上省略号&amp;rdquo;&amp;hellip;&amp;ldquo;,这表示该函数会接受任意数量的该类型参数。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func sum(vals...int) int {&#xA;    total := 0&#xA;               for _, val := range vals {&#xA;                       total += val&#xA;&#xA;               }&#xA;                   return total&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;对于可变参数，可以直接传递一个切片，然后后面加省略号&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;values := []int{1, 2, 3, 4}&#xA;fmt.Println(sum(values...)) // &amp;quot;10&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;interfac{}表示函数的最后一个参数可以接受任意类型&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;deferred函数&#34;&gt;Deferred函数&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;defer可以在异常判断的过程中多次执行诸如文件关闭的操作，使用defer可以在return的时候自动执行&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;调试复杂程序时，defer机制也常被用于记录何时进入和退出函数&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func bigSlowOperation() {&#xA;defer trace(&amp;quot;bigSlowOperation&amp;quot;)() // don&#39;t forget the&#xA;    extra parentheses&#xA;        // ...lots of work...&#xA;            time.Sleep(10 * time.Second) // simulate slow&#xA;                operation by sleeping&#xA;&#xA;}&#xA;func trace(msg string) func() {&#xA;start := time.Now()&#xA;    log.Printf(&amp;quot;enter %s&amp;quot;, msg)&#xA;    return func() {&#xA;            log.Printf(&amp;quot;exit %s (%s)&amp;quot;, msg,time.Since(start))&#xA;                    &#xA;    }&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;defer语句中的函数会在return语句更新返回值后再执行，又因为函数中定义的匿名函数可以访问该函数包括返回值变量在内的所有变量，所以，对匿名函数采用defer机制可以使其观察函数的返回值.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func double(x int) (result int) {&#xA;defer func() { fmt.Printf(&amp;quot;double(%d) = %d\n&amp;quot;, x,result)  }()&#xA;    return x + x&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;被延迟执行的匿名函数甚至可以修改函数返回给调用者的返回值&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func triple(x int) (result int) {&#xA;defer func() { result += x  }()&#xA;    return double(x)&#xA;&#xA;}&#xA;fmt.Println(triple(4)) // &amp;quot;12&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;对于读写文件信息的操作可能要慎用defer操作，通过os.Create打开的文件进行写入，在关闭文件时，不能采用defer机制。因为对于许多文件系统，尤其是NFS， 写入文件时发生的错误会被延迟到文件关闭时反馈。如果没有检查文件关闭时的反馈信息，可能会导致数据丢失，而我们还误以为写入操作成功。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;延迟函数的调用在释放堆栈信息之前&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;panic异常&#34;&gt;Panic异常&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;panic函数接受任何值作为参数，当某些不应该发生的场景发生时，我们就应该调用painc。&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果在deferred函数中调用了内置函数recover, 并且定义改defer语句的函数发生了panic异常，recover会使程序从panic中恢复，并返回panic value. 导致panic异常的函数不会继续运行，但能整场 返回。在未发生panic时调用recover， recover会返回nil&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func Parse(input string) (s *Syntax, err error) {&#xA;defer func() {&#xA;    if p := recover(); p != nil {&#xA;                err = fmt.Errorf(&amp;quot;internal error: %v&amp;quot;, p)&#xA;&#xA;    } &#xA;}()&#xA;    // ...parser...&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
      <pubDate>Sat, 11 Mar 2017 20:58:00 +0000</pubDate>
    </item>
    <item>
      <title>Golang第四章:复合数据类型</title>
      <link>http://openhex.cn/2017/2/25/Golang第四章-复合数据类型.html</link>
      <description>&lt;p&gt;主要介绍数组，slice, map和结构体&#xA;数组和结构体是聚合类型；数组有同构的元素组成，结构体有异构的元素组成，他们都是有固定内存大小的数据结构。&#xA;slice和map是动态的的数据结构，他们根据需要动态增长。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;数组&#34;&gt;数组&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;初始化&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var q [3]int = [3]int{1,2,3}&#xA;var q [3]int = [3]int{1,2}&#xA;q := [...]int{1,2,3} //...表示数组的长度根据初始化值得个数来计算&#xA;q := [...]int{1:1, 4:4,5:5} //提供索引和对应值的方式，长度则根据最大的索引值确定，这里是6，因此数组的长度是7&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果数组的元素类型是可以直接可以进行比较的，那么数组本身也是可以直接比较的，通过==进行比较，只有当两个数组的所有元素都是相等的时候数组才是相等的。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;数组作为函数参数时，是值传递，会复制一份到参数里(C语言则是传递的引用)，因此在函数内部的修改不会反映在原始数组中。因此由于这种机制传递大的数组的效率是非常低的。但是可以将函数的参数显示的设置为数组指针，这样对函数内部数组的修改就可以反映在原始数组里了。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;数组作为参数要指定长度，如果想要修改数组的值可以传递数组的指针&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func zero(ptr *[32]byte) {&#xA;    for i := range ptr {&#xA;        ptr[i] = 0 }&#xA;    }&#xA;}&#xA;&#xA;func zero(ptr *[32]byte) {&#xA;    *ptr = [32]byte{}&#xA;}&#xA;//&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;slice&#34;&gt;Slice&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;Slice(切片)代表变长的序列，序列中每个元素都有相同的类型。&#xA;&lt;img src=&#34;/media/archive/blog/images/6F89E07747579464A98601695116E687.png&#34; alt=&#34;80066410.png&#34; /&gt;&#xA;数组和Slice的关系：一个slice是一个轻量级的数据结构，提供了访问&lt;strong&gt;数组&lt;/strong&gt;子序列或者全部元素的功能，而且slice的底层确实引用了一个数组对象。一个slice有三个部分组成：指针，长度和容量。指针指向第一个slice元素对应的底层数组元素的地址，这个第一个元素并不一定是数组的第一个元素。长度对应slice元素的数目，长度不能超过容量，容量一般是从slice的开始位置到底层数据的结尾位置。内置的len和cap函数分别返回slice的长度和容量. 如果切片的操作超出了cap(s)的上限则会导致一个panic异常，但是超出len(s)则意味着扩展了slice，因为新的slice的长度会变大。&#xA;slice的初始化&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;s := []int{0,1,2,3,4,5,6} //slice&#xA;s := [...]int[0,1,2,3,4,5,6] //数组&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;好像和数组没很大的差别，但是仔细对比数组的初始化长度是固定的要么指定数组的长度要么&amp;hellip;，使用初始化的参数个数。&#xA;slice不需要指明序列的长度，这会隐式的创建一个合适大小的数组，然后slice的指针指向底层的数组&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;a1 := []int{1, 2, 3, 4, 5}&#xA;a2 := [...]int{1, 2, 3, 4, 5} //数组&#xA;fmt.Println(cap(a1)) //5&#xA;fmt.Println(cap(a2)) //5&#xA;b1 := []int{}&#xA;b2 := [...]int{}&#xA;fmt.Println(cap(b1)) //0&#xA;fmt.Println(cap(b2)) //0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;因为slice值包含指向第一个slice元素的指针，因此向函数传递一个slice将允许在函数内部修改底层数组的元素，换句话说，，复制一个slice只是对底层的数组创建了一个新的slice别名&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// reverse reverses a slice of ints in place.&#xA;func reverse(s []int) {&#xA;    for i, j := 0, len(s)-1; i &amp;lt; j; i, j = i+1, j-1 {&#xA;        s[i], s[j] = s[j], s[i]&#xA;    }&#xA;}&#xA;a := [...]int{0, 1, 2, 3, 4, 5}&#xA;reverse(a[:])&#xA;fmt.Println(a) // &amp;quot;[5 4 3 2 1 0]&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;slice之间不能比较，不能像数组那种==进行比较，因此必须我们自己遍历slice实现比较的功能，对于byte类型的slice，标准库提供了高度优化的bytes.Equal的比较方法。&#xA;原因见：&lt;/p&gt;&#xA;&#xA;&lt;p&gt;slice唯一合法的比较操作是和nil的比较，一个零值得slice等于nil.但是长度为0不一定等于nil, 比如：make([]int,3)[3:], 因此判断slice是否为空不能用slice == nil，要使用len(s) == 0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;使用make创建一个slice&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;make([]T, len)&#xA;make([]T, len, cap) // same as make([]T, cap)[:len]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;在底层，make创建了一个匿名数组，然后返回一个slice.只有通过slice才能访问底层的匿名数组&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var s []int //slice&#xA;var s [3]int //数组&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;append函数可以扩充slice&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;slice1 = append(slice1, x)&#xA;slice1 = append(slice1, slice2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;append的返回值是一个slice,因此必须将原来的slice重新赋值，在这个过程会产生内存的重新分配，下面是一个简单的模拟append的操作&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func appendInt(x []int, y int) []int {&#xA;    var z []int&#xA;    zlen := len(x) + 1&#xA;    if zlen &amp;lt;= cap(x) {&#xA;        // There is room to grow.  Extend the slice.&#xA;        z = x[:zlen]&#xA;    } else {&#xA;        // There is insufficient space.  Allocate a new array.&#xA;        // Grow by doubling, for amortized linear complexity.&#xA;        zcap := zlen&#xA;        if zcap &amp;lt; 2*len(x) {&#xA;            zcap = 2 * len(x)&#xA;        }&#xA;        z = make([]int, zlen, zcap)&#xA;        copy(z, x) // a built-in function; see text&#xA;    }&#xA;    z[len(x)] = y&#xA;    return z&#xA;}&#xA;&#xA;func main() {&#xA;    // var nums []int //slice 长度为0&#xA;    // fmt.Println(nums)&#xA;    // nums = appendInt(nums, 1)&#xA;    // nums = appendInt(nums, 2)&#xA;    // nums = appendInt(nums, 4) //cap=4&#xA;    // nums = appendInt(nums, 4) //cap=4&#xA;    // fmt.Println(nums, cap(nums))&#xA;    var x, y []int&#xA;    for i := 0; i &amp;lt; 10; i++ {&#xA;        y = appendInt(x, i)&#xA;        fmt.Printf(&amp;quot;%d cap=%d\t%v\n&amp;quot;, i, cap(y), y)&#xA;        x = y&#xA;    }&#xA;}&#xA;&#xA;///&#xA;/*&#xA;0 cap=1 [0]&#xA;1 cap=2 [0 1]&#xA;2 cap=4 [0 1 2]&#xA;3 cap=4 [0 1 2 3]&#xA;4 cap=8 [0 1 2 3 4]&#xA;5 cap=8 [0 1 2 3 4 5]&#xA;6 cap=8 [0 1 2 3 4 5 6]&#xA;7 cap=8 [0 1 2 3 4 5 6 7]&#xA;8 cap=16    [0 1 2 3 4 5 6 7 8]&#xA;9 cap=16    [0 1 2 3 4 5 6 7 8 9]&#xA;*/&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;slice可以用来实现stack&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;map&#34;&gt;Map&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;一个map就是一个哈希表的&lt;strong&gt;引用&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;args := map[string]int {&#xA;    &amp;quot;alice&amp;quot;: 12,&#xA;    &amp;quot;tom&amp;quot;:21, //最好加上逗号，避免编译器在这里添加“;”&#xA;}&#xA;//2.make创建&#xA;args := make(map[string]int)&#xA;args[&amp;quot;tom&amp;quot;] = 21&#xA;//删除&#xA;delete(args, &amp;quot;tom&amp;quot;)&#xA;//遍历&#xA;for k,v := range args{&#xA;    fmt.Printf(&amp;quot;%s\t%d\n&amp;quot;, k, v)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果一个key值不存在，则访问会返回value类型的零值&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;map的遍历是随机的，每次的顺序都不同，这是故意的，每次使用随机的遍历顺序可以强制要求程序不会依赖具体的哈希函数实现。如果要按照顺序对key/value进行遍历，必须显式的对key进行排序，可以使用sort包的Strings函数对字符串slice排序&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import &amp;quot;sort&amp;quot;&#xA;var names []string&#xA;//先取出所有Key&#xA;for name := range args {&#xA;names = append(names, name)&#xA;}&#xA;//排序&#xA;sort.Strings(names)&#xA;//有序访问&#xA;for _, name := range names {&#xA;fmt.Printf(&amp;quot;%s\t%d\n&amp;quot;, name, ages[name])&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;map的零值是nil,也就是没有指向任何的hash表&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;map声明后如果不进行make创建让其指向一个hash表，则为零值，但是依然可以进行查找，删除，len和range操作，不过不能进行访问和存储元素，这样会造成panic&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var args map[string]int  //nil&#xA;fmt.Println(len(args)) //0&#xA;args[&amp;quot;Tom&amp;quot;] = 32 //panic: assignment to entry in nil map&#xA;args = make(map[string]int)&#xA;args[&amp;quot;Tom&amp;quot;] = 32 //Ok&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;使用下面的代码测试key是否存在&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;age, ok := ages[&amp;quot;bob&amp;quot;]&#xA;if !ok {&#xA;fmt.Println(&amp;quot;No key:  Bob,&amp;quot;, age) //age=0&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;两个map不能进行比较，只能和nil进行比较&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func equal(x, y map[string]int) bool {&#xA;if len(x) != len(y) {&#xA;    return false&#xA;}&#xA;for k, xv := range x {&#xA;    if yv, ok := y[k]; !ok || yv != xv { //注意这里要通过ok和相等两个条件判断&#xA;        return false&#xA;    }&#xA;}&#xA;return true&#xA;}&#xA;//equal(map[string]int{&amp;quot;A&amp;quot;: 0}, map[string]int{&amp;quot;B&amp;quot;: 42})&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;map的vaule也可以是聚合类型，比如slice或者map，下面的graph可以表示一个图&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var graph = make(map[string]map[string]bool)&#xA;func addEdge(from, to string) {&#xA;edges := graph[from]&#xA;if edges == nil {&#xA;    edges = make(map[string]bool)&#xA;    graph[from] = edges&#xA;}&#xA;edges[to] = true&#xA;}&#xA;func hasEdge(from, to string) bool {&#xA;return graph[from][to]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;结构体&#34;&gt;结构体&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;结构体就是你种类型变量的集合&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type  Employee struct {&#xA;    ID        int&#xA;    Name      string&#xA;    Address   string&#xA;    DoB       time.Time&#xA;    Position  string&#xA;    Salary    int&#xA;    ManagerID int&#xA;}&#xA;var dilbert Employee&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;结构体变量的的成员的访问可以通过点操作，与C不同的是，对于一个指针类型的结构体变量同样可以通过点操作访问，下面两个是等价的&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var employeeOfTheMonth *Employee = &amp;amp;dilbert&#xA;employeeOfTheMonth.Position += &amp;quot; (proactive team player)&amp;quot;&#xA;*employeeOfTheMonth).Position += &amp;quot; (proactive team player)&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;结构体成员名字是以大写字母开头，那么该成员就是导出的，这是GO语言导出规则决定的&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;结构体成员不能是该结构体类型，但是可以是该类型的指针&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;重要:&lt;/strong&gt;&#xA;&amp;gt; 如果指针作为函数的参数，如果传递到函数的指针在函数调用前已经初始化指向了一个数据，那么在函数内部通过指针修改变量，在函数外部是可见的，但是如果指针是在函数内部初始化，时间上传递的指针也是值传递，内部看到的变量只是外部变量的一个复制，所以外部变量的指针仍然为空&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;结构体赋值可以按照变量的顺序赋值，也可以k-v的形式，但是在外部包中不能使用顺序赋值的方式对结构体中未导出的变量赋值&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Point struct{ X, Y,z int }&#xA;p := Point{1, 2, 3}&#xA;p := Point{X:1,Y:2,z:3}&#xA;//外部包&#xA;p := Point{1, 2, 3} //编译错误，compile error: can&#39;t reference z&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;结构体可以作为函数的参数和返回值，但是考虑到效率通常传入结构体的指针&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果结构体的全部成员是可以比较的，那么结构体也是可以比较的&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;可比较的结构体可以作为map的key&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;匿名类型指的是一个结构体可以包含一个结构体类型的成员变量，并且可以省略成员变量名，实际上改类型的成员会将该类型名作为变量名，在进行访问的时候可以直接访问匿名成员的成员变量，但是赋值的时候必须使用嵌套的方式进行复制&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Point struct {&#xA;X, Y int&#xA;}&#xA;type Circle struct {&#xA;Center Point&#xA;Radius int &#xA;}&#xA;type Wheel struct {&#xA;Circle Circle&#xA;Spokes int &#xA;}&#xA;//可以这么访问&#xA;var w Wheel&#xA;w.X = 8         // equivalent to w.Circle.Point.X = 8&#xA;w.Y = 8         // equivalent to w.Circle.Point.Y = 8&#xA;w.Radius = 5    // equivalent to w.Circle.Radius = 5&#xA;w.Spokes = 20&#xA;//不能这么赋值&#xA;w = Wheel{8, 8, 5, 20}                       // compile error: unknown fields&#xA;w = Wheel{X: 8, Y: 8, Radius: 5, Spokes: 20} // compile error: unknown fields&#xA;//这样赋值&#xA;w = Wheel{Circle{Point{8, 8}, 5}, 20}&#xA;w = Wheel{&#xA;Circle: Circle{&#xA;    Point:  Point{X: 8, Y: 8},&#xA;    Radius: 5,&#xA;},&#xA;Spokes: 20, // NOTE: trailing comma necessary here (and at Radius)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;匿名成员有个隐式的名字，因此不能同时包含两个类型相同的匿名成员，这会导致名字重涂&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;匿名成员同样有可见性的规则约束(首字母大写)&#xA;&lt;strong&gt;重要:&lt;/strong&gt;&#xA;&amp;gt; 结构体的匿名成员并不要求是结构体，任何类型都可以作为匿名成员，但是为什么要嵌入一个没有任何子成员类型的匿名成员类型呢？简短的点运算语法可以用于选择匿名成员的嵌套成员，也可以访问他们的方法。实际上，外层的结构体不仅仅获得了匿名成员类型的所有成员，而且也获得了该类型导出的全部的方法。这个机制可以将一个简单行为的队形组合成复杂行为的对象。组合是Go语言中面向对象编程的核心。&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;json&#34;&gt;JSON&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;使用encoding/json进行json的编码解码，可以直接编码struct结构体&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Movie struct {&#xA;Title  string&#xA;Year   int  `json:&amp;quot;released&amp;quot;`&#xA;Color  bool `json:&amp;quot;color,omitempty&amp;quot;`&#xA;Actors []string&#xA;}&#xA;var movies = []Movie{&#xA;{Title: &amp;quot;Casablanca&amp;quot;, Year: 1942, Color: false,&#xA;    Actors: []string{&amp;quot;Humphrey Bogart&amp;quot;, &amp;quot;Ingrid Bergman&amp;quot;}},&#xA;{Title: &amp;quot;Cool Hand Luke&amp;quot;, Year: 1967, Color: true,&#xA;    Actors: []string{&amp;quot;Paul Newman&amp;quot;}},&#xA;{Title: &amp;quot;Bullitt&amp;quot;, Year: 1968, Color: true,&#xA;    Actors: []string{&amp;quot;Steve McQueen&amp;quot;, &amp;quot;Jacqueline Bisset&amp;quot;}},&#xA;// ...&#xA;}&#xA;data, err := json.Marshal(movies)&#xA;if err != nil {&#xA;log.Fatalf(&amp;quot;JSON marshaling failed: %s&amp;quot;, err)&#xA;}&#xA;///&#xA;[{&amp;quot;Title&amp;quot;:&amp;quot;Casablanca&amp;quot;,&amp;quot;released&amp;quot;:1942,&amp;quot;Actors&amp;quot;:[&amp;quot;Humphrey Bogart&amp;quot;,&amp;quot;Ingr id Bergman&amp;quot;]},{&amp;quot;Title&amp;quot;:&amp;quot;Cool Hand Luke&amp;quot;,&amp;quot;released&amp;quot;:1967,&amp;quot;color&amp;quot;:true,&amp;quot;Ac tors&amp;quot;:[&amp;quot;Paul Newman&amp;quot;]},{&amp;quot;Title&amp;quot;:&amp;quot;Bullitt&amp;quot;,&amp;quot;released&amp;quot;:1968,&amp;quot;color&amp;quot;:true,&amp;quot; Actors&amp;quot;:[&amp;quot;Steve McQueen&amp;quot;,&amp;quot;Jacqueline Bisset&amp;quot;]}]&#xA;&#xA;//解码&#xA;//解码的时候可以选择性的进行解码，比如下面只选择电影的名字解码&#xA;var titles []struct{ Title string }&#xA;if err := json.Unmarshal(data, &amp;amp;titles); err != nil {&#xA;log.Fatalf(&amp;quot;JSON unmarshaling failed: %s&amp;quot;, err)&#xA;}&#xA;fmt.Println(titles) // &amp;quot;[{Casablanca} {Cool Hand Luke} {Bullitt}]&amp;quot;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;编码时默认使用结构体成员名字作为Json的key值，但是只有导出的成员(大写)才会被编码&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;Tag: 一个结构体成员Tag是和在编译节点关联到该成员的元信息字符串&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Movie struct {&#xA;Title  string&#xA;Year   int  `json:&amp;quot;released&amp;quot;`&#xA;Color  bool `json:&amp;quot;color,omitempty&amp;quot;`&#xA;Actors []string&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;结构体成员Tag可以是任意的字符串面值，但是通常是一系列用空格隔开的key:&amp;ldquo;values&amp;rdquo;键值对序列；因为值中含有双引号字符，因此Tag一般用原生字符串面值形式书写。json头健名对应的值用于控制encoding/json包的解码和解码行为，并且encoding/..下面其他的包也遵循这个约定。成员Tag中json对应值得第一个部分指定json对象的名字，比如上面讲Year指定为released，Color指定为color,Color的Tag还带了额外的一个omitempty选项，便是当Go语言结构体成员为空或者零值得时候不生成json对象，也就是Color为false时，json中就没有color这个成员&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;文本和html模板&#34;&gt;文本和HTML模板&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;后期再看，更加适合web编程&#xA;* 有text/template和htmp/template提供一个将变量值填充到一个文本或者HTML格式的模板&#xA;* {{action}}是模板中的替换标志，action可以使一个变量也可以使表达式&lt;/p&gt;&#xA;</description>
      <pubDate>Sat, 25 Feb 2017 12:00:21 +0000</pubDate>
    </item>
    <item>
      <title>Golang第三章:基础数据类型</title>
      <link>http://openhex.cn/2017/2/25/Golang第三章-基础数据类型.html</link>
      <description>&lt;h3 id=&#34;整型&#34;&gt;整型&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;有符号：int8, int16, int32, int64, int&#xA;无符号: uint8, unit16, uint32, uint64, uint&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var x uint8 = 1&amp;lt;&amp;lt;1 | 1&amp;lt;&amp;lt;5 //00100010&#xA;var y uint8 = 1&amp;lt;&amp;lt;1 | 1&amp;lt;&amp;lt;2 //00000110&#xA;fmt.Printf(&amp;quot;%08b\n&amp;quot;, x)   // &amp;quot;00100010&amp;quot;, the set {1, 5}&#xA;fmt.Printf(&amp;quot;%08b\n&amp;quot;, y)   // &amp;quot;00000110&amp;quot;, the set {1, 2}&#xA;&#xA;fmt.Printf(&amp;quot;%08b\n&amp;quot;, x&amp;amp;y)  // &amp;quot;00000010&amp;quot;, the intersection {1}&#xA;fmt.Printf(&amp;quot;%08b\n&amp;quot;, x|y)  // &amp;quot;00100110&amp;quot;, the union {1, 2, 5} //或&#xA;fmt.Printf(&amp;quot;%08b\n&amp;quot;, x^y)  // &amp;quot;00100100&amp;quot;, the symmetric difference {2, 5}不同为1&#xA;fmt.Printf(&amp;quot;%08b\n&amp;quot;, x&amp;amp;^y) // &amp;quot;00100000&amp;quot;, the difference {5} 如果y对应的位为1，则为0；否则为x的值&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!-- more --&gt;&#xA;&#xA;&lt;p&gt;下面的例子&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;medals := []string{&amp;quot;gold&amp;quot;, &amp;quot;silver&amp;quot;, &amp;quot;bronze&amp;quot;}&#xA;for i := len(medals) - 1; i &amp;gt;= 0; i-- {&#xA;    fmt.Println(medals[i]) // &amp;quot;bronze&amp;quot;, &amp;quot;silver&amp;quot;, &amp;quot;gold&amp;quot;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果i是一个无符号数，则i--永远不会小于0，因此将会出现out ot index的错误。所以len函数设计时候返回的是有符号的int&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;o := 0666&#xA;fmt.Printf(&amp;quot;%d %[1]o %#[1]o\n&amp;quot;, o) // &amp;quot;438 666 0666&amp;quot;&#xA;x := int64(0xdeadbeef)&#xA;fmt.Printf(&amp;quot;%d %[1]x %#[1]x %#[1]X\n&amp;quot;, x)&#xA;// Output:&#xA;// 3735928559 deadbeef 0xdeadbeef 0XDEADBEEF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;浮点数&#34;&gt;浮点数&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;两种：float32和float64&#xA;max.MaxFloat32: 表示float32能表示的最大数。大约3.4e38&#xA;max.MaxFloat64 大约1.8e308&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;布尔类型&#34;&gt;布尔类型&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;布尔值不会隐式的转换为0或者1，需要自己转换。同样不像C语言中，大于0的整数就是true，必须显示的进行转换。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;字符串&#34;&gt;字符串&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;字符串的值不能改变，也就是说字符串只读&lt;/li&gt;&#xA;&lt;li&gt;各种编码，ASCII, Unicode, UTF-8&lt;/li&gt;&#xA;&lt;li&gt;字符串和Byte切片&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;常量&#34;&gt;常量&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;常量表达式的值在编译期计算，不是在运行期，常量的值不可修改。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;const pi = 3.14159&#xA;const(&#xA;    e = 2.3232322222242424&#xA;    p = 3.1415926533232323&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;常量间的所有算术运算，逻辑运算和比较结果也是常量，对常量的类型转换或者以下函数的调用结果也是返回常量结果：len, cap, real, imag, complex和unsafe.Sizeof&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在一个const声明语句中，在第一个声明的常量所在行，iota将会被置0，然后在每一个有常量声明的行加一&lt;/p&gt;&#xA;</description>
      <pubDate>Sat, 25 Feb 2017 11:59:30 +0000</pubDate>
    </item>
    <item>
      <title>Golang第二章:程序结构</title>
      <link>http://openhex.cn/2017/2/25/Golang第二章-程序结构.html</link>
      <description>&lt;h3 id=&#34;命名&#34;&gt;命名&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个名字必须以一个字母（Unicode字母）或下划线开头，后面可以跟任意数量的字母、数字或下划线。&lt;/li&gt;&#xA;&lt;li&gt;名字的开头字母的大小写决定了名字在包外的可见性。如果一个名字是大写字母开头的（译注：必须是在函数外部定义的包级名字；包级函数名本身也是包级名字），那么它将是导出的，也就是说可以被外部的包访问，例如fmt包的Printf函数就是导出的，可以在fmt包外部访问。包本身的名字一般总是用小写字母。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;!--more --&gt;&#xA;&#xA;&lt;h3 id=&#34;指针&#34;&gt;指针&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不仅仅是指针会创建别名，很多其他引用类型也会创建别名，例如slice、map和chan，甚至结构体、数组和接口都会创建所引用变量的别名。&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;程序中的sep和n变量分别是指向对应命令行标志参数变量的指针，因此必须用*sep和*n形式的指针语法间接引用它们。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;var n = flag.Bool(&amp;quot;n&amp;quot;, false, &amp;quot;omit trailing newline&amp;quot;)&#xA;var sep = flag.String(&amp;quot;s&amp;quot;, &amp;quot; &amp;quot;, &amp;quot;separator&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;new&#34;&gt;new&lt;/h3&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;另一个创建变量的方法是调用用内建的new函数。表达式new(T)将创建一个T类型的匿名变量，初始化为T类型的零值，然后返回变量地址，返回的指针类型为*T。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果两个类型都是空的，也就是说类型的大小是0，例如struct{}和 [0]int, 有可能有相同的地址（依赖具体的语言实现）（译注：请谨慎使用大小为0的类型，因为如果类型的大小为0的话，可能导致Go语言的自动垃圾回收器有不同的行为，具体请查看runtime.SetFinalizer函数相关文档）。&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;变量的生命周期&#34;&gt;变量的生命周期&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;变量的生命周期指的是在程序运行期间变量有效存在的时间间隔。对于在包一级声明的变量来说，它们的生命周期和整个程序的运行周期是一致的。而相比之下，局部变量的声明周期则是动态的：每次从创建一个新变量的声明语句开始，直到该变量不再被引用为止，然后变量的存储空间可能被回收。函数的参数变量和返回值变量都是局部变量。它们在函数每次被调用的时候创建。&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;函数的有右小括弧也可以另起一行缩进，同时为了防止编译器在行尾自动插入分号而导致的编译错误，可以在末尾的参数变量后面显式插入逗号&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;for t := 0.0; t &amp;lt; cycles*2*math.Pi; t += res {&#xA;x := math.Sin(t)&#xA;y := math.Sin(t*freq + phase)&#xA;img.SetColorIndex(&#xA;    size+int(x*size+0.5), size+int(y*size+0.5),&#xA;    blackIndex, // 最后插入的逗号不会导致编译错误，这是Go编译器的一个特性&#xA;)               // 小括弧另起一行缩进，和大括弧的风格保存一致&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;那么Go语言的自动垃圾收集器是如何知道一个变量是何时可以被回收的呢？这里我们可以避开完整的技术细节，基本的实现思路是，从每个包级的变量和每个当前运行函数的每一个局部变量开始，通过指针或引用的访问路径遍历，是否可以找到该变量。如果不存在这样的访问路径，那么说明该变量是不可达的，也就是说它是否存在并不会影响程序后续的计算结果。&#xA;因为一个变量的有效周期只取决于是否可达，因此一个循环迭代内部的局部变量的生命周期可能超出其局部作用域。同时，局部变量可能在函数返回之后依然存在。&#xA;编译器会自动选择在栈上还是在堆上分配局部变量的存储空间，但可能令人惊讶的是，这个选择并不是由用var还是new声明变量的方式决定的。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果将指向短生命周期对象的指针保存到具有长生命周期的对象中，特别是保存到全局变量时，会阻止对短生命周期对象的垃圾回收（从而可能影响程序的性能)&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;赋值&#34;&gt;赋值&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果将指向短生命周期对象的指针保存到具有长生命周期的对象中，特别是保存到全局变量时，会阻止对短生命周期对象的垃圾回收（从而可能影响程序的性能）。&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;元组赋值是另一种形式的赋值语句，它允许同时更新多个变量的值。在赋值之前，赋值语句右边的所有表达式将会先进行求值，然后再统一更新左边对应变量的值。这对于处理有些同时出现在元组赋值语句左右两边的变量很有帮助，例如我们可以这样交换两个变量的值：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;x, y = y, x&#xA;a[i], a[j] = a[j], a[i]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;类型&#34;&gt;类型&lt;/h3&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;类型声明语句一般出现在包一级，因此如果新创建的类型名字的首字符大写，则在外部包也可以使用。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;我们在这个包声明了两种类型：Celsius和Fahrenheit分别对应不同的温度单位。它们虽然有着相同的底层类型float64，但是它们是不同的数据类型，因此它们不可以被相互比较或混在一个表达式运算。刻意区分类型，可以避免一些像无意中使用不同单位的温度混合计算导致的错误；因此需要一个类似Celsius(t)或Fahrenheit(t)形式的显式转型操作才能将float64转为对应的类型。Celsius(t)和Fahrenheit(t)是类型转换操作，它们并不是函数调用。类型转换不会改变值本身，但是会使它们的语义发生变化。另一方面，CToF和FToC两个函数则是对不同温度单位下的温度进行换算，它们会返回不同的值。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;只有当两个类型的底层基础类型相同时，才允许这种转型操作，或者是两者都是指向相同底层结构的指针类型，这些转换只改变类型而不会影响值本身&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;包和文件&#34;&gt;包和文件&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在Go语言中，一个简单的规则是：如果一个名字是大写字母开头的，那么该名字是导出的（译注：因为汉字不区分大小写，因此汉字开头的名字是没有导出的）。&lt;/li&gt;&#xA;&lt;li&gt;在Go语言程序中，每个包都是有一个全局唯一的导入路径。导入语句中类似&amp;rdquo;gopl.io/ch2/tempconv&amp;rdquo;的字符串对应包的导入路径。Go语言的规范并没有定义这些字符串的具体含义或包来自哪里，它们是由构建工具来解释的。当使用Go语言自带的go工具箱时（第十章），一个导入路径代表一个目录中的一个或多个Go源文件。&lt;/li&gt;&#xA;&lt;li&gt;照惯例，一个包的名字和包的导入路径的最后一个字段相同，例如gopl.io/ch2/tempconv包的名字一般是tempconv。&lt;/li&gt;&#xA;&lt;li&gt;init初始化函数除了不能被调用或引用外，其他行为和普通函数类似。在每个文件中的init初始化函数，在程序开始执行时按照它们声明的顺序被自动调用。&lt;/li&gt;&#xA;&lt;li&gt;初始化工作是自下而上进行的，main包最后被初始化。以这种方式，可以确保在main函数执行之前，所有依赖的包都已经完成初始化工作了。&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;一个有意思的程序，位运算&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;package main&#xA;import (&#xA;&amp;quot;fmt&amp;quot;&#xA;)&#xA;var pc [256]byte&#xA;&#xA;func init() {&#xA;for i := range pc {&#xA;    // pc[i] = pc[i/2] + byte(i&amp;amp;1) //除以2相当于右移1位&#xA;    pc[i] = pc[i&amp;gt;&amp;gt;1] + byte(i&amp;amp;1)&#xA;}&#xA;}&#xA;func PopCount(x uint64) int {&#xA;return int(pc[byte(x&amp;gt;&amp;gt;(0*8))] +&#xA;    pc[byte(x&amp;gt;&amp;gt;(1*8))] +&#xA;    pc[byte(x&amp;gt;&amp;gt;(2*8))] +&#xA;    pc[byte(x&amp;gt;&amp;gt;(3*8))] +&#xA;    pc[byte(x&amp;gt;&amp;gt;(4*8))] +&#xA;    pc[byte(x&amp;gt;&amp;gt;(5*8))] +&#xA;    pc[byte(x&amp;gt;&amp;gt;(6*8))] +&#xA;    pc[byte(x&amp;gt;&amp;gt;(7*8))])&#xA;}&#xA;func main() {&#xA;fmt.Println(PopCount(32323))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;作用域&#34;&gt;作用域&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;声明语句的作用域对应的是一个源代码的文本区域；它是一个编译时的属性。一个变量的生命周期是指程序运行时变量存在的有效时间段，在此时间区域内它可以被程序的其他部分引用；是一个运行时的概念。&lt;/p&gt;&#xA;</description>
      <pubDate>Sat, 25 Feb 2017 11:59:27 +0000</pubDate>
    </item>
    <item>
      <title>Golang第一章:入门</title>
      <link>http://openhex.cn/2017/2/25/Golang第一章-入门.html</link>
      <description>&lt;h3 id=&#34;os-args获取参数&#34;&gt;os.Args获取参数&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;os.Args 是一个字符串的切片，它的第一个元素os.Args[0]是命令本身的名字，其他元素则是程序启动时传给它的参数。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;i++ 给i加1，是语句，而在c语言中则是表达式，因此在golang中j = i++ 是非法的。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;for循环&#34;&gt;for循环&lt;/h3&gt;&#xA;&lt;pre&gt;&lt;code&gt;for initialization; condition; post {&#xA;    // zero or more statements&#xA;}&#xA;// a traditional &amp;quot;while&amp;quot; loop&#xA;for condition {&#xA;    // ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;字符串&#34;&gt;字符串&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;string 类型可以看成一种特殊的slice, 因此可以使用len获取长度，同时支持切片操作，但是对于单个元素，如a[0]的结果是一个byte，输出来是asiic码，需要string(a[0])这样转换，但是可以通过切片操作获取子串，如a[2:]&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;是否存在某子串-子串出现次数&#34;&gt;是否存在某子串，子串出现次数&lt;/h4&gt;&#xA;&lt;pre&gt;&lt;code&gt;//contains 和containsAny都是调用Index来判断子串是否出现在字符串中&#xA;    //空格隔开子串&#xA;    fmt.Println(strings.ContainsAny(&amp;quot;hello&amp;quot;, &amp;quot;s e&amp;quot;)) //true&#xA;    fmt.Println(strings.ContainsAny(&amp;quot;hello&amp;quot;, &amp;quot;&amp;quot;))    //false&#xA;    fmt.Println(strings.ContainsAny(&amp;quot;hello&amp;quot;, &amp;quot;lo&amp;quot;))  //true&#xA;&#xA;    //count也就是字符串匹配实现的是Rabin-Karp算法，Count 是计算子串在字符串中出现的无重叠的次&#xA;    fmt.Println(strings.Count(&amp;quot;fivevev&amp;quot;, &amp;quot;ve&amp;quot;))  //2&#xA;    fmt.Println(strings.Count(&amp;quot;fivevev&amp;quot;, &amp;quot;&amp;quot;))    //8 utf8.RuneCountInString(s) + 1 也就是长度+1&#xA;    fmt.Println(strings.Count(&amp;quot;fivevev&amp;quot;, &amp;quot;vev&amp;quot;)) //1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;字符串的分割&#34;&gt;字符串的分割&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;六个三组函数：Fields 和 FieldsFunc、Split 和 SplitAfter、SplitN 和 SplitAfterN&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;fileds和filedsfunc&#34;&gt;Fileds和FiledsFunc&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;func Fields(s string) []string&#xA;func FieldsFunc(s string, f func(rune) bool) []string&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Fields 用一个或多个连续的空格分隔字符串 s,返回子字符串的数组（slice）&#xA;由于是用空格分隔，因此结果中不会含有空格或空子字符串，例如：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;Fields are: %q&amp;quot;, strings.Fields(&amp;quot;  foo bar  baz   &amp;quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;输出：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Fields are: [&amp;quot;foo&amp;quot; &amp;quot;bar&amp;quot; &amp;quot;baz&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;FieldsFunc 用这样的Unicode代码点 c 进行分隔：满足 f&amp;copy; 返回 true。该函数返回[]string。如果字符串 s 中所有的代码点(unicode code points)都满足f&amp;copy;或者 s 是空，则 FieldsFunc 返回空slice。&#xA;也就是说，我们可以通过实现一个回调函数来指定分隔字符串 s 的字符。比如上面的例子，我们通过 FieldsFunc 来实现：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;fmt.Println(strings.FieldsFunc(&amp;quot;  foo bar  baz   &amp;quot;, unicode.IsSpace))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;实际上，Fields 函数就是调用 FieldsFunc 实现的：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;func Fields(s string) []string {&#xA;    return FieldsFunc(s, unicode.IsSpace)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;split-和-splitafter-splitn-和-splitaftern&#34;&gt;Split 和 SplitAfter、 SplitN 和 SplitAfterN&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;之所以将这四个函数放在一起讲，是因为它们都是通过一个同一个内部函数来实现的。它们的函数签名及其实现：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;func Split(s, sep string) []string { return genSplit(s, sep, 0, -1) }&#xA;func SplitAfter(s, sep string) []string { return genSplit(s, sep, len(sep), -1) }&#xA;func SplitN(s, sep string, n int) []string { return genSplit(s, sep, 0, n) }&#xA;func SplitAfterN(s, sep string, n int) []string { return genSplit(s, sep, len(sep), n) }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Split和SplitAfter的区别是，After会保留分隔符&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;//分割字符串&#xA;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.Split(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;))      //[&amp;quot;foo&amp;quot; &amp;quot;bar&amp;quot; &amp;quot;baz&amp;quot;]&#xA;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.SplitAfter(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;)) //[&amp;quot;foo,&amp;quot; &amp;quot;bar,&amp;quot; &amp;quot;baz&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;带 N 的方法可以通过最后一个参数 n 控制返回的结果中的 slice 中的元素个数，当 n &amp;lt; 0 时，返回所有的子字符串；当 n == 0 时，返回的结果是 nil；当 n &amp;gt; 0 时，表示返回的 slice 中最多只有 n 个元素，其中，最后一个元素不会分割，比如：&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;pre&gt;&lt;code&gt;fmt.Printf(&amp;quot;%q\n&amp;quot;, strings.SplitN(&amp;quot;foo,bar,baz&amp;quot;, &amp;quot;,&amp;quot;, 2))&#xA;//[&amp;quot;foot&amp;quot; &amp;quot;bar,baz&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;字符串数组-或slice-的连接&#34;&gt;字符串数组(或slice)的连接&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;func Join(a []string, sep string) string&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;标准库的是实现方式&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;func Join(a []string, sep string) string {&#xA;    if len(a) == 0 {&#xA;        return &amp;quot;&amp;quot;&#xA;    }&#xA;    if len(a) == 1 {&#xA;        return a[0]&#xA;    }&#xA;    n := len(sep) * (len(a) - 1)&#xA;    for i := 0; i &amp;lt; len(a); i++ {&#xA;        n += len(a[i])&#xA;    }&#xA;    b := make([]byte, n)&#xA;    bp := copy(b, a[0])&#xA;    for _, s := range a[1:] {&#xA;        bp += copy(b[bp:], sep)&#xA;        bp += copy(b[bp:], s)&#xA;    }&#xA;    return string(b)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;标准库的实现没有用 bytes 包，当然也不会简单的通过 + 号连接字符串。Go 中是不允许循环依赖的，标准库中很多时候会出现代码拷贝，而不是引入某个包。这里 Join 的实现方式挺好，我个人猜测，不直接使用 bytes 包，也是不想依赖 bytes 包（其实 bytes 中的实现也是 copy 方式）。&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;字符串替换&#34;&gt;字符串替换&lt;/h5&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;pre&gt;&lt;code&gt;// 用 new 替换 s 中的 old，一共替换 n 个。&#xA;// 如果 n &amp;lt; 0，则不限制替换次数，即全部替换&#xA;func Replace(s, old, new string, n int) string&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;只能替换一种string&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;Replacer&#xA;这是一个结构，没有导出任何字段，实例化通过 func NewReplacer(oldnew &amp;hellip;string) *Replacer 函数进行，其中不定参数 oldnew 是 old-new 对，即进行多个替换。&#xA;解决上面说的替换一种的问题：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;r := strings.NewReplacer(&amp;quot;&amp;lt;&amp;quot;, &amp;quot;&amp;amp;lt;&amp;quot;, &amp;quot;&amp;gt;&amp;quot;, &amp;quot;&amp;amp;gt;&amp;quot;)&#xA;fmt.Println(r.Replace(&amp;quot;This is &amp;lt;b&amp;gt;HTML&amp;lt;/b&amp;gt;!&amp;quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;另外，Replacer 还提供了另外一个方法：&#xA;func (r *Replacer) WriteString(w io.Writer, s string) (n int, err error)&#xA;它在替换之后将结果写入 io.Writer 中。&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
      <pubDate>Sat, 25 Feb 2017 11:59:10 +0000</pubDate>
    </item>
    <item>
      <title>mmap内存映射</title>
      <link>http://openhex.cn/2017/2/17/mmap内存映射.html</link>
      <description>&lt;h2 id=&#34;相关知识&#34;&gt;相关知识&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;内存映射是个很有用，也很有意思的思想。我们都知道操作系统分为用户态和内核态，用户态是不能直接和物理设备打交道的，如果想把硬盘的一块区域读到用户态，则需要两次拷贝(硬盘-&amp;gt;内核-&amp;gt;用户)，但是内存映射的设计只需要发生一次的拷贝，大大的提高了读取数据的效率。那么内存映射的原理和内核是如何实现的呢？&lt;/p&gt;&#xA;&#xA;&lt;p&gt;因为内存映射涉及到虚拟内存的管理，虚拟内存到物理内存的映射，因此在详细介绍内存映射前先普及(回忆)一下相关的概念。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;cpu的架构&#34;&gt;CPU的架构&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;目前CPU架构主要分为三种：SMP, NUMA, MPP，详细的介绍参考&lt;a href=&#34;http://kdf5000.com/2017/02/17/CPU架构/&#34;&gt;CPU架构&lt;/a&gt;。这里主要关注MMU和TLB在CPU中的角色和位置。&#xA;&lt;img src=&#34;/media/archive/blog/images/2017/cpu.png&#34; alt=&#34;CPU&#34; /&gt;&#xA;上图是Intel的i7处理器一个核的架构图，图中可以看出每一个CPU核都已一个自己的MMU和TLB。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;linux进程和线程&#34;&gt;Linux进程和线程&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;关于Linux进程和线程的话题想必大家都不陌生，这也是面试时候经常被问到的问题，后续我会专门写一篇博文介绍Linux线程和进程的区别，已经内核是如何实现的。这里我们只需要知道下面几个概念即可：&#xA;* 操作系统本身也是一个进程&#xA;* 操作系统内核层面没有线程的概念，只有进程的概念，因此线程在内核看来也是一个进程，只是比较特殊&#xA;* 线程的实现是glibc实现的(pthread)，包括创建，管理等&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;虚拟地址&#34;&gt;虚拟地址&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;虚拟地址是面向进程的一套虚拟内存的地址，为了更好的管理内存，并且能够保证内存对程序员而言是透明的，也就是写程序的时候对每一个程序来说都是一样的地址空间，因此提出了虚拟内存的概念，对应的就是虚拟地址。这里我们不关注具体的细节，为了后面的讲解，我们简单的了解一下虚拟内存在内核层面是怎么进行管理的。&#xA;&lt;img src=&#34;/media/archive/blog/images/2017/vm_manage.png&#34; alt=&#34;CPU&#34; /&gt;&#xA;这张图右侧的进程虚拟器对于每一个进程都是一样的，就是上面说的虚拟空间，然后对于进程，内核对每一个进程都维护了一个task_struch的结构，其中有三个重要的成员，pgd指向改进程的页表首地址，然后mmap和mm_rb都是用来管理vm的结构，vm是虚拟内存管理的基本单元，含有内存的类型，起始和结束地址，mmap是线性表实现的，mm_rb则使用红黑树进行管理，分别适用不同的场景。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;mmu&#34;&gt;MMU&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;MMU(Memory Management Unit),内存管理单元，主要负责CPU内存访问的时候将虚拟地址转换为物理地址的单元。也就是说CPU想要访问内存必须先经过MMU的转换，获得真正的物理地址，才能读写物理内存的数据。其实MMU只是一个简单的计算单元，它通过虚拟地址查找页表，找到对应的物理地址，然后返回给CPU。在查找页表的过程中可能发生多次的物理内存访问，这取决于系统采用页表管理系统是几级的，目前Linux的页表是四级的，因此需要查询四次页表，每一次查询都会获得一个物理地址指向下一级页表的内存地址，知道最后获得实际要访问的内存物理地址。&#xA;&lt;img src=&#34;/media/archive/blog/images/2017/mmu.png&#34; alt=&#34;CPU&#34; /&gt;&#xA;从图中可以看出，当CPU得到一个虚拟地址去访问内存的时候会先将虚拟地址发送到MMU(1)，然后MMU会先从TLB查询是否存在这个虚拟地址到物理地址的缓存，如果存在则直接返回给CPU，如果没有则会根据虚拟地址讲过计算获得物理内存中页表项的地址然后读取得到PTE(Linux可能要读取四次)，PTE就是物理内存地址或者硬盘存储地址。然后MMU会将该PTE缓存在TLB中，最后使用这个物理地址再次访问物理内存或者硬盘地址获得要访问的内容。&lt;/p&gt;&#xA;&#xA;&lt;h2 id=&#34;内存映射&#34;&gt;内存映射&lt;/h2&gt;&#xA;&#xA;&lt;h3 id=&#34;mmap&#34;&gt;mmap&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;基础知识介绍完毕，那么到底什么是内存映射呢，映射让我们不禁都会想起数学上的映射关系，是的就是那个意思，这里是讲设备或者硬盘存储的一块空间映射到物理内存，然后操作这块物理内存就是在操作实际的硬盘空间，不需要经过内核态传递。比如你的硬盘上有一个文件，你可以使用linux系统提供的mmap接口，讲这个文件映射到进程一块虚拟地址空间，这块空间会对应一块物理内存，当你读写这块物理空间的时候，就是在读取实际的磁盘文件，就是这么直接，这么高效。通常诸如共享库的加载都是通过内存映射的方式加载到物理内存的。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;mmap本身其实是一个很简单的操作，在进程的页表中添加一个页表项，该页表项是物理内存的地址。调用mmap的时候，内核会在改进程的虚拟空间的映射区域查找一块满足需求的空间用于映射该文件，然后生成该虚拟地址的页表项，改页表项此时的有效位(标志是否已经在物理内存中)为0，页表项的内容是文件的磁盘地址，此时mmap的任务已经完成。&#xA;&lt;img src=&#34;/media/archive/blog/images/2017/mmap.png&#34; alt=&#34;CPU&#34; /&gt;&#xA;当mmap建立完页表的映射后，就可以操作改块内存了，进行的所有改动都会自动写会磁盘文件。第一次访问该块内存的时候，因为页表项的有效位还是0，就会发生缺页中断，然后CPU会使用该页表项的内容也就是磁盘的文件地址，讲该地址指向的内容加载到物理内存，并需改页表项的内容为该物理地址，有效位置为1.&#xA;&lt;img src=&#34;/media/archive/blog/images/2017/mmap_pgfault.png&#34; alt=&#34;CPU&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;munmap&#34;&gt;munmap&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;有映射必然有解除映射，系统提供了一个munmap的接口去解除指定地址的映射关系。munmap主要的功能是清除页表项，解除这个映射关系，但是这个过程中会涉及到缓存的刷新，虚拟内存vm的删除，TLB的一致性(tlb shootdown操作)，这里就不再详细讲解。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;附件&#34;&gt;附件&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;a src=&#34;/media/archive/blog/images/2017/内存映射.pptx&#34;&gt;1. 内存映射&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 17 Feb 2017 21:50:12 +0000</pubDate>
    </item>
    <item>
      <title>CPU架构</title>
      <link>http://openhex.cn/2017/2/17/CPU架构.html</link>
      <description>&lt;p&gt;目前流行的架构主要有三种: 对称多处理器结构 (SMP ： Symmetric Multi-Processor) ，非一致存储访问结构 (NUMA ： Non-Uniform Memory Access) ，以及海量并行处理结构 (MPP ： Massive Parallel Processing)&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;对称多处理器架构&#34;&gt;对称多处理器架构&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;下面是维基百科的解释&#xA;&amp;gt; SMP(Symmetric multiprocessing)对称多处理，也译为均衡多处理、对称性多重处理，是一种多处理器的电脑硬件架构，在对称多处理架构下，每个处理器的地位都是平等的，对资源的使用权限相同。现代多数的多处理器系统，都采用对称多处理架构，也被称为对称多处理系统（Symmetric multiprocessing system）。在这个系统中，拥有超过一个以上的处理器，这些处理器都连接到同一个共享的主内存上，并由单一操作系统来控制。在多核心处理器的例子中，对称多处理架构，将每一个核心都当成是独立的处理器。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;所谓对称多处理器结构，是指服务器中多个 CPU 对称工作，无主次或从属关系。各 CPU 共享相同的物理内存，每个 CPU 访问内存中的任何地址所需时间是相同的，因此 SMP 也被称为一致存储器访问结构 (UMA ： Uniform Memory Access) 。多个CPU之间没有区别，平等地访问内存、外设、一个操作系统。操作系统管理着一个队列，每个处理器依次处理队列中的进程。如果两个处理器同时请求访问一个资源（例如同一段内存地址），由硬件、软件的锁机制去解决资源争用问题。对 SMP 服务器进行扩展的方式包括增加内存、使用更快的 CPU 、增加 CPU 、扩充 I/O( 槽口数与总线数 ) 以及添加更多的外部设备 ( 通常是磁盘存储 ) 。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;由于这种共享的特征，导致了SMP架构的扩展能力非常有限，比如内存的访问，随着CPU的增多，则访问同一物理地址的CPU讲增多，则访问冲突迅速增加，不断的加锁，讲导致性能下降。实验证明，SMP服务器的CPU最好的情况是2-4个CPU.&#xA;&lt;img src=&#34;/media/archive/blog/images/2017/SMP.png&#34; alt=&#34;NUMA&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;numa-non-uniform-memory-access&#34;&gt;NUMA(Non-Uniform Memory Access)&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;由于SMP系统的扩展性问题，其不适合大型系统的需求，经过众多科研工作者的努力，研制出了NUMA架构(非一致存储访问)。和SMP相比，NUMA所有的cpu(核)并不是都共享一个总线，NUMA架构讲处理器分为不同的节点，每个节点可以有多个CPU，一个节点内的CPU共享一个总线，内存和IO，然后不同的节点通过互联模块进行信息的交换。因此跨节点的内存访问的延迟远大于本地内存访问，设计程序的时候要尽可能不要跨节点访问内存。&#xA;下图很好的展示了SMP和NUMA两种架构的区别。&#xA;&lt;img src=&#34;/media/archive/blog/images/2017/NUMA.png&#34; alt=&#34;NUMA&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;MPP (Massively Parallel Processing)，大规模并行处理系统，这样的系统是由许多松耦合的处理单元组成的，要注意的是这里指的是处理单元而不是处理器。每个单元内的CPU都有自己私有的资源，如总线，内存，硬盘等。在每个单元内都有操作系统和管理数据库的实例复本。这种结构最大的特点在于不共享资源。NUMA架构很好地解决了扩展性的问题，一个NUMA服务器可以有上百个CPU，但是随着核数的增加系统的性能并不是线性的增加，这主要还是由于远端内存的访问效率远远低于本地内存。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;mpp-massive-parallel-processing&#34;&gt;MPP（Massive Parallel Processing)&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;和 NUMA 不同， MPP 提供了另外一种进行系统扩展的方式，它由多个 SMP 服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。其基本特征是由多个 SMP 服务器 ( 每个 SMP 服务器称节点 ) 通过节点互联网络连接而成，每个节点只访问自己的本地资源 ( 内存、存储等 ) ，是一种完全无共享 (Share Nothing) 结构，因而扩展能力最好，理论上其扩展无限制，目前的技术可实现 512 个节点互联，数千个 CPU。每个节点不能访问另一个节点的内存，必须通过节点互联网络实现的，这个过程一般称为数据重分配 (Data Redistribution）&#xA;MPP (Massively Parallel Processing)，大规模并行处理系统，这样的系统是由许多松耦合的处理单元组成的，要注意的是这里指的是处理单元而不是处理器。每个单元内的CPU都有自己私有的资源，如总线，内存，硬盘等。在每个单元内都有操作系统和管理数据库的实例复本。这种结构最大的特点在于不共享资源。&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 17 Feb 2017 21:47:57 +0000</pubDate>
    </item>
    <item>
      <title>Ansible: 自动化管理服务器(2)</title>
      <link>http://openhex.cn/2017/1/12/Ansible-自动化管理服务器-2.html</link>
      <description>&lt;h3 id=&#34;ansible-命令参数&#34;&gt;Ansible 命令参数&lt;/h3&gt;&#xA;&#xA;&lt;table&gt;&#xA;&lt;thead&gt;&#xA;&lt;tr&gt;&#xA;&lt;th&gt;参数&lt;/th&gt;&#xA;&lt;th&gt;含义&lt;/th&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/thead&gt;&#xA;&#xA;&lt;tbody&gt;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-v&lt;/td&gt;&#xA;&lt;td&gt;详细模式，如果命令执行成功，输出详细的结果&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-i Path&lt;/td&gt;&#xA;&lt;td&gt;指定host文件的路径，默认是/etc/ansible/hosts&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-f Num&lt;/td&gt;&#xA;&lt;td&gt;指定一个整数，默认是5，指定fork开启同步进程的个数&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-m Name&lt;/td&gt;&#xA;&lt;td&gt;指定使用的module名称，默认是command&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-M path&lt;/td&gt;&#xA;&lt;td&gt;指定module的目录。默认是/usr/share/ansible&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-a Module_Args&lt;/td&gt;&#xA;&lt;td&gt;指定module的参数&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-k&lt;/td&gt;&#xA;&lt;td&gt;提示输入ssh密码，而不是基于ssh的密钥认证&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-sudo&lt;/td&gt;&#xA;&lt;td&gt;指定使用sudo获得root权限&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-K&lt;/td&gt;&#xA;&lt;td&gt;提示输入sudo密码，与-sudo一起使用&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-u Username&lt;/td&gt;&#xA;&lt;td&gt;指定被控机器的执行用户&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;-C&lt;/td&gt;&#xA;&lt;td&gt;测试此命令会改变什么内容，不会真正的去执行&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;ansible的常用模块&#34;&gt;Ansible的常用模块&lt;/h3&gt;&#xA;&#xA;&lt;h4 id=&#34;shell模块&#34;&gt;shell模块&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;使用shell来执行命令，如&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ansible all -m shell -a &amp;quot;mkdir kongdefei&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;将会在每一个客户机上创建kongdefei的目录&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;copy模块&#34;&gt;copy模块&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;实现主控机向客户机复制文件，类似scp的功能，比如复制主控机器的/etc/hosts到客户端机&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ansible all -m copy -a &amp;quot;src=/etc/hosts dest=/tmp/hosts&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;file模块&#34;&gt;file模块&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;该模块称之为文件属性模块，可以创建，删除，修改文件属性等操作&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#创建文件&#xA;$ansible all -m file -a &amp;quot;dest=/tmp/a.txt state=touch&amp;quot;&#xA;#更改文件的用户和权限&#xA;$ansible all -m file -a &amp;quot;dest=/tmp/a.txt mode=600 owener=www-data group=root&amp;quot;&#xA;#创建目录&#xA;$ansible all -m file -a &amp;quot;dest=/tmp/kongdefei owner=kongdefei group=root state=directory&amp;quot;&#xA;#删除文件或者目录&#xA;$ansible all -m file -a &amp;quot;dest=/tmp/kongdefei state=absent&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt;state的其他选项：link(链接)、hard(硬链接)&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;stat模块&#34;&gt;stat模块&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;获取指定文件的状态信息，比如atime, ctime, mtime, md5, uid, gid等&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ansible all -m stat -a &amp;quot;path=/home/kongdefei/tidb/tidb-latest-linux-amd64.tar.gz&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;返回&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;172.16.1.96 | SUCCESS =&amp;gt; {&#xA;    &amp;quot;changed&amp;quot;: false, &#xA;    &amp;quot;stat&amp;quot;: {&#xA;        &amp;quot;atime&amp;quot;: 1484225125.1479475, &#xA;        &amp;quot;checksum&amp;quot;: &amp;quot;b3afe00a995630184d98194c1a61d6f8fced1f05&amp;quot;, &#xA;        &amp;quot;ctime&amp;quot;: 1484225125.5839477, &#xA;        &amp;quot;dev&amp;quot;: 2049, &#xA;        &amp;quot;executable&amp;quot;: false, &#xA;        &amp;quot;exists&amp;quot;: true, &#xA;        &amp;quot;gid&amp;quot;: 0, &#xA;        &amp;quot;gr_name&amp;quot;: &amp;quot;root&amp;quot;, &#xA;        &amp;quot;inode&amp;quot;: 27787277, &#xA;        &amp;quot;isblk&amp;quot;: false, &#xA;        &amp;quot;ischr&amp;quot;: false, &#xA;        &amp;quot;isdir&amp;quot;: false, &#xA;        &amp;quot;isfifo&amp;quot;: false, &#xA;        &amp;quot;isgid&amp;quot;: false, &#xA;        &amp;quot;islnk&amp;quot;: false, &#xA;        &amp;quot;isreg&amp;quot;: true, &#xA;        &amp;quot;issock&amp;quot;: false, &#xA;        &amp;quot;isuid&amp;quot;: false, &#xA;        &amp;quot;md5&amp;quot;: &amp;quot;1de3012618bc0c1d1e42534d365a8ca5&amp;quot;, &#xA;        &amp;quot;mode&amp;quot;: &amp;quot;0644&amp;quot;, &#xA;        &amp;quot;mtime&amp;quot;: 1484225124.8959477, &#xA;        &amp;quot;nlink&amp;quot;: 1, &#xA;        &amp;quot;path&amp;quot;: &amp;quot;/home/kongdefei/tidb/tidb-latest-linux-amd64.tar.gz&amp;quot;, &#xA;        &amp;quot;pw_name&amp;quot;: &amp;quot;root&amp;quot;, &#xA;        &amp;quot;readable&amp;quot;: true, &#xA;        &amp;quot;rgrp&amp;quot;: true, &#xA;        &amp;quot;roth&amp;quot;: true, &#xA;        &amp;quot;rusr&amp;quot;: true, &#xA;        &amp;quot;size&amp;quot;: 51645042, &#xA;        &amp;quot;uid&amp;quot;: 0, &#xA;        &amp;quot;wgrp&amp;quot;: false, &#xA;        &amp;quot;woth&amp;quot;: false, &#xA;        &amp;quot;writeable&amp;quot;: true, &#xA;        &amp;quot;wusr&amp;quot;: true, &#xA;        &amp;quot;xgrp&amp;quot;: false, &#xA;        &amp;quot;xoth&amp;quot;: false, &#xA;        &amp;quot;xusr&amp;quot;: false&#xA;    }&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;管理软件模块&#34;&gt;管理软件模块&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;指像yum, apt等管理包的软件&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#安装nginx软件包&#xA;$ansible web -m yum -a &amp;quot;name=nginx state=present&amp;quot;&#xA;$ansible web -m apt -a &amp;quot;name=nginx state=present&amp;quot;&#xA;&#xA;#安装包到一个特定的版本&#xA;$ansible web -m yum -a &amp;quot;name=nginx-1.6.2 state=present&amp;quot;&#xA;$ansible web -m apt -a &amp;quot;name=nginx-1.6.2 state=present&amp;quot;&#xA;&#xA;#指定某个源仓库安装某软件包&#xA;$ansible web -m yum -a &amp;quot;name=php55w enablerepo=remi state=present&amp;quot;&#xA;&#xA;#更新一个软件包是最新版本&#xA;$ansible web -m yum -a &amp;quot;name=nginx state=latest&amp;quot;&#xA;$ansible web -m apt -a &amp;quot;name=nginx state=latest&amp;quot;&#xA;&#xA;#卸载一个软件&#xA;$ansible web -m yum -a &amp;quot;name=nginx state=absent&amp;quot;&#xA;$ansible web –m apt -a &amp;quot;name=nginx state=absent&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Ansible 支持很多操作系统的软件包管理，使用时 -m 指定相应的软件包管理工具模块，如果没有这样的模块，可以自己定义类似的模块或者使用 command 模块来安装软件包&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;user模块&#34;&gt;User模块&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;用户模块主要用于创建，更改，删除用户&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#创建一个用户&#xA;$ansible all -m user -a &amp;quot;name=hadoop password=123456&amp;quot;&#xA;&#xA;#删除用户&#xA;ansible all -m user -a &amp;quot;name=hadoop state=absent&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;service模块&#34;&gt;service模块&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;主要是控制被控机的各种service。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#启动,重启，停止httpd服务&#xA;$ansible all -m service -a &amp;quot;name=httpd state=started(restarted|stopped)&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Thu, 12 Jan 2017 21:07:04 +0000</pubDate>
    </item>
    <item>
      <title>Ansible: 自动化管理服务器(1)</title>
      <link>http://openhex.cn/2017/1/12/Ansible-自动化管理服务器-1.html</link>
      <description>&lt;h2 id=&#34;ansible配置&#34;&gt;Ansible配置&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Ansible 提供一种最简单的方式用于发布、管理和编排计算机系统的工具，你可在数分钟内搞定&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ansible 是一个模型驱动的配置管理器，支持多节点发布、远程任务执行。默认使用 SSH 进行远程连接。无需在被管理节点上安装附加软件，可使用各种编程语言进行扩展&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;http://7xtgvp.com2.z0.glb.clouddn.com/blog/images/Ansible.png&#34; alt=&#34;Ansible&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;安装&#34;&gt;安装&lt;/h3&gt;&#xA;&#xA;&lt;h4 id=&#34;安装节点&#34;&gt;安装节点&lt;/h4&gt;&#xA;&#xA;&lt;table&gt;&#xA;&lt;thead&gt;&#xA;&lt;tr&gt;&#xA;&lt;th&gt;节点&lt;/th&gt;&#xA;&lt;th&gt;ip&lt;/th&gt;&#xA;&lt;th&gt;操作系统&lt;/th&gt;&#xA;&lt;th&gt;角色&lt;/th&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/thead&gt;&#xA;&#xA;&lt;tbody&gt;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;gd67&lt;/td&gt;&#xA;&lt;td&gt;172.16.1.67&lt;/td&gt;&#xA;&lt;td&gt;Ubuntu 14.04&lt;/td&gt;&#xA;&lt;td&gt;Controller Node&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;gd86&lt;/td&gt;&#xA;&lt;td&gt;172.16.1.86&lt;/td&gt;&#xA;&lt;td&gt;Ubuntu 14.04&lt;/td&gt;&#xA;&lt;td&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;gd87&lt;/td&gt;&#xA;&lt;td&gt;172.16.1.87&lt;/td&gt;&#xA;&lt;td&gt;Ubuntu 14.04&lt;/td&gt;&#xA;&lt;td&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;gd88&lt;/td&gt;&#xA;&lt;td&gt;172.16.1.88&lt;/td&gt;&#xA;&lt;td&gt;Ubuntu 14.04&lt;/td&gt;&#xA;&lt;td&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;gd89&lt;/td&gt;&#xA;&lt;td&gt;172.16.1.89&lt;/td&gt;&#xA;&lt;td&gt;Ubuntu 14.04&lt;/td&gt;&#xA;&lt;td&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;gd92&lt;/td&gt;&#xA;&lt;td&gt;172.16.1.92&lt;/td&gt;&#xA;&lt;td&gt;Ubuntu 14.04&lt;/td&gt;&#xA;&lt;td&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;gd93&lt;/td&gt;&#xA;&lt;td&gt;172.16.1.93&lt;/td&gt;&#xA;&lt;td&gt;Ubuntu 14.04&lt;/td&gt;&#xA;&lt;td&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;gd94&lt;/td&gt;&#xA;&lt;td&gt;172.16.1.94&lt;/td&gt;&#xA;&lt;td&gt;Ubuntu 14.04&lt;/td&gt;&#xA;&lt;td&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;gd95&lt;/td&gt;&#xA;&lt;td&gt;172.16.1.95&lt;/td&gt;&#xA;&lt;td&gt;Ubuntu 14.04&lt;/td&gt;&#xA;&lt;td&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td&gt;gd96&lt;/td&gt;&#xA;&lt;td&gt;172.16.1.96&lt;/td&gt;&#xA;&lt;td&gt;Ubuntu 14.04&lt;/td&gt;&#xA;&lt;td&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&#xA;&lt;h4 id=&#34;安装ansible&#34;&gt;安装Ansible&lt;/h4&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo apt-get install software-properties-common&#xA;$ sudo apt-add-repository ppa:ansible/ansible&#xA;$ sudo apt-get update&#xA;$ sudo apt-get install ansible&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;免密登录&#34;&gt;免密登录&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;设置ControllerNode能够免密登录被管理的远程主机，使用系统自带的&lt;code&gt;ssh-copy-id&lt;/code&gt;复制ssh的公钥到被控主机，下面是自动运行的脚本&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;#!/usr/bin/env bash&#xA;hosts=(&#xA;&amp;quot;172.16.1.86&amp;quot;&#xA;&amp;quot;172.16.1.87&amp;quot;&#xA;&amp;quot;172.16.1.88&amp;quot;&#xA;&amp;quot;172.16.1.89&amp;quot;&#xA;&amp;quot;172.16.1.92&amp;quot;&#xA;&amp;quot;172.16.1.93&amp;quot;&#xA;&amp;quot;172.16.1.94&amp;quot;&#xA;&amp;quot;172.16.1.95&amp;quot;&#xA;&amp;quot;172.16.1.96&amp;quot;&#xA;)&#xA;for host in ${hosts[@]};&#xA;do&#xA;    expect &amp;lt;&amp;lt;EOF&#xA;    spawn ./ssh-copy-id root@$host&#xA;    expect {&#xA;        &amp;quot;(yes/no)?&amp;quot; { send &amp;quot;yes\n&amp;quot;; exp_continue  }&#xA;        &amp;quot;password:&amp;quot; { send &amp;quot;123456\n&amp;quot;  }&#xA;    }&#xA;    expect eof&#xA; EOF&#xA;done&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;脚本里的被控主机列表，用户名和密码根据自己的情况修改&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;使用&#34;&gt;使用&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;现在就可以开始你的第一条命令了，不过首先需要将所有被控主机添加到ansible的host里，即/etc/ansible/hosts中&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;172.16.1.86&#xA;172.16.1.87&#xA;172.16.1.88&#xA;172.16.1.89&#xA;172.16.1.92&#xA;172.16.1.93&#xA;172.16.1.94&#xA;172.16.1.95&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;现在就可以执行第一条命令了&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ansible all -m ping&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Ansible会像SSH那样试图用你的当前用户名来连接你的远程机器.要覆写远程用户名,只需使用’-u’参数. 如果你想访问 sudo模式,这里也有标识(flags)来实现:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# as devin&#xA;$ ansible all -m ping -u root&#xA;# as devin, sudoing to root&#xA;$ ansible all -m ping -u root --sudo&#xA;# as devin, sudoing to batman&#xA;$ ansible all -m ping -u root --sudo --sudo-user kdf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;注意：&lt;/strong&gt;上面免密登录的步骤使用的用户名要和现在一样&lt;/p&gt;&#xA;&#xA;&lt;p&gt;如果得到下面的结果，那么恭喜你已经安装配置成功了！&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;root@gd67:/home/kongdefei/scripts# ansible all -m ping&#xA;172.16.1.86 | SUCCESS =&amp;gt; {&#xA;    &amp;quot;changed&amp;quot;: false, &#xA;    &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;&#xA;}&#xA;172.16.1.88 | SUCCESS =&amp;gt; {&#xA;    &amp;quot;changed&amp;quot;: false, &#xA;    &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;&#xA;}&#xA;172.16.1.87 | SUCCESS =&amp;gt; {&#xA;    &amp;quot;changed&amp;quot;: false, &#xA;    &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;&#xA;}&#xA;172.16.1.92 | SUCCESS =&amp;gt; {&#xA;    &amp;quot;changed&amp;quot;: false, &#xA;    &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;&#xA;}&#xA;172.16.1.89 | SUCCESS =&amp;gt; {&#xA;    &amp;quot;changed&amp;quot;: false, &#xA;    &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;&#xA;}&#xA;172.16.1.93 | SUCCESS =&amp;gt; {&#xA;    &amp;quot;changed&amp;quot;: false, &#xA;    &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;&#xA;}&#xA;172.16.1.94 | SUCCESS =&amp;gt; {&#xA;    &amp;quot;changed&amp;quot;: false, &#xA;    &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;&#xA;}&#xA;172.16.1.95 | SUCCESS =&amp;gt; {&#xA;    &amp;quot;changed&amp;quot;: false, &#xA;    &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;&#xA;}&#xA;172.16.1.96 | SUCCESS =&amp;gt; {&#xA;    &amp;quot;changed&amp;quot;: false, &#xA;    &amp;quot;ping&amp;quot;: &amp;quot;pong&amp;quot;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Thu, 12 Jan 2017 00:12:33 +0000</pubDate>
    </item>
    <item>
      <title>2016年总结</title>
      <link>http://openhex.cn/2017/1/1/2016年总结.html</link>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;这个世界的奇妙之处就在于，你永远不知道那些年龄比你小却比你努力的人，蕴藏多么巨大的能量，明明两年前刚见面还是一个略显青涩的邻家男孩，再见面已成了有口皆碑的专家大牛，惶惶乎一种朝为田舍郎，暮登天子堂的感觉油然而生，正是应了那句老话，士别三日，当刮目相待！&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;2016年，我的本命年，从年初就露出了今年注定是不顺的一年的迹象。刚过完年就和最好的朋友因为一些问题发生了争吵，刚到学校用了四年的笔记本就彻底罢工，总之，一切都是那么地霉气。不过一年走完，回头想想，虽然遇到很多不开心的事情，也并不是那么的不堪，收获了很多，似乎自己也变的更加地成熟了一些。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;初心&#34;&gt;初心&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;因为2015年在怀柔的半年上课，对学校的课程彻底的失去了兴趣，每日匆匆忙忙的去上课，看似日程满满，然而每门课又都是那么的无味，有的课内容挺好，但是怎么就感觉都是浅尝辄止，似乎老师也是匆匆忙忙。当然也是存在一些不错的课程的，比如俆君老师的网络数据挖掘，讲课总是那么激情，每个主题都以工业界为背景，也许是因为在来计算所前一直在工业界的缘故吧；陈世敏老师(清华本科，CMU博士)的大数据系统于大规模数据分析，陈老师年纪轻轻就是“百人计划”入选者，在系统方面也有着其独特的见解和眼光。总之，对上课完全失去了兴趣，因此在与导师的沟通下，逐渐的开始融入实验室的工作，希望能够早日找到自己感兴趣的方向。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;为什么读研?  读研的想法最初是产生于大二的时候种子班的经历，种子班当时号称项目引导教学，真实讲理论和实践相结合。但是体验过之后，想一想更像是提前进入了公司实习，几乎完全忽略了理论知识的学习，比如当时一周学完模电，然后自己做了一个小音响，电路都是现成的只是焊焊板子。这样真的有用吗？当然现在种子班依然看着很成功，其历届毕业的学生也都遍布互联网各个大公司。至于好不好，这有涉及到另一个问题---大学教育的意义何在。从那个时候就下决心，这不是我想要的生活，我不甘心就这么像个技工似毕业直接去工作。我要读研，要在一个方向深入分析，做出可以拿得出手的成果(当时真的好天真)，从此努力学习文化课，争取大四能够拿到保研名额。当然，从大二开始，加权也是一年比一年好，最后大四顺利的成为了当年年度年级第一，也争取到了保研名额，成功保送到了计算所。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;今年一年的实验室工作，对我们组的工作有了更深的了解，才意识到自己是多么的天真，读研并不是想象中的那样，也曾有过休学的念头，也曾无比的反感老板安排的杂碎的任务，但是想一想这是多么的可笑。人生本来就是这样，如果一切都按照你心中想的那样发展，真的就是你喜欢的吗？生活需要挑战，需要锻炼自己平衡工作和兴趣的能力。这一年的磕磕碰碰，调研了安全，设计了一个基于大数据技术的安全系统，深入研究了多核操作系统RainForest，终于找到了自己的兴趣所在，也算的上幸运。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;收获&#34;&gt;收获&lt;/h3&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;从研一入学没再问家里要一分钱，学费，生活费都是自己实习，奖学金以及研究生工资所得&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;用自己的积蓄买了Mac Pro，一辆一千多的山地车&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;从研一到现在做了几个团队项目和个人项目，提高了自己分析问题和组织团队的能力&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://huntcode.herokuapp.com&#34;&gt;HuntCode&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;类似于Demo8, Next的形式，HuntCode专注于开源项目的分享，将你所知道的优秀开源项目分享出去，让其他人可以在这里发现酷炫的开源项目，同时也能推广自己或者其他人的优秀项目&lt;/p&gt;&#xA;&#xA;&lt;p&gt;​&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/image/blog/huntcode.png&#34; alt=&#34;HuntCode&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/KDF5000/HydraHbaseCache&#34;&gt;HydraHbaseCache&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;解决Hbase热点数据响应速度缓慢的问题，使用Redis在HBase上面建一层分布式缓存最后提供一个新的Client SDK&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;项目地址：&lt;/strong&gt;&lt;a href=&#34;https://github.com/KDF5000/HydraHbaseCache&#34;&gt;https://github.com/KDF5000/HydraHbaseCache&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/image/blog/hbase.png&#34; alt=&#34;hbase&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://kdf5000.com/2016/11/12/使用Python和BitTorrentSync定期给Kindle推送电子书/&#34;&gt;Kindle伴侣&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;使用Python开发，利用Bittorrent和Resilio Sync，自动发送电子书到Kindle，目前部署在树莓派上稳定运行&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;项目地址:&lt;/strong&gt;&lt;a href=&#34;https://github.com/KDF5000/WeeklyBook&#34;&gt;https://github.com/KDF5000/WeeklyBook&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/image/blog/raspberry.jpeg&#34; alt=&#34;raspberry&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;基于大数据技术的安全防御系统&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;为了解决现在流向的IDS，单机性能瓶颈以及提高单纯的规则引擎检测异常事件的鲁棒性，利用现在大数据技术设计了一个安全防御系统，目前尚未实现，2017年上半年的首要任务。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/image/blog/Security2.png&#34; alt=&#34;Security2&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/KDF5000/ICTAuth&#34;&gt;ICTAuth&lt;/a&gt;: 命令行认证ICT网络，适用于没有图形化界面的情况下连接网络，比如树莓派，目前用于树莓派，保证IPV6 Nginx代理24小时不断网的要求。&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://pdos.csail.mit.edu/6.824/&#34;&gt;MIT6.824 Spring 2016&lt;/a&gt;断断续续做到了Lab3-Part B， 学习了Golang，简直爱上了它，然后实现了简易版hadoop的map,reduce，实现了Raft，实现了一个简单的kvserver。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;获得了华为硕士生奖学金&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;跟着师兄投了一篇ASPLOS的论文，虽然被拒了，现在继续做，明年准备投SOSP&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h3 id=&#34;未完成&#34;&gt;未完成&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;其实没完成的事情有很多，比如计划年底完成MIT6.824课程的，但是由于实验室事情，一直拖延，不过还是自己的时间管理能力太差，新的一年一定要提高自己的时间安排能力，还有学习C++和java的计划也还未进行。还有很多想读的书没有读，比如菊与刀，时间简史等，还有就是今年读的书都是小说题材，明年要多读一些其他题材的书籍，尽量扩充自己的知识面。同时明年就要找工作了，还要适当的刷题，学习一些基础知识，比如操作系统，网络，设计模式。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;2016是一个充满了霉气和挑战的一年，跌跌撞撞总算是结束了。这一年似乎也思考了很多问题，比如&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;为什么读研？自己到底喜欢什么？&lt;/li&gt;&#xA;&lt;li&gt;惶惶恐恐在担忧什么？&lt;/li&gt;&#xA;&lt;li&gt;什么才是真正的朋友？怎么才能让友谊长久？&lt;/li&gt;&#xA;&lt;li&gt;为什么总是觉得自己比别人差?这种落差感到底来自哪里？&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;每一个问题都曾困扰自己很久，也曾导致自己一度想要休学，也与好朋友争论不休。不管如何，都要感谢2016，感谢身边的每一个人，感谢你们让我的经历丰富多彩，感谢你们让我变得更加坚强和成熟。2017要做的事情还有很多，想开头的引用，比你优秀的人都那么努力，你还有什么理由不努力？哪怕永远追不上，也比做一个行尸走肉强！&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;This is a new year. A new beginning. And things will change.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;</description>
      <pubDate>Sun, 01 Jan 2017 23:56:55 +0000</pubDate>
    </item>
    <item>
      <title>使用iptables的NAT功能控制内网设备访问外网</title>
      <link>http://openhex.cn/2016/12/8/使用iptables的NAT功能控制内网设备访问外网.html</link>
      <description>&lt;p&gt;通过iptables的NAT功能控制内网上网。&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;前提条件&#34;&gt;前提条件&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;一台能够上网的主机，并且和其他需要控制上网的主机在同一个内网。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;hw98: 172.18.11.98 可以上外网&lt;/p&gt;&#xA;&#xA;&lt;p&gt;hw网段: 172.18.11.0/24&lt;/p&gt;&#xA;&#xA;&lt;p&gt;目的：通过hw98装发内网内的其他机器数据包，从而实现上网控制的目的&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;步骤&#34;&gt;步骤&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;hw98设置路由转发的功能&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;开启内核的ip转发功能&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forward&#xA;或者&#xA;$ vim /etc/sysctl.conf&#xA;# Controls IP packet forwarding&#xA;net.ipv4.ip_forward = 1&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;添加确认包和关联包的通过&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;!--more --&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;设置iptables的nat表，添加FORWARD规则&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ iptables -t nat -A POSTROUTING -s 172.18.11.0/24 -j SNAT --to 172.18.11.98&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;内网内需要进行上网的机器&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;删除默认的路由，如果没有就不用删除&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$route del default gw *.*.*.*  #通过route 查看&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;添加hw98为默认路由&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$route add default gw 172.18.11.98&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;上面内网的设置在重启网络的时候会被修改，所以最好的方法就是在网络配置里就行设置，在centos就是&lt;code&gt;/etc/sysconfig/network-scripts/ifcfg-eth0&lt;/code&gt;，只用设置GATEWAY即可，如果有必要也可以设置一下DNS&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;GATEWAY=172.18.11.98&#xA;DNS1=159.226.39.1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;说明：&lt;/strong&gt;没有深入理解整个运行过程，其实可以通过hw98的iptables设置进行更加细致的访问控制，比如控制内网内特定的主机访问外网，这里是全部允许，也可以控制特定的端口等等。&lt;/p&gt;&#xA;</description>
      <pubDate>Thu, 08 Dec 2016 19:59:19 +0000</pubDate>
    </item>
    <item>
      <title>auditd的使用</title>
      <link>http://openhex.cn/2016/11/12/auditd的使用.html</link>
      <description>&lt;p&gt;linux系统中用户空间的组件，负责系统安全审计，将审计的记录写入磁盘&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;安装&#34;&gt;安装&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;ubuntu可以直接使用apt-get安装，centos使用yum安装&lt;/p&gt;&#xA;&#xA;&lt;p&gt;安装后有下面相关的工具：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;auditctl :&lt;/strong&gt; 即时控制审计守护进程的行为的工具，比如如添加规则等等。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;/etc/audit/audit.rules :&lt;/strong&gt; 记录审计规则的文件。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;aureport :&lt;/strong&gt; 查看和生成审计报告的工具。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;ausearch :&lt;/strong&gt; 查找审计事件的工具&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;auditspd :&lt;/strong&gt; 转发事件通知给其他应用程序，而不是写入到审计日志文件中。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;autrace :&lt;/strong&gt; 一个用于跟踪进程的命令。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;/etc/audit/auditd.conf :&lt;/strong&gt; auditd工具的配置文件。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;首次安装auditd后，审计规则是空的，使用下面的命令查看&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$auditctl -l&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!-- more --&gt;&#xA;&#xA;&lt;h4 id=&#34;添加auditd规则&#34;&gt;添加auditd规则&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;监控文件和目录的更改&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$sudo auditctl -w /etc/passwd -p rwxa&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;选项：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;-w path: 指定监控的路径，上面指定监控的文件路径/etc/passwd&lt;/li&gt;&#xA;&lt;li&gt;-p: 指定出发审计的文件/目录的访问权限&lt;/li&gt;&#xA;&lt;li&gt;rwxa: 指定的触发条件，r读，w写，x执行权限，a属性&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;如果不加-p参数，-w参数后面指定的是目录，则将会对指定目录所有访问进行监控。默认的权限是rwax，可以使用&lt;code&gt;auditctl -l&lt;/code&gt;查看已经添加的规则&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/blog/image/auditd-0.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;查看审计日志&#34;&gt;查看审计日志&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;使用ausearch可以查看auditd的审计日志。比如上面已经对/etc/passwd文件添加了审计，那么使用下面的命令可以查看审计日志。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$sudo ausearch - f/etc/passwd&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;-f&lt;/code&gt;需要查找的审计目标的日志&lt;/p&gt;&#xA;&#xA;&lt;p&gt;输出可能如下&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;p&gt;time-&amp;gt;Wed Sep 21 15:07:51 2016&#xA;type=PATH msg=audit(1474441671.907:881): item=1 name=&amp;ldquo;/root/kongdefei/.test.swp&amp;rdquo; inode=44826697 dev=08:03 mode=0100600 ouid=0 ogid=0 rdev=00:00&#xA;type=PATH msg=audit(1474441671.907:881): item=0 name=&amp;ldquo;/root/kongdefei/&amp;rdquo; inode=80856866 dev=08:03 mode=040755 ouid=0 ogid=0 rdev=00:00&#xA;type=CWD msg=audit(1474441671.907:881):  cwd=&amp;ldquo;/root/kongdefei&amp;rdquo;&#xA;type=SYSCALL msg=audit(1474441671.907:881): arch=c000003e syscall=87 success=yes exit=0 a0=699f130 a1=1 a2=1 a3=2 items=2 ppid=18763 pid=18798 auid=4294967295 uid=0 gid=0 euid=0 s&#xA;uid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts1 ses=4294967295 comm=&amp;ldquo;vi&amp;rdquo; exe=&amp;ldquo;/bin/vi&amp;rdquo; key=(null)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;time: 审计时间&lt;/li&gt;&#xA;&lt;li&gt;name: 审计对象&lt;/li&gt;&#xA;&lt;li&gt;cwd: 当前路径&lt;/li&gt;&#xA;&lt;li&gt;sys call：系统调用&lt;/li&gt;&#xA;&lt;li&gt;auid: 设计用户id&lt;/li&gt;&#xA;&lt;li&gt;uid和gid：访问文件的用户id和用户组id,uid=0 gid=0表明是root用户&lt;/li&gt;&#xA;&lt;li&gt;comm: 用户访问文件的命令&lt;/li&gt;&#xA;&lt;li&gt;exe: 上面命令的执行文件路径&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;上面内容表示/root/kongdefei/.test.swp文件被root用户使用vi命令编辑过&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;查看审计报告&#34;&gt;查看审计报告&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;一旦定义规则后，他会自动运行，过一段时间后，可以使用auditd的工具aureport生成简要的日志报告。直接使用下面命令即可&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$sudo aureport&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/blog/image/auditd-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;可以看出有51次授权失败，102次登录。可以使用下面命令查看授权失败的详细信息&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$sudo aureport -au&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/blog/image/auditd-4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;凡是no（如2，5）的都是授权失败。可以看出是root用户ssh登录的时候失败。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;-m&lt;/code&gt;可以查看所有账户修改相关的事件&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$sudo aureport -m&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/blog/image/auditd-2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;auditd配置文件&#34;&gt;auditd配置文件&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;上面使用-w添加审计规则只是暂时的，系统重启后就会消失，可以修改/etc/audit/audit.rules文件是规则持久有效。上面添加的规则可以直接写入/etc/audit/audit.rules文件中&lt;/p&gt;&#xA;&#xA;&lt;p&gt;如下：&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/blog/image/auditd-3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;然后重启auditd即可&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ service auditd restart #或者/etc/init.d/auditd restart&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;总结&#34;&gt;总结&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Auditd是Linux上的一个审计工具。你可以阅读auidtd文档获取更多使用auditd和工具的细节。例如，输入 &lt;strong&gt;man auditd&lt;/strong&gt; 去看auditd的详细说明，或者键入 &lt;strong&gt;man ausearch&lt;/strong&gt; 去看有关 ausearch 工具的详细说明。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;请谨慎创建规则&lt;/strong&gt;。太多规则会使得日志文件急剧增大！&lt;/p&gt;&#xA;</description>
      <pubDate>Sat, 12 Nov 2016 19:37:32 +0000</pubDate>
    </item>
    <item>
      <title>使用Python和BitTorrentSync定期给Kindle推送电子书</title>
      <link>http://openhex.cn/2016/11/12/使用Python和BitTorrentSync定期给Kindle推送电子书.html</link>
      <description>&lt;h4 id=&#34;kindle伴侣&#34;&gt;Kindle伴侣&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;最近发现一个很好用的Kindle电子书分享网站&lt;a href=&#34;http://kindlefere.com/&#34;&gt;Kindle伴侣&lt;/a&gt;，资源丰富，除了提供各种热门图书免费下载，还提供了很多关于Kindle的使用技巧，里面个人最喜欢的功能就是每周一书，使用BitTorrent每周自动同步热门电子书。使用的是著名的同步软件Resilio Sync， 最开始是在自己电脑上安装了软件同步，但是要一直开着，感觉好麻烦，有时候电脑还要关了，影响同步。刚好手里有个闲置的树莓派，于是就想着用它来同步，然后通过邮件将电子书发送到kindle，这样就可以每周都看到热门的电子书了。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/blog/image/kindle-0.png&#34; alt=&#34;kindle-0&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;resilio-sync服务搭建&#34;&gt;Resilio Sync服务搭建&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Linux下的安装方法可以参考官网：&lt;a href=&#34;https://help.getsync.com/hc/en-us/articles/206178924&#34;&gt;https://help.getsync.com/hc/en-us/articles/206178924&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;下面的方法适合Raspbian，你可以按照官网的说明安装在任何一台可以上网的服务器上。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step1:&lt;/strong&gt; 下载deb安装包&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ wget https://download-cdn.getsync.com/2.0.128/PiWD/bittorrent-sync-pi-server_2.0.128_armhf.deb&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step2:&lt;/strong&gt; 安装Sync包&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ sudo dpkg -i bittorrent-sync-pi-server_2.0.128_armhf.deb&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Step3:&lt;/strong&gt;访问Sync Web UI&lt;/p&gt;&#xA;&#xA;&lt;p&gt;通过浏览器访问&lt;a href=&#34;http://yourrespbianip:8888&#34;&gt;http://yourrespbianip:8888&lt;/a&gt; ,其中是你respian的ip地址或者你托管的服务器地址，你应该看到下面的界面&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/blog/image/kindle-1.png&#34; alt=&#34;kindle-1&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;然后到&lt;a href=&#34;http://kindlefere.com/&#34;&gt;Kindle伴侣&lt;/a&gt;随便点一个[每周一书]可以找到使用bittorrentSync同步的同步key，通过web页面添加到你自己的sync服务器即可。记得你选择的文件夹要有写入权限，我是直接给的777。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;监控进程&#34;&gt;监控进程&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;这里使用Python作为监控程序监控上面Sync服务器的同步文件夹，一旦出现新的指定格式（如MOBI)的文件，则将会向指定的kindle邮箱发送新增加的电子书，邮箱的设置需要到amazon自己设置，想必用过kindle的应该不陌生。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;代码可以从github下载：&lt;a href=&#34;https://github.com/KDF5000/WeeklyBook&#34;&gt;https://github.com/KDF5000/WeeklyBook&lt;/a&gt; , 然后修改&lt;code&gt;server.py&lt;/code&gt;里面的kindle邮箱，以及授权的邮箱&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;EXT_LIST = [&amp;lsquo;mobi&amp;rsquo;] #想要发送的邮件格式&#xA;HOST_NAME = &amp;lsquo;smtp.cstnet.cn&amp;rsquo; #授权邮箱的smtp服务器&#xA;HOST_PORT = 25 #smtp服务器端口&#xA;USER_NAME = &amp;lsquo;kongdefei@ict.ac.cn&amp;rsquo; #授权邮箱的用户名&#xA;USER_PASS = &amp;lsquo;*********************&amp;rsquo;&#xA;KINDLE_MAILS = [&amp;lsquo;kdf5000@kindle.cn&amp;rsquo;] #接收电子书的kindle邮箱，可以在亚马逊查看&#xA;FROM_NAME = &amp;lsquo;kongdefei@ict.ac.cn&amp;rsquo; #发送邮件的from名字，建议使用发送的邮箱地址&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h5 id=&#34;后台启动&#34;&gt;后台启动&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;进入下载的文件夹，执行下面的命令&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$nohup python server.py /home/ubuntu/kongdefei/SyncBook &amp;gt; sync.out 2&amp;gt;&amp;amp;1 &amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;其中&lt;code&gt;/home/ubuntu/kongdefei/SyncBook&lt;/code&gt;是sync同步的文件夹，sync.out是server.py的输出日志，该程序将会在后台持续运行监控&lt;code&gt;/home/ubuntu/kongdefei/SyncBook&lt;/code&gt;下面是否有新的电子书的出现。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;至此，你就可以享受你心爱的kindle每周定时收到一本优秀的电子书了。&#xA;顺便晒一下树莓派[憨笑]&#xA;&lt;img src=&#34;/media/archive/blog/image/raspberry.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sat, 12 Nov 2016 18:45:59 +0000</pubDate>
    </item>
    <item>
      <title>本科毕业设计</title>
      <link>http://openhex.cn/2016/9/16/bachelor-final-project.html</link>
      <description>&lt;p&gt;已经毕业一年多了，突然想到本科毕业设计，辛辛苦苦做了几个月的APP就那样被我们束之高阁了，白白浪费当时的汗水，所以最近想把当时的代码重新部署一下放到服务器上让项目重新跑起来，把服务端代码下了下来，由于当时不是自己写的服务端，部署起来有点难，搞了一天，无果，无奈只好放弃，等有时间再重新看看代码部署一下吧。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;项目简介&#34;&gt;项目简介&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;项目名称: Moment(灵动)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;当今时代可以说90后逐渐要成为社会的主体，而90后一个显著的特点就是，喜欢分享，喜欢线上交流，喜欢异想天开，如果能够打造一个这样的平台，想必是所有有想法的人的一个福音。所以本项目的目的给有想法有创造欲望的年轻人，打造一个便于用户随手记录灵感碎片，并且可以随时随地分享灵感，利用互联网的开放精神，凝聚万千用户的点滴灵感完善自己的想法的移动应用。而且在这里你可能还会找到志同道合的好友，形成一个专属于你们的圈子，大家相互学习，共同进步。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;上面巴拉巴拉都是废话，其实灵动的诞生源于自己的一个小需求，每天接受大量的信息、知识，脑子里也有各种异想天开的idea，但是每次都未成形就忘的一干二净，因此很希望有个APP能够整理自己的这些小思绪，这么一说下个印象笔记不就行了？？但是但是，如果有个app能够自动关联自己历史的一些idea，能够将一些可能相关的idea给整理出来，也许能够成为一个大新闻呢！如果能够将这些想法共享出去，召集一下志同道合的小伙伴一同讨论，也许擦出来火花呢？这样一想感觉酷酷的，然后拍板子，自己开发一个！可是当时哪懂什么机器学习，深度学习这么高大上的东西，于是乎，灵动最最最核心的智能化功能就拜拜了，最后成了一个笔记+论坛的应用！！！当然现在也不知道怎么实现那个核心功能，如果有大神知道怎么实现了，带我飞！还有个类似的产品原型还在脑子里，也许可以一起作为个小目标。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;下面是APP的演示视频：&lt;/p&gt;&#xA;&#xA;&lt;!-- &lt;div align=center&gt;&#xA; &lt;iframe &#xA;    width=768 height=432  &#xA;    src=&#34;blob:https://www.bilibili.com/24cd5d91-898e-44f3-bb2a-f7cc267ca95c&#34;   &#xA;    frameborder=0 allowfullscreen play=&#34;false&#34;&gt;  &#xA; &lt;/iframe&gt;&#xA;&lt;/div&gt;  --&gt;&#xA;&#xA;&lt;iframe&#xA; src=&#34;//player.bilibili.com/player.html?aid=89671060&amp;cid=153154347&amp;page=1&#34; &#xA; scrolling=&#34;no&#34; border=&#34;0&#34; frameborder=&#34;no&#34; framespacing=&#34;0&#34; allowfullscreen=&#34;true&#34;&#xA; width=&#34;100%&#34;&#xA; height=436 &gt;&#xA;&#xA;&lt;/iframe&gt;&#xA;&#xA;&lt;!-- more --&gt;&#xA;&#xA;&lt;p&gt;答辩PPT截图：&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片01.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片02.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片03.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片04.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片05.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片06.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片07.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片08.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片09.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片10.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片11.jpg&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/images/moment/幻灯片12.jpg&#34; alt=&#34;&#34; /&gt;&#xA;最后附上当时答辩的PPT：&lt;a src=&#34;/media/archive/video/Moment.pptx&#34;&gt;答辩ppt&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 16 Sep 2016 19:53:23 +0000</pubDate>
    </item>
    <item>
      <title>大数据安全漫谈</title>
      <link>http://openhex.cn/2016/9/11/大数据安全漫谈.html</link>
      <description>&lt;p&gt;&lt;img src=&#34;/media/archive/security/images/Scurity.svg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sun, 11 Sep 2016 20:45:44 +0000</pubDate>
    </item>
    <item>
      <title>snort规则入门</title>
      <link>http://openhex.cn/2016/9/7/snort规则入门.html</link>
      <description>&lt;h3 id=&#34;snort规则&#34;&gt;snort规则&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;一种简单的，轻量级的规则描述语言，包括连个逻辑部分：格则头和规则选项。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;规则头&lt;/strong&gt;：规则的动作，协议，源和目标ip与网络掩码，源和目标端口信息&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;规则选项&lt;/strong&gt;: 报警信息内容和要检查的包的具体部分&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;alert tcp any any -&amp;gt; 192.168.1.0/24 111 (content:&amp;quot;|00 01 86 a5|&amp;quot;; msg: &amp;quot;mountd access&amp;quot;;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;第一个括号前的部分是规则头（rule header），包含的括号内的部分是规则选项（rule options）。规则选项部分中冒号前的单词称为选项关键字（option keywords）。注意，不是所有规则都必须包含规则选项部分，选项部分只是为了使对要收集或报警，或丢弃的包的定义更加严格。组成一个规则的所有元素 对于指定的要采取的行动都必须是真的。当多个元素放在一起时，可以认为它们组成了一个逻辑与（AND）语句。同时，snort规则库文件中的不同规则可以 认为组成了一个大的逻辑或（OR）语句&lt;/p&gt;&#xA;&#xA;&lt;!-- more --&gt;&#xA;&#xA;&lt;h4 id=&#34;规则头&#34;&gt;规则头&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;规则的头包含了定义一个包的who，where和what信息，以及当满足规则定义的所有属性的包出现时要采取的行动&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;规则动作&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;snort中有五种动作： alert, log, pass, activate. dynamic&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1、Alert-使用选择的报警方法生成一个警报，然后记录（log）这个包。&#xA;  2、Log-记录这个包。&#xA;  3、Pass-丢弃（忽略）这个包。&#xA;  4、activate-报警并且激活另一条dynamic规则。&#xA;  5、dynamic-保持空闲直到被一条activate规则激活，被激活后就作为一条log规则执行。&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;协议&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;四种： tcp, ump, icmp, ip&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;ip地址&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;表示处理一个给定规则的ip地址和端口信息。 any用来定义任何地址。snort没有提供ip地址查询域名的机制。地址直接有数字型ip地址和一个cidr块组成。Cidr块只是作用在规则地址和要检查的进入任何包的网络掩码。/24表示c类网络， /16表示b类网络，/32表示一个特定的机器的地址。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;例如，192.168.1.0/24代表从192.168.1.1到192.168.1.255的 地址块。在这个地址范围的任何地址都匹配使用这个192.168.1.0/24标志的规则。这种记法给我们提供了一个很好的方法来表示一个很大的地址空间。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;有一个操作符可以应用在ip地址上，它是否定运算符（negation operator）。这个操作符告诉snort匹配除了列出的ip地址以外的所有ip地址。否定操作符用&amp;rdquo;！&amp;rdquo;表示。下面这条规则对任何来自本地网络以外的流都进行报警。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;  alert tcp !192.168.1.0/24 any -&amp;gt; 192.168.1.0/24 111 (content: &amp;quot;|00 01 86 a5|&amp;quot;; msg: &amp;quot;external mountd access&amp;quot;;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;这个规则的ip地址代表&amp;rdquo;任何源ip地址不是来自内部网络而目标地址是内部网络的tcp包&amp;rdquo;。&#xA;  你也可以指定ip地址列表，一个ip地址列表由逗号分割的ip地址和CIDR块组成，并且要放在方括号内“[”，“]”。此时，ip列表可以不包含空格在ip地址之间。下面是一个包含ip地址列表的规则的例子。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;  alert tcp ![192.168.1.0/24,10.1.1.0/24] any -&amp;gt; [192.168.1.0/24,10.1.1.0/24] 111 (content: &amp;quot;|00 01 86 a5|&amp;quot;; msg: &amp;quot;external mountd access&amp;quot;;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;端口号&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;端口号可以用几种方法表示，包括&amp;rdquo;any&amp;rdquo;端口、静态端口定义、范围、以及通过否定操作符。&amp;rdquo;any&amp;rdquo;端口是一个通配符，表示任何端口。静态端口定 义表示一个单个端口号，例如111表示portmapper，23表示telnet，80表示http等等。端口范围用范围操作符&amp;rdquo;：&amp;rdquo;表示。范围操作符 可以有数种使用方法，如下所示：&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;log udp any any -&amp;gt; 192.168.1.0/24 1:1024&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;记录来自任何端口的，目标端口范围在1到1024的udp流&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;log tcp any any -&amp;gt; 192.168.1.0/24 :6000&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;记录来自任何端口，目标端口小于等于6000的tcp流&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;log tcp any :1024 -&amp;gt; 192.168.1.0/24 500:&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;记录来自任何小于等于1024的特权端口，目标端口大于等于500的tcp流&lt;/p&gt;&#xA;&#xA;&lt;p&gt;端口否定操作符用&amp;rdquo;！&amp;rdquo;表示。它可以用于任何规则类型（除了any，这表示没有，呵呵）。例如，由于某个古怪的原因你需要记录除x windows端口以外的所有一切，你可以使用类似下面的规则：&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;log tcp any any -&amp;gt; 192.168.1.0/24 !6000:6010&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h4 id=&#34;方向操作符&#34;&gt;方向操作符&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;方向操作符&amp;rdquo;-&amp;gt;&amp;ldquo;表示规则所施加的流的方向。方向操作符左边的ip地址和端口号被认为是流来自的源主机，方向操作符右边的ip地址和端口信 息是目标主机，还有一个双向操作符&amp;rdquo;&amp;lt;&amp;gt;&amp;ldquo;。它告诉snort把地址/端口号对既作为源，又作为目标来考虑。这对于记录/分析双向对话很方 便，例如telnet或者pop3会话。用来记录一个telnet会话的两侧的流的范例如下：&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;log !192.168.1.0/24 any &amp;lt;&amp;gt; 192.168.1.0/24 23&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h4 id=&#34;activate和dynamic规则&#34;&gt;Activate和dynamic规则&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;注：Activate 和 dynamic 规则将被tagging 所代替。在snort的将来版本，Activate 和 dynamic 规则将完全被功能增强的tagging所代替。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Activate 和 dynamic 规则对给了snort更强大的能力。你现在可以用一条规则来激活另一条规则，当这条规则适用于一些数据包时。在一些情况下这是非常有用的，例如你想设置一 条规则：当一条规则结束后来完成记录。Activate规则除了包含一个选择域：activates外就和一条alert规则一样。Dynamic规则除 了包含一个不同的选择域：activated_by 外就和log规则一样，dynamic规则还包含一个count域。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Actevate规则除了类似一条alert规则外，当一个特定的网络事件发生时还能告诉snort加载一条规则。Dynamic规则和log规则类似，但它是当一个activate规则发生后被动态加载的。把他们放在一起如下图所示：&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;activate tcp !$HOME_NET any -&amp;gt; $HOME_NET 143 (flags: PA; content: &amp;ldquo;|E8C0FFFFFF|/bin&amp;rdquo;; activates: 1; msg: &amp;ldquo;IMAP buffer overflow!&amp;rdquo;;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;dynamic tcp !$HOME_NET any -&amp;gt; $HOME_NET 143 (activated_by: 1; count: 50;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;h4 id=&#34;规则选项&#34;&gt;规则选项&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;规则选项组成了snort入侵检测引擎的核心，既易用又强大还灵活。所有的snort规则选项用分号&amp;rdquo;；&amp;rdquo;隔开。规则选项关键字和它们的参数用冒号&amp;rdquo;：&amp;rdquo;分开。按照这种写法，snort中有42个规则选项关键字。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;msg - 在报警和包日志中打印一个消息。&#xA;logto - 把包记录到用户指定的文件中而不是记录到标准输出。&#xA;ttl - 检查ip头的ttl的值。&#xA;tos 检查IP头中TOS字段的值。&#xA;id - 检查ip头的分片id值。&#xA;ipoption 查看IP选项字段的特定编码。&#xA;fragbits 检查IP头的分段位。&#xA;dsize - 检查包的净荷尺寸的值 。&#xA;flags -检查tcp flags的值。&#xA;seq - 检查tcp顺序号的值。&#xA;ack - 检查tcp应答（acknowledgement）的值。&#xA;window 测试TCP窗口域的特殊值。&#xA;itype - 检查icmp type的值。&#xA;icode - 检查icmp code的值。&#xA;icmp_id - 检查ICMP ECHO ID的值。&#xA;icmp_seq - 检查ICMP ECHO 顺序号的值。&#xA;content - 在包的净荷中搜索指定的样式。&#xA;content-list 在数据包载荷中搜索一个模式集合。&#xA;offset - content选项的修饰符，设定开始搜索的位置 。&#xA;depth - content选项的修饰符，设定搜索的最大深度。&#xA;nocase - 指定对content字符串大小写不敏感。&#xA;session - 记录指定会话的应用层信息的内容。&#xA;rpc - 监视特定应用/进程调用的RPC服务。&#xA;resp - 主动反应（切断连接等）。&#xA;react - 响应动作（阻塞web站点）。&#xA;reference - 外部攻击参考ids。&#xA;sid - snort规则id。&#xA;rev - 规则版本号。&#xA;classtype - 规则类别标识。&#xA;priority - 规则优先级标识号。&#xA;uricontent - 在数据包的URI部分搜索一个内容。&#xA;tag - 规则的高级记录行为。&#xA;ip_proto - IP头的协议字段值。&#xA;sameip - 判定源IP和目的IP是否相等。&#xA;stateless - 忽略刘状态的有效性。&#xA;regex - 通配符模式匹配。&#xA;distance - 强迫关系模式匹配所跳过的距离。&#xA;within - 强迫关系模式匹配所在的范围。&#xA;byte_test - 数字模式匹配。&#xA;byte_jump - 数字模式测试和偏移量调整。&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;h4 id=&#34;参考&#34;&gt;参考&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Snort 中文手册: &lt;a href=&#34;http://man.chinaunix.net/network/snort/Snortman.htm&#34;&gt;http://man.chinaunix.net/network/snort/Snortman.htm &lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Wed, 07 Sep 2016 16:50:17 +0000</pubDate>
    </item>
    <item>
      <title>Jstrom安装问题解决方案汇总</title>
      <link>http://openhex.cn/2016/6/20/Jstrom安装问题解决方案汇总.html</link>
      <description>&lt;h4 id=&#34;jstrom-2-1-1安装错误&#34;&gt;jstrom 2.1.1安装错误&lt;/h4&gt;&#xA;&#xA;&lt;h5 id=&#34;the-hostname-which-supervisor-get-is-localhost&#34;&gt;&lt;strong&gt;the hostname which  supervisor get is localhost&lt;/strong&gt;&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;查看[源码(&lt;a href=&#34;https://github.com/alibaba/jstorm/blob/master/jstorm-core/src/main/java/com/alibaba/jstorm/schedule/FollowerRunnable.java)81行&#34;&gt;https://github.com/alibaba/jstorm/blob/master/jstorm-core/src/main/java/com/alibaba/jstorm/schedule/FollowerRunnable.java)81行&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;this.data = data;&#xA;        this.sleepTime = sleepTime;&#xA;        boolean isLocaliP;&#xA;        if (!ConfigExtension.isNimbusUseIp(data.getConf())) {&#xA;            this.hostPort = NetWorkUtils.hostname() + &amp;quot;:&amp;quot; + String.valueOf(Utils.getInt(data.getConf().get(Config.NIMBUS_THRIFT_PORT)));&#xA;            isLocaliP = NetWorkUtils.hostname().equals(&amp;quot;localhost&amp;quot;);&#xA;        } else {&#xA;            this.hostPort = NetWorkUtils.ip() + &amp;quot;:&amp;quot; + String.valueOf(Utils.getInt(data.getConf().get(Config.NIMBUS_THRIFT_PORT)));&#xA;            isLocaliP = NetWorkUtils.ip().equals(&amp;quot;127.0.0.1&amp;quot;);&#xA;        }&#xA;        try {&#xA;            if (isLocaliP) {&#xA;                throw new Exception(&amp;quot;the hostname which Nimbus get is localhost&amp;quot;);&#xA;            }&#xA;        } catch (Exception e1) {&#xA;            LOG.error(&amp;quot;get nimbus host error!&amp;quot;, e1);&#xA;            throw new RuntimeException(e1);&#xA;        }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;可以看出Nimbus的ip设置不能为localhost和127.0.0.1，而启动nimbus server的脚本start.sh如下，使用的是&lt;code&gt;hostname -i&lt;/code&gt;然后匹配storm.yaml文件找到用户设置的地址，而&lt;code&gt;hostname -i&lt;/code&gt;输出的时候主机名对应的ip地址，因为不能用127.0.0.1和localhost,因此只能将主机名映射到本机ip地址，所以就要在/etc/hosts文件添加映射关系，例如本机的主机名为kdf5000，ip地址为10.30.5.64，　则需要添加下面信息&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;10.30.5.64　kdf5000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;重新启动即可。&lt;/p&gt;&#xA;</description>
      <pubDate>Mon, 20 Jun 2016 16:35:09 +0000</pubDate>
    </item>
    <item>
      <title>面向HBase的内存key-value缓存的实现</title>
      <link>http://openhex.cn/2016/6/19/面向HBase的内存key-value缓存的实现.html</link>
      <description>&lt;h3 id=&#34;0x01-背景&#34;&gt;0x01 背景&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;之所以要实现这个缓存主要原因如下(但是由于不是实际业务场景需求，所以可能不太准确，也可能不存在这个需求):&#xA;* 非结构化数据的爆炸式增长&#xA;* 处理速度的要求越来越高&#xA;* HBase是面向硬盘的&#xA;* 内存容量越来越大&#xA;* 热点数据可以在内存放下&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;0x02-设计方案&#34;&gt;0x02 设计方案&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;通常的要实现缓存，主要是在有两个大方向实现，一个是在客户端实现，另一个时在服务端实现&#xA;* 客户端实现&#xA; - 修改Hbase Client的源码,在Put, Get等关键操作的地方加入缓存机制&#xA; - 在client端设计一种缓存服务层,并实现一个分布式的Key-value缓存系统, 对Hbase Client进行重新封装&#xA;* 服务端实现&#xA; - 修改HBase Server端的源码,在Put, Get等关键操作的地方加入缓存机制&#xA; - 在服务端添加一层代理服务,解析所有client的请求,对Put, Get等关键&lt;/p&gt;&#xA;&#xA;&lt;p&gt;两种大方向的第一种方案，都是直接修改hbase的源代码，直接修改源码性能可能会更好一些，但是修改源码，会过于依赖Hbase的版本，对于每一个base的版本更新都可能要重新查看源码，重新修改，除此之外1)中的方案一，由于是在本地客户端进行的缓存，所以没有实现分布式的缓存，因此可能存在缓存命中率低，缓存数据不一致的情况。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;客户端实现中的方案二，进行重新地封装，不对源码进行修改，同时使用分布式的缓存，既可以提高缓存的命中率，同时又解决的对hbase过度依赖的问题，但是可能会降低性能。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;服务端实现中的方案二，通过设计一个缓存代理服务，同样可以解决对hbase的过度依赖，降低了整个系统的耦合性，但是实现一个代理服务并非那么简单，而且需要对hbase的通信机制以及相关协议比较了解。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;通过上面的分析比较，考虑到可行性和技术背景，客户端实现的方案二是最合适的方案，而且其性能和可维护性，扩展性相对来说都是比较好的。分布式缓存系统我们使用Redis实现，并且在client本地实现一个LocalCache，尽可能的提高缓存的命中率，减少通信造成的时间延迟（局部性假设，同一个客户端最有可能访问它put的数据）。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;0x03-实现&#34;&gt;0x03 实现&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;系统的整体架构图如下&#xA;&lt;img src=&#34;/media/archive/blog/image/hydracache/0.png&#34; alt=&#34;Alt text&#34; /&gt;&#xA;按照0x02的方案，要实现的是图中的client，对原生的hbase client进行封装，包括redis主要实现了如下功能&#xA;&lt;img src=&#34;/media/archive/blog/image/hydracache/Functions.png&#34; alt=&#34;Alt text&#34; /&gt;&#xA;主要包括redis集群的实现，以及redis client的封装和localcache的实现。&#xA;* Redis集群部署：可以快速的搭建一个redis集群，用于HydraCache缓存系统的分布式缓存。&#xA;* Redis缓存：使用Redis缓存关键数据，提高系统的读取速度。&#xA;* LocalCache: 一个本地的缓存系统，此处有一个局部性假设，认为同一个客户端最有可能访问它put的数据，实现常用的LRU和LFU等缓存策略。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;hbase集群的搭建&#34;&gt;HBase集群的搭建&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;本系统由于处于实验阶段，并且没有真是的分布式环境，所以使用Docker在本机大家一个分布式HBase环境。Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。相比传统的虚拟机，Docker更加的节省资源，在一台普通的机器上启动多个容器，基本上没有压力，因此在单机上使用Docker搭建HBase分布式集群可以比较真实地模拟真实的分布式集群。&#xA;本系统实现的基于Docker的分布式集群，具有下面的特性：&#xA;* 使用serf和dnsmasq 作为集群节点管理和dns解析&#xA;* 可以自定义集群hadoop和hbase的配置，配置完后只需重新build镜像即可&#xA;* ssh远程登录集群节点容器&lt;/p&gt;&#xA;&#xA;&lt;p&gt;具体的安装部署使用方法，&lt;a href=&#34;http://kdf5000.github.io/2016/05/16/%E4%BD%BF%E7%94%A8Docker%E5%8D%95%E6%9C%BA%E6%90%AD%E5%BB%BAHadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83/&#34;&gt;使用Docker单机搭建Hadoop完全分布式环境&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;redis集群的搭建&#34;&gt;Redis集群的搭建&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Redis是一个高性能的key-value数据库，由于其数据是放在内存中的，因此经常被用做缓存系统，提供系统的响应速度，不过与像memcached这样的内存缓存系统不同的是，redis会周期性地把数据写入磁盘。&#xA;Redis 3.0后开始支持集群的部署，整个集群的架构如图&#xA;&lt;img src=&#34;/media/archive/blog/image/hydracache/redis-cluster.jpg&#34; alt=&#34;Alt text&#34; /&gt;&#xA;Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 CRC16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。&#xA;    使用哈希槽的好处就在于可以方便的添加或移除节点。&#xA;* 当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了；&#xA;* 当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了；&lt;/p&gt;&#xA;&#xA;&lt;p&gt;详细的使用说明可以参考我实现的一个快速搭建分布式redis的解决方案&lt;a href=&#34;https://github.com/KDF5000/redis-cluster&#34;&gt;redis集群的搭建&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;client的实现&#34;&gt;Client的实现&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;　　　Client主要对HBase Client进行封装，结合Redis实现缓存机制，并且实现一个LocalCache的功能。通过Redis缓存，可以实现多个客户端共享缓存的数据，缩短响应时间，LocalCache提高了同一个客户端读取近期读取的数据的响应速度，对于有些场景下的应用，可以减少通信时间从而减少了响应时间。&#xA;　　　Redis和HBase封装的类如图&#xA;　　　&lt;img src=&#34;/media/archive/blog/image/hydracache/class.png&#34; alt=&#34;Alt text&#34; /&gt;&#xA;由图可知，Client大致分为三个部分，HydraCacheClientImpl，Cache(LocalCache)和RedisCluster.&#xA;HydraCacheImpl负责是Client对外的核心接口，调用Cache和RedisCluster控制整个缓存策略。一共有四中模式，不使用缓存，只使用Redis, 只使用LocalCache, Redis和LocalCache都使用。&#xA;* 不使用缓存模式下，内部其实就是HBase Client的一个简单调用；&#xA;* 只使用Redis模式，在读取指定的key时，会先查看redis是否已经缓存了该数据，如果存在则直接读出返回客户端，如果不存在则去HBase读取数据并加入到Redis缓存里；&#xA;* 只使用LocalCache模式，用户可以在初始化的时候选择使用LRU(Least Recently Used)和LFU(Least Frequently Used)中的任何一种淘汰策略，当读取指定的key时，会先判断本地缓存中是否存在这个数据，如果存在则直接返回，否则读取HBase并存入缓存中；&#xA;* 使用Redis和LocalCache模式，在读取数据的时候会先判断本地缓存中是否存在对应的数据，如果存在则直接返回，否则读取redis判断是否存在相应数据，如果存在则直接返回，否则再从HBase读取然后分别存入本地缓存和redis里.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;缓存策略流程图如下&#xA;&lt;img src=&#34;/media/archive/blog/image/hydracache/cacheprocess.png&#34; alt=&#34;Alt text&#34; /&gt;&#xA;HydraCacheImpl实现数据的缓存主要是在进行get操作中进行，如果缓存中没有命中，则读取hbase，然后对数据进行缓存。核心代码(省去异常判断)如下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;public String get(String tableName, String rowKey, String family, String columnName, int expireTime) {&#xA;        //先判断缓存里有没有&#xA;        String key = tableName+&amp;quot;_&amp;quot;+rowKey+&amp;quot;_&amp;quot;+family+&amp;quot;_&amp;quot;+columnName;&#xA;        String valString = getDataFromCache(key);&#xA;        if(valString != null){&#xA;            return valString;&#xA;        }&#xA;        Table table = null;&#xA;        Connection connection = null;&#xA;        connection = ConnectionFactory.createConnection(HydraCacheClientImpl.conf);&#xA;        table = connection.getTable(TableName.valueOf(tableName));&#xA;        Get g = new Get(rowKey.getBytes());&#xA;        g.addColumn(family.getBytes(), columnName.getBytes());&#xA;        Result result = table.get(g);&#xA;        byte[] bytes = result.getValue(family.getBytes(), columnName.getBytes());&#xA;        String valueStr = new String(bytes);&#xA;        //set cache&#xA;        if(valueStr != null){&#xA;            this.setData2Cache(key, valueStr, expireTime);&#xA;        }&#xA;        return valueStr;&#xA;}&#xA;private String getDataFromCache(String key){&#xA;           String val = null;&#xA;        if(this.localCacheOn &amp;amp;&amp;amp; this.localCache != null){&#xA;            val = this.localCache.get(key);&#xA;            if(val !=null){&#xA;                return val;&#xA;            }&#xA;        }&#xA;        if(this.cacheOn == true &amp;amp;&amp;amp; this.redisCluster != null){&#xA;            val = this.redisCluster.get(key);&#xA;            if(val !=null){&#xA;　　　　        return val;&#xA;            }&#xA;        }&#xA;        return val;&#xA;}&#xA;private void setData2Cache(String key, String val, int expire){&#xA;        if(this.localCacheOn == true){&#xA;            this.localCache.set(key, val, expire*1000);//将s转换为ms&#xA;        }&#xA;        if(this.cacheOn == true &amp;amp;&amp;amp; this.redisCluster != null){&#xA;　　this.redisCluster.set(key, val, expire);&#xA;        }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;另外一个在缓存系统中重要的问题就是淘汰策略的指定，什么时候进行缓存过期的清除以及缓存达到限制大小是淘汰哪些缓存。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Redis提供了以下几种策略拱用户选择:&#xA;&lt;strong&gt;noenviction&lt;/strong&gt;：不清除数据，只是返回错误，这样会导致浪费掉更多的内存，对大多数写命令（DEL 命令和其他的少数命令例外）&#xA;&lt;strong&gt;allkeys-lru&lt;/strong&gt;：从所有的数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰，以供新数据使用&#xA;&lt;strong&gt;volatile-lru&lt;/strong&gt;：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰，以供新数据使用&#xA;&lt;strong&gt;allkeys-random&lt;/strong&gt;：从所有数据集（server.db[i].dict）中任意选择数据淘汰，以供新数据使用&#xA;&lt;strong&gt;volatile-random&lt;/strong&gt;：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰，以供新数据使用&#xA;&lt;strong&gt;volatile-ttl&lt;/strong&gt;：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰，以供新数据使用&lt;/p&gt;&#xA;&#xA;&lt;p&gt;LocalCache实现了LRU和LFU两种缓存策略，用户可以根据自己的业务场景进行选择。&#xA;缓存失效的时候怎么办？通常主要两种方法，一种是消极的方法，在主键被访问时如果发现它已经失效，那么就删除它；另一种时积极的方法，周期性地从设置了失效时间的主键中选择一部分失效的主键删除。Redis对两种方法都有实现，LocaCache则采用消极的策略，在get请求缓存数据的时候，判断数据是否过期，如果过期则将其删除.&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;0x04-测试&#34;&gt;0x04 测试&lt;/h3&gt;&#xA;&#xA;&lt;h4 id=&#34;测试环境&#34;&gt;测试环境&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;　　笔记本型号：lenovo y470&#xA;　　操作系统：Ubuntu 14.04&#xA;　　内存：DDR3８G&#xA;　　网卡：1000Mbps&#xA;　　Hbase分布式环境(Docker)：三个节点，一个master，２个slave&#xA;　　Redis集群：三个master和每个master一个slave&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;测试结果&#34;&gt;测试结果&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Hbase集群使用docker搭建，运行在本地机器上，启动三个节点，一个master，两个slave。Redis集群同样运行在本地机器上，使用不同的端口代表一个redis实例，一共启动六个，三个作为master，三个分别作为master的slave。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;为了测试hbase原声client和hydraCache的性能，我们开发了一个HBench用于产生数据集和负载，写入hbase的数据集是一个顺序序列，负载读取用的数据集随机生成。主要测试了get操作的性能，测试的模式涵盖了第五节中的四种模式。在数据集规模的选择上，分别进行了四组规模的数据集，分别是300, 600,900和1200条数据。测试的结果如图&#xA;&lt;img src=&#34;/media/archive/blog/image/hydracache/Size-Time_2.png&#34; alt=&#34;Alt text&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;图中每一组第第一列代表使用原生hbase客户端读取相应条数数据的时间，第二列是单独使用redis缓存所需的时间，第三列是使用LocalCache所需时间，第四列是同时使用redis和LocalCache所消耗的时间。从图中可以看出，使用缓存的响应时间明显要小于不适用缓存，使用缓存的情况下，仅仅使用LocalCache的时间是最短的，次之是同时使用redis和localCache，然后是单独使用redis。这与预期的结果还是比较一致的，使用缓存的情况下，由于在第二次读取某些key值的时候会从缓存中读取，这比从hbase中读取要快的多，而从localcache中读取数据因为减少了网络通信通信所以时间要更少一些。而同时使用redis和localcache的时间介于两者之间的原因应该是，读取数据的时候大部分情况下是可以从localcache中读取的，但是在设置缓存的时候要保存数据到redis，因此增加了操作的时间，故总的响应时间要多于单独使用localcache但是要比单独使用redis稍微好那么一点，不过并不是很明显。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;对于缓存模式的选择，要根据实际的业务场景进行分析择取，比如有些应用，同一个客户端存取的数据很少在进行读取，但是可能读取其他客户端添加的数据，这个时候使用redis既可以满足，就能达到很好的效果；而另一些客户端则读取的数据大部分都是前面自己添加的数据，这样的业务场景就很适合使用localcache。&lt;/p&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;相关项目:&lt;/strong&gt;&#xA;HydraCache: [&lt;a href=&#34;https://github.com/KDF5000/HydraHbaseCache](&#34;&gt;https://github.com/KDF5000/HydraHbaseCache](&lt;/a&gt;)&#xA;Docker分布式Hbase: &lt;a href=&#34;https://github.com/KDF5000/hydra-hadoop&#34;&gt;https://github.com/KDF5000/hydra-hadoop&lt;/a&gt;&#xA;分布式Redis集群：&lt;a href=&#34;https://github.com/KDF5000/redis-cluster&#34;&gt;https://github.com/KDF5000/redis-cluster&lt;/a&gt;&#xA;性能测试：&lt;a href=&#34;https://github.com/KDF5000/HBench&#34;&gt;https://github.com/KDF5000/HBench&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sun, 19 Jun 2016 09:54:46 +0000</pubDate>
    </item>
    <item>
      <title>Public Speaking: Work Overtime</title>
      <link>http://openhex.cn/2016/6/2/Public-Speaking-Work-Overtime.html</link>
      <description>&lt;p&gt;　　   Overtime is the amount of time someone works beyond normal working hours. But in IT companies, working overtime seems to be a standard ingredient in cooperate culture. The last few years, programmer has been highly controversial.They earn much money but spend less and die earlier than people in other careers.It does not mean that they don`t like shopping. It is just because they work all day and don`t have time to shop. So why do they always work overtime? Do they always have so much tasks to do ? According to my own internship experience and my friends`s experiences, there are always three reasons: the manager allocates task based on 10 or more working hours rather than 8 hours one day; The employee are busy with meetings, communication with their colleagues and surfing social websites, like weibo, wechat; The last reason is that they really enjoy their job. Today I won`t talk about how to arrange tasks and manage your time, I will give you some reasons to stop working overtime.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;　&lt;!--more--&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;　　   First, Working overtime will increase health problems. There are already several sudden death events because of working excessive overtime. Last April, a thirty six years old IT Worker , who graduated from Tsinghua University, suddenly died on flush toilets. Before the day of his death, he sent a message to his mother, which said “I am so tired!”. And last December, a group leader in Tencent suddenly died when he took a walk with his wife after a long time overtime. Besides these reported events, many studies indicate that some health problems that have been linked to long working hours, such as higher blood pressure, higher suicide rates and lower-back injury.&#xA;　　&lt;br /&gt;&#xA;　　   Second, overtime merely reduces the efficiency instead of increasing the productivity. The productivity is not linear. The thinking behind why managers believe twice the hours results in twice the output is short-sighted and toxic. Don`t forget that we are humans and not robots. Specially, software development is a mental activity. The programmer need time to think so that they can create powerful product.If they always are busy with coding, they can not come up with creative ideas.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;　　   Last but not the least, life is more than working and earning money, and there are friends and relatives. If you always spend so much time on getting ahead at work, you are letting other things slide in your life, such as your dream, your friendship. Every relationship needs time to manage and maintain. In other words, you’ve let your quality of life go down the toilet in exchange for a little bit more money and perhaps some praise from the boss -though you don’t always get that either. So remember that beyond the pots and pans, there should be peotry and far afield.&#xA;　　&#xA;　　   If you are a manager in IT companies, try to find new management method fitting for programmer to increase the productivity, instead of enforcing them to work overtime. If you are an employee, try to find your own way to increase efficiency instead of working overtime. Life is short, enjoy it.&#xA;　　&lt;/p&gt;&#xA;</description>
      <pubDate>Thu, 02 Jun 2016 00:11:10 +0000</pubDate>
    </item>
    <item>
      <title>PHP常见的五种设计模式——策略模式</title>
      <link>http://openhex.cn/2016/5/23/PHP常见的五种设计模式——策略模式.html</link>
      <description>&lt;h4 id=&#34;策略模式&#34;&gt;策略模式&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;在此模式中，算法是从复杂类提取的，因而可以方便地替换。例如，如果要更改搜索引擎中排列页的方法，则策略模式是一个不错的选择。思考一下搜索引擎的几个部分 —— 一部分遍历页面，一部分对每页排列，另一部分基于排列的结果排序。在复杂的示例中，这些部分都在同一个类中。通过使用策略模式，您可将排列部分放入另一个类中，以便更改页排列的方式，而不影响搜索引擎的其余代码.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;类图如下：&#xA;&lt;img src=&#34;/media/archive/blog/image/strategy.png&#34; alt=&#34;Alt text&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;php实现:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;/**&#xA; * strategy &#xA; * @author KDF5000&#xA; * @since 2016-5-24 22:51&#xA; */&#xA;&#xA;interface IStrategy{&#xA;    public function filter($name);&#xA;}&#xA;&#xA;class FindAfterStrategy implements IStrategy{&#xA;&#xA;    private $_name;&#xA;    public function __construct($name){&#xA;        $this-&amp;gt;_name = $name;&#xA;    }&#xA;&#xA;    public function filter($name){&#xA;        return strcmp($this-&amp;gt;_name, $name) &amp;lt;= 0;&#xA;    }&#xA;&#xA;}&#xA;&#xA;class RandomStrategy implements IStrategy{&#xA;&#xA;    public function filter($name){&#xA;        return rand(0,1) &amp;gt;= 0.5;&#xA;    }&#xA;}&#xA;&#xA;class UserList{&#xA;&#xA;    private $_list = array();&#xA;&#xA;    public function __construct($names){&#xA;        if($names != null){&#xA;            foreach ($names as $name) {&#xA;                # code...&#xA;                $this-&amp;gt;_list[] = $name;&#xA;            }&#xA;        }&#xA;    }&#xA;&#xA;    public function add($name){&#xA;        $this-&amp;gt;_list[] = $name;&#xA;    }&#xA;&#xA;    public function find($filter){&#xA;        $res = array();&#xA;        foreach ($this-&amp;gt;_list as $name) {&#xA;            # code...&#xA;            if($filter-&amp;gt;filter($name)){&#xA;                $res[] = $name;&#xA;            }&#xA;        }&#xA;        return $res;&#xA;    }&#xA;}&#xA;&#xA;$user_list = new UserList(array(&#39;Jack&#39;,&#39;Tom&#39;, &#39;Mike&#39;, &#39;Devin&#39;));&#xA;$res_1 = $user_list-&amp;gt;find(new FindAfterStrategy(&amp;quot;M&amp;quot;));&#xA;$res_2 = $user_list-&amp;gt;find(new RandomStrategy());&#xA;&#xA;var_dump($res_1);&#xA;var_dump($res_2);&#xA;&#xA;?&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Mon, 23 May 2016 23:11:04 +0000</pubDate>
    </item>
    <item>
      <title>PHP常见的五种设计模式——命令链模式</title>
      <link>http://openhex.cn/2016/5/23/PHP常见的五种设计模式——命令链模式.html</link>
      <description>&lt;h4 id=&#34;命令链模式&#34;&gt;命令链模式&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;命令链模式同样是为了解决耦合的问题，该模式将命令处理者和发号命令方分开，发号命令方不用知道谁处理该命令，以及以什么样的方式处理，它向所有已经注册的命令处理者发送命令，收到成功信息即可。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;具体实现的时候，需要定义一个统一的接口，然后命令处理者实现接口，然后命令发号者用一个list保存所有注册的命令处理者，每个命令处理者会处理特定的一个或者一些命令，当发令者发送命令时，遍历list向已经注册的命令处理者发送命令，收到一个正确处理的信息即说明改命令已经有人处理。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;类图如下：&#xA;&lt;img src=&#34;/media/archive/blog/image/commandchain.png&#34; alt=&#34;Alt text&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;php实现如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;/**&#xA; * command chain&#xA; * @author KDF5000 &#xA; * @since 2016-5-24&#xA; */&#xA;&#xA;interface ICommand{&#xA;    public function onCommand($name, $args);&#xA;}&#xA;&#xA;class CommandChain{&#xA;&#xA;    private $_commands = array();&#xA;&#xA;    public function addCommand($command){&#xA;        $this-&amp;gt;_commands[] = $command;&#xA;    }&#xA;&#xA;    public function runCommand($name, $args){&#xA;        foreach ($this-&amp;gt;_commands as $cmd) {&#xA;            # code...&#xA;            if($cmd-&amp;gt;onCommand($name, $args)){&#xA;                return;&#xA;            }&#xA;        }&#xA;    }&#xA;}&#xA;&#xA;class UserCommand implements ICommand{&#xA;&#xA;    public function onCommand($name, $args){&#xA;        if($name == &amp;quot;addUser&amp;quot;){&#xA;            echo __CLASS__.&amp;quot; reponses to addUser command!&amp;quot;.PHP_EOL;&#xA;            return true;&#xA;        }&#xA;        return false;&#xA;    }&#xA;}&#xA;&#xA;class MailCommand implements ICommand{&#xA;&#xA;    public function onCommand($name, $args){&#xA;        if($name == &amp;quot;mail&amp;quot;){&#xA;            echo __CLASS__.&amp;quot; reponses to mail command!&amp;quot;.PHP_EOL;&#xA;            return true;&#xA;        }&#xA;        return false;&#xA;    }&#xA;}&#xA;&#xA;&#xA;$commandChain = new CommandChain();&#xA;$commandChain-&amp;gt;addCommand(new UserCommand());&#xA;$commandChain-&amp;gt;addCommand(new MailCommand());&#xA;$commandChain-&amp;gt;runCommand(&amp;quot;mail&amp;quot;, null);&#xA;$commandChain-&amp;gt;runCommand(&amp;quot;addUser&amp;quot;, null);&#xA;?&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Mon, 23 May 2016 21:10:28 +0000</pubDate>
    </item>
    <item>
      <title>PHP常见的五种设计模式——观察者模式</title>
      <link>http://openhex.cn/2016/5/22/PHP常见的五种设计模式——观察者模式.html</link>
      <description>&lt;h4 id=&#34;观察者模式&#34;&gt;观察者模式&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;观察者模式是解决组件(对象)之间紧耦合的一种方式，顾名思义，既然叫观察者模式，涉及到两个对象，一个观察者，一个被观察者。观察者模式想要实现的效果就是当被观察者发生改变的时候，要主动通知观察者，自己发生了改变，至于观察者得知消息后做什么操作，被观察者无从得知，也不需要知道。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;要实现这种模式，被观察的对象，要保存需要通知的观察者，这里称之为注册。然后当被观察者发生某个需要通知观察者的改变的时候，遍历已经注册的观察者，通知他们使用函数调用的方式。具体实现需要定义两个接口，用于观察者和被观察者实现，类图如下：&#xA;&lt;img src=&#34;/media/archive/blog/image/观察者模式.png&#34; alt=&#34;Alt text&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h5 id=&#34;php-实现&#34;&gt;PHP 实现&lt;/h5&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;定义一个IObserver, 用于观察者实现&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;interface IObserver{&#xA;    public function onChange($sender, $args);&#xA;}&#xA;&#xA;interface IObservable{&#xA;    public function addObserver($observer);&#xA;}&#xA;&#xA;class ObservedObj implements IObservable{&#xA;&#xA;    private $_observer = array();&#xA;&#xA;    public function addObserver($observer){&#xA;        $this-&amp;gt;_observer []= $observer;&#xA;    }&#xA;&#xA;    public function changeSomeThing($str){&#xA;        foreach ($this-&amp;gt;_observer as $ob) {&#xA;            # code...&#xA;            $ob-&amp;gt;onChange($this, $str);&#xA;        }&#xA;    }&#xA;}&#xA;&#xA;class Observer implements IObserver{&#xA;&#xA;    private $_name = null;&#xA;&#xA;    function __construct($name){&#xA;        $this-&amp;gt;_name = $name;&#xA;    }&#xA;    function onChange($sender, $args){&#xA;        echo $this-&amp;gt;_name.&amp;quot; received msg:changed &amp;quot;.$args.PHP_EOL;&#xA;    }&#xA;}&#xA;&#xA;$observedOb = new ObservedObj();&#xA;$observedOb-&amp;gt;addObserver(new Observer(&amp;quot;ob1&amp;quot;));&#xA;$observedOb-&amp;gt;addObserver(new Observer(&amp;quot;ob2&amp;quot;));&#xA;$observedOb-&amp;gt;changeSomeThing(&amp;quot;hello&amp;quot;);&#xA;?&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
      <pubDate>Sun, 22 May 2016 18:59:32 +0000</pubDate>
    </item>
    <item>
      <title>使用Docker单机搭建Hadoop完全分布式环境</title>
      <link>http://openhex.cn/2016/5/16/使用Docker单机搭建Hadoop完全分布式环境.html</link>
      <description>&lt;h4 id=&#34;0x01-hydra-hadoop&#34;&gt;0x01 Hydra-hadoop&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;使用docker在单机部署hadoop和hbase的分布式环境，本项目具有一下特征：&#xA;* 使用serf和dnsmasq 作为集群节点管理和dns解析&#xA;* 可以自定义集群hadoop和hbase的配置，配置完后只需重新build镜像即可&#xA;* ssh远程登录集群节点容器&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;项目地址&lt;/strong&gt;：&lt;a href=&#34;https://github.com/KDF5000/hydra-hadoop&#34;&gt;https://github.com/KDF5000/hydra-hadoop&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;0x02-项目目录&#34;&gt;0x02 项目目录&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;serf-dnsmasq&lt;/strong&gt;： serf和dnsmasq服务，用于管理集群节点的退出和添加，dnsmasq用于dns的解析&#xA;**hadoop-fake: ** 实现一个伪分布式的hadoop环境&#xA;&lt;strong&gt;hadoop-hydra:&lt;/strong&gt; 基于&lt;code&gt;hadoop-fake&lt;/code&gt;实现一个完全分布式的集群环境，master和slave均使用该镜像&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;0x03-使用说明&#34;&gt;0x03 使用说明&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;默认本机已经安装了docker环境和git工具&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;0x01-clone&#34;&gt;0x01 clone&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/KDF5000/hydra-hadoop&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;0x02-创建镜像&#34;&gt;0x02 创建镜像&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;按照下面的顺序执行相应操作&#xA;* 进入serf-dnsmasq目录，执行&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;sudo docker build -t=&amp;quot;kdf5000/serf-dnsmasq&amp;quot; .&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;进入hadoop-fake目录，执行&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;sudo docker build -t=&amp;quot;kdf5000/ubuntu-hadoop&amp;quot; .&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;进入hadoop-hydra目录，执行&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;sudo docker build -t=&amp;quot;kdf5000/hadoop-hydra&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;创建集群&#34;&gt;创建集群&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;进入项目根目录，执行&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$./start-container.sh [num] //num可选，默认为3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;默认时启动三个容器，一个作为master，两个slave，如果想要启动其他数目的容器，直接在后面添加数目即可&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h5 id=&#34;启动hadoop&#34;&gt;启动hadoop&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;上一步，启动容器之后，回直接进入master的shell交互界面，可以使用下面的命令验证dns和serf服务是否正确&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$serf members&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果出现下面的结果说明服务已经正确安装和启动&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@master:~# serf members&#xA;master.kdf5000.com  172.17.0.2:7946  alive  &#xA;slave1.kdf5000.com  172.17.0.3:7946  alive  &#xA;slave2.kdf5000.com  172.17.0.4:7946  alive&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;如果没有出现上面的结果，请仔细检查前面步骤是否有问题，否则下面步骤将不能正确执行&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在master主机进入用户(root)目录，然后执行根目录下的脚本&lt;code&gt;start_service.sh&lt;/code&gt;,将启动hadoop集群。&#xA;使用&lt;code&gt;jps&lt;/code&gt;验证是否启动成功，如果在master上出现下面信息说明启动成功&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@master:~# jps&#xA;1598 HRegionServer&#xA;393 NameNode&#xA;909 NodeManager&#xA;1415 HQuorumPeer&#xA;6209 Jps&#xA;666 SecondaryNameNode&#xA;515 DataNode&#xA;812 ResourceManager&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;使用下面命令进入slave主机，进行同样验证，出现下面信息及说明启动成功&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;root@master:~/# ssh slave1.kdf5000.com&#xA;Warning: Permanently added &#39;slave1.kdf5000.com,172.17.0.3&#39; (ECDSA) to the list of known hosts.&#xA;Welcome to Ubuntu 14.04.4 LTS (GNU/Linux 3.13.0-85-generic x86_64)&#xA;&#xA; * Documentation:  https://help.ubuntu.com/&#xA;Last login: Sun May 15 13:29:30 2016 from master.kdf5000.com&#xA;root@slave1:~# &#xA;root@slave1:~# jps&#xA;1853 Jps&#xA;293 NodeManager&#xA;195 DataNode&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;wordcount测试&#34;&gt;wordcount测试&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;进入master主机的用户根目录，执行&lt;code&gt;wordcount.sh&lt;/code&gt;脚本，观察执行过程，如果最后输出下面信息说明完全分布式hadoop环境搭建成功&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;wordcount output:&#xA;16/05/15 14:01:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&#xA;2    ello&#xA;1    ocker&#xA;1    adoop&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr /&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;参考项目&lt;/strong&gt;&#xA;1. &lt;a href=&#34;https://github.com/kiwenlau/hadoop-cluster-docker&#34;&gt;https://github.com/kiwenlau/hadoop-cluster-docker&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/alvinhenrick/docker-serf&#34;&gt;https://github.com/alvinhenrick/docker-serf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/alvinhenrick/hadoop-mutinode&#34;&gt;https://github.com/alvinhenrick/hadoop-mutinode&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</description>
      <pubDate>Mon, 16 May 2016 18:40:21 +0000</pubDate>
    </item>
    <item>
      <title>Public Speaking: Conformity</title>
      <link>http://openhex.cn/2016/5/2/Public-Speaking-Work-Conformity.html</link>
      <description>&lt;h4 id=&#34;conformity&#34;&gt;Conformity&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Last week, when I was waiting for the green man at a pedestrian crossing, a phenomenon attracted me. Because there were not so much cars, somebody crossed the street before the light changed green. Somebody else began to cross the street. We broken the rules. One or two individuals broken the rule, before long, everyone did this. Why? because others broken the rule. This psychology is called Conformity.&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;Conformity is a type of social influence involving a change in belief for behavior in order to fit in with a group. This change is in response to real or imagined group pressure. In 1951, Asch , a pioneer in social psychology , conducted an experiment to  investigate the extent to which social pressure from a majority group could affect a person to conform. In the experiment, there were eight participants, but only one was real participant. They were given two cards: one has one line, and the other has three lines. They are asked to compare the length of the one line with the other three to determine which is the same length as the original line. The other participants give their answers, one by one.They unanimously gave an answer that was clearly wrong. The real participant sat at the end of the row and gave his or her answer last. Over the 12 critical trials about 75% of participants conformed at least once, and 25% of participant never conformed. Why did the participants conform so readily? With the interview after the experiment, there are always two main reasons: because they are afraid of being ridiculed or have a desire to be liked and because they believe that the group is better informed than they are.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is conformity good or bad? In fact, it`s a absurd question.Conformity can make you feel that you are a part of a group and provides a sense of security by following norms. On the other hand, blinding following indicates is always not the smart option.    So how to deal with the conformity? The first thing you should know is that conformity is a common psychology and don`t have any prejudice. With the right understanding, you should try your best to establish your own value system. And learn to think and analyze a problem independently. The other people`s choice can be a reference, but not a final decision.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In a word, conformity is general in our life. We should treat conformity correctly and develop our own judgment and value system. Thanks!&lt;/p&gt;&#xA;</description>
      <pubDate>Mon, 02 May 2016 16:14:38 +0000</pubDate>
    </item>
    <item>
      <title>awk的使用入门</title>
      <link>http://openhex.cn/2016/4/30/awk的使用入门.html</link>
      <description>&lt;h4 id=&#34;简介&#34;&gt;简介&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;awk&lt;/code&gt;是一个强大的文本分析工具，在对数据分析生成报告时，特别枪弹。&lt;code&gt;awk&lt;/code&gt;的原理是把文件以行读入，以指定的分隔符对每行分片，提供一些函数或者自己写程序逻辑，可以对每片进行出来。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;使用方法&#34;&gt;使用方法&lt;/h4&gt;&#xA;&lt;pre&gt;&lt;code&gt;awk [options] &#39;script&#39; var=value file(s) &#xA;awk [options] -f scriptfile var=value file(s)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;常用命令选项&#34;&gt;常用命令选项&lt;/h5&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;-F field_separator&lt;/strong&gt; 制定每行文本的域分隔符，可以是字符串也可以是正则表达式&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;-v var=value&lt;/strong&gt; 赋值一个用户变量，将外部变量值传入awk&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;-f&lt;/strong&gt; 制定awk脚本文件&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h4 id=&#34;awk的模式和操作&#34;&gt;awk的模式和操作&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;awk 脚本，也就是上一节中的script部分，格式遵循下面的形式&#xA;&amp;gt; awk &amp;ldquo;pattern action&amp;rdquo;  files&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;模式-pattern&#34;&gt;模式(Pattern)&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;模式可以是下面的任意一个:&#xA;* /正则表达式/: 使用//包围的正则表达式&#xA;* 关系表达式：使用运算符进行操作，可以是字符串或数字的比较测试&#xA;* 模式匹配表达式： 用运算符&lt;code&gt;~&lt;/code&gt;(匹配)和&lt;code&gt;~!&lt;/code&gt;(不匹配)&#xA;*  BEGIN语句块，pattern匹配块，END语句块&lt;/p&gt;&#xA;&#xA;&lt;!-- more --&gt;&#xA;&#xA;&lt;h5 id=&#34;操作&#34;&gt;操作&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;操作有一个或者多个命令，函数，表达式组成，有换行符或者分毫分开，必须位于大括号内部，主要有下面操作：&#xA;* 变量或数组赋值&#xA;* 输出命令&#xA;* 内置函数&#xA;* 控制流程语句&lt;/p&gt;&#xA;&#xA;&lt;p&gt;awk操作中的变量或者数组可以直接使用，不用进行初始化，其他语法和c风格比较像&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;入门实例&#34;&gt;入门实例&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;假设last -n 5的输出如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;hadoop@kdf5000:~$ last -n 5&#xA;hadoop   pts/9        :1               Sat Apr 30 15:18   still logged in   &#xA;hadoop   pts/22       :1               Sat Apr 30 14:49 - 14:49  (00:00)    &#xA;hadoop   :1           :1               Sat Apr 30 14:48   still logged in   &#xA;hust-lh  :0           :0               Sat Apr 30 14:46   still logged in   &#xA;hadoop   pts/1        :0               Sat Apr 30 14:03 - 14:46  (00:43)    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果只显示最近登陆的5个账号&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;hadoop@hust-lh:~$ last -n 5 | awk &#39;{print $1}&#39;&#xA;hadoop&#xA;hadoop&#xA;hadoop&#xA;hust-lh&#xA;hadoop&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;awk工作流程是这样的：读入有&lt;code&gt;\n&lt;/code&gt;换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，&lt;code&gt;$0&lt;/code&gt; 则表示所有域,&lt;code&gt;$1&lt;/code&gt;表示第一个域,&lt;code&gt;$n&lt;/code&gt;表示第n个域。默认域分隔符是&amp;rdquo;空白键&amp;rdquo; 或 &amp;ldquo;[tab]键&amp;rdquo;,所以&lt;code&gt;$1&lt;/code&gt;表示登录用户，&lt;code&gt;$3&lt;/code&gt;表示登录用户ip,以此类推。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;如果只是显示/etc/passwd的账户&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;hadoop@hust-lh:~$ cat /etc/passwd |awk  -F &#39;:&#39;  &#39;{print $1}&#39;  &#xA;root&#xA;daemon&#xA;bin&#xA;sys&#xA;sync&#xA;games&#xA;man&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;这种是&lt;code&gt;awk+action&lt;/code&gt;的示例，每行都会执行&lt;code&gt;action{print $1}&lt;/code&gt;。&lt;code&gt;-F&lt;/code&gt;指定域分隔符为&lt;code&gt;&#39;:&#39;&lt;/code&gt;。&#xA;如果只是显示&lt;code&gt;/etc/passwd&lt;/code&gt;的账户和账户对应的&lt;code&gt;shell&lt;/code&gt;,而账户与&lt;code&gt;shell&lt;/code&gt;之间以逗号分割,而且在所有行添加列名&lt;code&gt;name&lt;/code&gt;,&lt;code&gt;shell&lt;/code&gt;,在最后一行添加&lt;code&gt;&amp;quot;blue,/bin/nosh&amp;quot;&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;hadoop@hust-lh:~$ cat /etc/passwd |awk  -F &#39;:&#39;  &#39;BEGIN {print &amp;quot;name,shell&amp;quot;}  {print $1&amp;quot;,&amp;quot;$7} END {print &amp;quot;blue,/bin/nosh&amp;quot;}&#39;&#xA;name,shell&#xA;root,/bin/bash&#xA;daemon,/usr/sbin/nologin&#xA;bin,/usr/sbin/nologin&#xA;sys,/usr/sbin/nologin&#xA;sync,/bin/sync&#xA;blue,/bin/nosh&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;awk工作流程是这样的：先执行&lt;code&gt;BEGING&lt;/code&gt;，然后读取文件，读入有&lt;code&gt;\n&lt;/code&gt;换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，&lt;code&gt;$0&lt;/code&gt;则表示所有域,&lt;code&gt;$1&lt;/code&gt;表示第一个域,&lt;code&gt;$n&lt;/code&gt;表示第&lt;code&gt;n&lt;/code&gt;个域,随后开始执行模式所对应的动作&lt;code&gt;action&lt;/code&gt;。接着开始读入第二条记录······直到所有的记录都读完，最后执行&lt;code&gt;END&lt;/code&gt;操作。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;搜索/etc/passwd有root关键字的所有行&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;hadoop@hust-lh:~$ awk -F: &#39;/root/&#39; /etc/passwd&#xA;root:x:0:0:root:/root:/bin/bash&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;这种是&lt;code&gt;pattern&lt;/code&gt;的使用示例，匹配了&lt;code&gt;pattern&lt;/code&gt;(这里是&lt;code&gt;root&lt;/code&gt;)的行才会执行&lt;code&gt;action&lt;/code&gt;(没有指定&lt;code&gt;action&lt;/code&gt;，默认输出每行的内容)。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;搜索支持正则，例如找&lt;code&gt;root&lt;/code&gt;开头的: &lt;code&gt;awk -F: &#39;/^root/&#39; /etc/passwd&lt;/code&gt;&#xA;搜索/etc/passwd有root关键字的所有行，并显示对应的&lt;code&gt;shell&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;hadoop@hust-lh:~$ awk -F: &#39;/root/{print $7}&#39; /etc/passwd          &#xA;/bin/bash&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;这里指定了action{print $7}&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;脚本编程&#34;&gt;脚本编程&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;awk可是将脚本写到文件中，然后使用-f调用脚本文件&#xA;下面是一个统计一个文件中对应的单词出现的次数，文件的每一行有两个字段，一个是单词，一个是次数，每个单词可能出现多次，求每个单词后面每个数组的和&#xA;下面是输入文件示例：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;hello 1&#xA;world 2&#xA;hello 2&#xA;world 1&#xA;haha  4&#xA;demo  3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;脚本如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#!/bin/awk&#xA;BEGIN{&#xA;    print &amp;quot;word&amp;quot;, &amp;quot;count&amp;quot;&#xA;    count=0&#xA;}&#xA;{&#xA;    word[$1] = word[$1] + $2;&#xA;    count = count + $2&#xA;}&#xA;END{&#xA;   for (v in word){&#xA;      print v, word[v]&#xA;   }&#xA;   print &amp;quot;Total Count:&amp;quot;, count&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;执行脚本&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;hadoop@hust-lh:~/BigData/Homework/Hw2/GraphLite/GraphLite-0.20/res$ awk -f count.awk word&#xA;word count&#xA;demo 3&#xA;haha 4&#xA;hello 3&#xA;world 3&#xA;Total Count: 13&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;其实和c语言的风格时一样的，程序控制语句支持if, for, while, do while。&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;awk内置变量&#34;&gt;awk内置变量&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;下面是awk的内置变量，这些变量时可以直接在脚本里使用的&#xA;&amp;gt;&#xA;ARGC ：命令行参数个数&#xA;ARGV  ：命令行参数排列&#xA;ENVIRON   ：支持队列中系统环境变量的使用&#xA;FILENAME   ：awk浏览的文件名&#xA;FNR     ： 浏览文件的记录数&#xA;FS    ：设置输入域分隔符，等价于命令行 -F选项&#xA;NF  ：浏览记录的域的个数&#xA;NR：已读的记录数&#xA;OFS ：输出域分隔符&#xA;ORS ：输出记录分隔符&#xA;RS ：控制记录分隔符&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;结语&#34;&gt;结语&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;awk&lt;/code&gt;是一个非常强大好用的文本处理工具，&lt;code&gt;awk&lt;/code&gt;的更多使用方法参考&lt;a href=&#34;http://www.gnu.org/software/gawk/manual/gawk.html&#34;&gt;http://www.gnu.org/software/gawk/manual/gawk.html&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sat, 30 Apr 2016 16:05:46 +0000</pubDate>
    </item>
    <item>
      <title>《岛上书店》有感</title>
      <link>http://openhex.cn/2016/4/15/岛上书店-有感.html</link>
      <description>&lt;p&gt;&lt;img src=&#34;/media/archive/blog/image/daoshangshudian.jpg&#34; alt=&#34;岛上书店&#34; /&gt;&#xA;并没有看书的习惯，知道这本书是因为年前一个朋友遇到了诸多不顺，前来诉苦，我就随口说了一句“人总会有艰难的时候，一切都会好起来了”，然后朋友就给我回了一句“每个人的生命中，都有最艰难的那一年，将人生变得美好而辽阔”，两句话真的好像，他告诉我这句话来自《岛上书店》，当时心里窃喜，我还挺能说出来这么富有哲理 的话。百度了一下，《岛上书店》竟然是各大商城的畅销书，由于向来对畅销，热销之类的词比较反感，于是不了了之。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;今年是本命年，可能是因为没有买红色饰品的缘故，刚过完春节就诸事不顺，借钱的朋友竟然搞起了传销，电脑歇菜，kindle碎屏，跟喜欢的妹子莫名的吵架，于是想起了《岛上书店》，心想也许喝点鸡汤能够让自己好受点。&lt;/p&gt;&#xA;&#xA;&lt;!-- more --&gt;&#xA;&#xA;&lt;p&gt;岛上书店讲的是一个中年男子A.J.的故事。A.J.和妻子妮可放弃博士学位，回到妮可的家乡，一个孤独的小岛，开了一家小岛书店，只为追求读书的乐趣。可是老天总是不会眷顾那些追求平凡的人，没过几年，妮可就不幸车祸去世，A.J.因为失去世上唯一的爱人，从此借酒消愁。可是“灾难”似乎并没有停止的意思，A.J.有天喝醉了，忘记锁门，早上醒来，发现自己收藏的价值几十万的藏书不见了，一个2岁孤儿被遗弃在书店门口，孤儿的母亲希望小女孩能够在书店的氛围下长大，留信嘱托A.J.抚养其长大。而且神奇的A.J.竟然特别喜欢小女孩（玛雅），于是决定收养她。本以为这样两个人会相依为命，知道玛雅长大。可是一个出版社的推销员艾米，闯入了A.J.的生活。A.J.四年前就已经认识了艾米，并且第一次见面特别讨厌艾米，可以说是一场相当不愉快的见面。可是因为一本书，A.J.爱上了艾米，但是艾米已经有一个美国英雄式的未婚夫，虽然A.J.依然鼓起勇气向艾米表达了爱意，但是他心里很清楚他们两个人是不可能的。知道有一天，艾米摔伤了，A.J.借口去拿书，去艾米家里见她。艾米告诉他，自己不想和一个没有共同兴趣爱好的人在一起，想找一个能够分享激情的人常伴一生。就这样两个人在一起了！两个人幸福的举办了婚礼，共同经营着小岛书店，女儿玛雅由于从小就在书店长大，成了一个优秀的小作家。直到有一天A.J.被诊断得了脑瘤，这个幸福的生活无情的被打破了。虽然做了手术可是A.J.依然没有挺过去，丢下了艾米和玛雅，两个深爱着的女人，离开了世界。小岛书店面临着即将关闭的命运， 但是大家认为小岛不能没有书店，没有书店的小镇不能算一个小镇，于是A.J.的好友亚斯兰特和A.J.的妻妹，亚斯兰特的妻子决定接管书店，继续经营。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;小岛上的居民在小岛书店的影响下，大家都喜欢上了读书，虽然大家品味不尽相同，但是都能从读书中知道自己那片世界。《岛上书店》看似是在希望人们回归书店，鼓励大家去读书，并且讲述了一个本来命运糟糕，厄运不断的男子，近乎放弃自己的生活，可是最后有找到了爱情和亲情，从此生活发生了翻天腹地的变化，一个原本孤僻的中年男人，变得乐观开朗起来，甚至影响了整个小镇的居民，成为了小镇不可或缺的一部分。可能是因为最近自己的遭遇，读这本书的时候有很多共鸣，比如A.J.喜欢上艾米却不能在一起的时候跟自己是多么的相似。整个故事场景就是在一个小岛上，在一个普通的书店里，认为只有那么几个，故事也没有那么多跌宕起伏，却给我印象特别深刻。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我们每个人的生活难道不也是这样吗，总是遭遇不幸，但是没有什么大不了的，用书中的话说就是“生活中每一桩糟糕事，几乎都是时机不当的结果，每件好事，都是时机恰到好处的结果”，一切的不幸都会过去的，这些不幸之后，回头来看总是教会了我们很多，自己也会成长许多。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;好句摘录：&lt;/strong&gt;&#xA;&amp;gt;&#xA;&amp;gt; 无人为孤岛，一书一世界&#xA;&amp;gt;&#xA;&amp;gt; 大多数人如果能给更多事情一个机会的话，他们的问题都能解决。&#xA;&amp;gt;&#xA;&amp;gt; 没有人会漫无目的的旅行，那些迷路者是希望迷路。&#xA;&amp;gt;&#xA;&amp;gt; 生活中每一桩糟糕事，几乎都是时机不当的结果，每件好事，都是时机恰到好处的结果。&#xA;&amp;gt;&#xA;&amp;gt; 一个人无法自成孤单，要么至少，一个人无法自成最理想的孤岛。&#xA;&amp;gt;&#xA;&amp;gt; 我们读书而后知道自己并不孤单。我们读书，因为我们孤单;我们读书，然后就不孤单。&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 15 Apr 2016 00:04:27 +0000</pubDate>
    </item>
    <item>
      <title>How to finish a Ph.D.</title>
      <link>http://openhex.cn/2016/3/29/How-to-finish-a-Ph-D.html</link>
      <description>&lt;p&gt;A few years ago I was asked by several Ph.D. students what advice I could give to finish a Ph.D. While I don&amp;rsquo;t think there is only one answer I do have some principles that worked well for me- if you are a current PhD student hopefully you will find this useful also. If you have any comments or suggestions, I&amp;rsquo;d love to hear from you.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Over the years I have received many positive comments from Ph.D. students from the U.S., Canada, and as far as China and Korea. Several students have linked this site form their sites. Thank you so much for your feedback. It means a lot to me that some of my thoughts made a difference to you.&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;**Begin with the end in mind **&#xA;I found it always helpful to know I what my overall goal was. During my PH.D. I aimed to finish in my Ph.D. in 3 years. I didn&amp;rsquo;t make that in the end - it took 4 years - but that isn&amp;rsquo;t important. The important thing is that I knew in order to make 3 years I had to do a certain course load in the first and second term , I had to take the comprehensive exam the first time it was offered, I had a rough idea of how much time I had to write the dissertation. There are road blocks along the way and things turn out different than you expect. But if you know your overall goal obstacles won&amp;rsquo;t through you off the course, you are just taking a detour.&#xA;You have no obligation to write an important or even useful thesis&lt;br /&gt;&#xA;Sometimes students set out to write this all-encompassing break-through thesis and then fail because they try to accomplish too much at once. Very few researchers achieve fame because of their dissertation work. Try to write a good dissertation, not a great dissertation. Further, don&amp;rsquo;t insist on writing a useful thesis. Your primary goal is to get a Ph.D. , not to change the world. There is enough time for changing the world after your dissertation when you have less constraints about what criteria your work has to meet.&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;**Write! **&#xA;A psychology student told me once that he spends the entire day doing research and then forces himself at the end of the day to summarize what he found - even if he doesn&amp;rsquo;t think he found anything that day. This is important for several reasons : (a) writing helps your thoughts to crystallize (b) you accomplish your daily task which will make you feel good &amp;copy; you can track your progress (d) when you write your thesis you have material to draw on (e) you won&amp;rsquo;t forget what you were thinking two weeks ago. In my opinion most students start too late putting their thoughts into words.&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Exercise regularly&lt;/strong&gt;&#xA;I have always found I can work better when I am physically in good shape. During stressful times such as exams, I exercise more often rather than less often. The energy I get from exercise more than compensates for the &amp;ldquo;time lost&amp;rdquo;.&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Enjoy your &amp;ldquo;play time&lt;/strong&gt;&#xA;There is a time to work and a time to play. I try to work hard when I work, and not to think at all about work when I don&amp;rsquo;t work. For example, every year I fly home to Germany for Christmas. I never take work to Germany. All that would accomplish is that I would feel bad the whole time about not doing the work. When you have worked hard all week and can afford to take the week-end off, try to get out and do something fun. Try not to think about work at all.&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Talk to others about your problems&lt;/strong&gt;&#xA;After finishing his Ph.D. a social scientist at an Ivy League university told me once that at some point during his Ph.D. he had so much dissertation anxiety that he went to see a psychologist at the medical center. To his surprise the waiting room for the psychologist was packed and he recognized several other people. Everyone was there for the same reason. He later emailed one of the students he saw whether he wanted to talk about it . Within 10 minutes he got a reply email : the other student was just as desperate to talk about it. Most Ph.D. students at some point or another have problems - talking to fellow students or professors almost always helps. You are not alone. (The above mentioned student graduated smoothly and now excels working at a very prestigious institution).&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Record your progress&lt;/strong&gt;&#xA;Sometime during my second year of my Ph.D. I started writing down every week-end what I had accomplished during the preceeding week. I took great care in this and I often reread what I had done in the past few weeks. This weekly ritual became very important to me and motivated me a great deal. Sometimes in the middle of the week I would realize that I hadn&amp;rsquo;t accomplished anything to be recorded at the end of the week and I would make sure I would get something done.&#xA;In addition, I kept a list of things to do at the white board and marked each item off once I had done it. I wouldn&amp;rsquo;t erase it until a few days later though - because that gave me the satisfaction of seeing what I had accomplished already. I still follow this habit to this day.&#xA;During a Ph.D. you often try something and it doesn&amp;rsquo;t work in the end. That can be frustrating - but I feel that tracking what you have done helps to overcome this frustration. The path to success has unexpected twists and turns in a Ph.D. - and while a failed attempt looks like no progress it really is.&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;**Don&amp;rsquo;t find excuses - don&amp;rsquo;t do too many other important things. **&#xA;Some of the brightest students sometimes have trouble finishing because they are so successful doing other things that may reasonably also be considered important. A very bright young fellow I know kept taking on temporary consulting jobs working for the UN in Brazil and all kind of other exciting and useful jobs. Working for the UN in Brazil is a great experience and you may not want to pass it up. But at some point finishing your Ph.D. outweighs taking on extra consulting jobs.&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Choose a dissertation topic you are passionate about&lt;/strong&gt;&#xA;You will do your best work when you work on a topic that you really care about. This not always possible - but if you have the choice go for it. Also, it is better to come up with your own thesis topic rather than having your supervisor find you a thesis topic. You will find it easier to care deeply about a thesis topic that you came up with yourself.&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Work on your strengths, not on your weaknesses&lt;/strong&gt;&#xA;I was once fortunate enough to have a brunch with the famous statistician Erich Lehman - organized by Agnes Herzberg in Kingston, Ontario. Dr. Lehman had an unusual career and had many things to say. I will never forget the following advice he gave : when in England the professors noticed that his background in mathematics was much stronger than in physics. They therefore forced him to take extra classes in physics. On hindsight Dr. Lehman felt that that was a big mistake. He didn&amp;rsquo;t have any passion for physics and he claims he wasn&amp;rsquo;t good at it either - so there was an extraordinary effort going into something that wasn&amp;rsquo;t necessary.&#xA;There may be situations where our passion requires us to work on something we are not good at. For example, my friend Fiona was never interested in any handyman work. However, she was a theatre major and some point she had to know technical theatre operations. And when it was relevant to theatre, she all of the sudden took an interest in handyman work as it related to theatrical set construction.&#xA;Unless necessary though I always thought that it was good advice to work on one&amp;rsquo;s strengths - because otherwise we&amp;rsquo;ll be constantly disillusioned and frustrated.&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Take charge - it&amp;rsquo;s your life not your supervisor&amp;rsquo;s&lt;/strong&gt;&#xA;I have always found taking an active role leads to better results than a passive or reactive role. It makes life more exciting. For those of us who like playing computer games - it&amp;rsquo;s like the difference of playing the game and watching the game. Playing is just more fun.&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;**Do what is right for you - including the choice of discontinuing your Ph.D. **&#xA;A Ph.D. is not for everyone and I think not to continue a Ph.D. ought to be one of your options. I am most impressed with Judy whom I met during my time as a student. She successfully mastered the comprehensive exam, and then decided that she wasn&amp;rsquo;t really all that interested in research. I still hear her say &amp;ldquo;You know, it&amp;rsquo;s not for everyone&amp;rdquo; - not disappointed but just matter of fact. She is happier now. However , I do think you should only quit because you have come to the conclusion that you do not enjoy research, not because &amp;ldquo;it&amp;rsquo;s overwhelming&amp;rdquo;, &amp;ldquo;it&amp;rsquo;s too much work&amp;rdquo;, or &amp;ldquo;I don&amp;rsquo;t know whether I can do it&amp;rdquo; or &amp;ldquo;I don&amp;rsquo;t like my supervisor&amp;rdquo;. People can do more than they think - they just have to really try.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Repost From&lt;/em&gt;: &lt;a href=&#34;http://www.schonlau.net/&#34;&gt;http://www.schonlau.net/&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Tue, 29 Mar 2016 12:17:32 +0000</pubDate>
    </item>
    <item>
      <title>Welcome</title>
      <link>http://openhex.cn/2016/3/25/welcome.html</link>
      <description>&lt;p&gt;When you read the post, &lt;code&gt;PuGo&lt;/code&gt; is running successfully.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This post is generated from file &lt;code&gt;source/welcome.md&lt;/code&gt;. You can learn it and try to write your own article with following guide.&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;front-matter&#34;&gt;Front-Matter&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;Post&amp;rsquo;s front-matter, including title, author etc, are created by first code section with block &lt;strong&gt;```toml &amp;hellip;.. ```&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;# post title, required&#xA;title = &amp;quot;Welcome&amp;quot;&#xA;&#xA;# post slug, use to build permalink and url, required&#xA;slug = &amp;quot;welcome&amp;quot;&#xA;&#xA;# post description, show in header meta&#xA;desc = &amp;quot;welcome to try pugo site generator&amp;quot;&#xA;&#xA;# post created time, support&#xA;# 2015-11-28, 2015-11-28 12:28, 2015-11-28 12:28:38&#xA;date = &amp;quot;2015-12-20 12:20:20&amp;quot;&#xA;&#xA;# post updated time, optional&#xA;# if null, use created time&#xA;update_date = &amp;quot;2015-12-20 12:30:30&amp;quot;&#xA;&#xA;# author identifier, reference to meta [[author]], required&#xA;author = &amp;quot;pugo&amp;quot;&#xA;&#xA;# tags, optional&#xA;tags = [&amp;quot;pugo&amp;quot;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;content&#34;&gt;Content&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;The content is data after first block. All words will be parsed as markdown content.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;&#xA;When you read the post, `PuGo` is running successfully.&#xA;&#xA;This post is generated from file `source/welcome.md`. You can learn it and try to write your own article with following guide.&#xA;&#xA;...... (markdown content)&#xA;&#xA;Markdown is a lightweight markup language with plain text formatting syntax designed&#xA;so that it can be converted to HTML and many other formats using a tool by the same name.&#xA;Markdown is often used to format readme files, for writing messages in online discussion forums,&#xA;and to create rich text using a plain text editor.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Fri, 25 Mar 2016 12:20:20 +0000</pubDate>
    </item>
    <item>
      <title>回溯法解旅行商问题(TSP)</title>
      <link>http://openhex.cn/2015/11/20/回溯法解旅行商问题-TSP.html</link>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;旅行商问题,常被成为旅行推销员问题,是指一名推销员要拜访多个地点,如何找到再拜访每个地点一次后再回到起点的最短路径.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;进一步的抽象,可以转化为图论的问题,将每个城市看成图$G(V,E)$中的一个顶点,则旅行商问题可以转化为从一个顶点s出发,找到一条最短的路径从s出发,经过所有的顶点,最后回到s.最简单的解法就是枚举,枚举所有的路径,计算其每条路径的长度,取最短的即可.枚举最有的路径则其有$(n-1)!$条路径,如果n非常大,则解的空间将非常大,我们可以使用一个棵树,那么这棵树将非常庞大.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;既然我们可以用一个树表示类似问题的所有可能解,那么现在的问题就变成了从解空间树中找到最短的路径,也就是树的搜索问题,搜索每一条路,找到最短的路径.但是再进一步思考一下,其实很多分枝,我们在没有搜索到叶子节点的时候就可以通过一些已知的条件就可以判断这条路不可能是最短的路.回溯法就是这个思路,按照深度优先的策略搜索解空间树,通过一些约束条件或者边界条件判断是否继续搜索下去,如果不满足约束条件则直接返回.&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;因此回溯法求解问题的过程大致如下:&#xA;&amp;gt;&#xA;* 确定问题的解空间(用树表示)&#xA;* 确定问题的约束条件和边界条件&#xA;* 深度优先搜索解空间幷根据约束条件和边界条件进行裁剪&lt;/p&gt;&#xA;&#xA;&lt;p&gt;再一次的回到旅行商问题(TSP),以此为例看一下回溯法的解题过程&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;确定解空间&#34;&gt;确定解空间&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;假设有四个城市A,B,C,D, 从A出发,求经过所有顶点后回到A的最短路径,我们用树表示所有的解,前面已经说过是一个$(n-1)!$排列树,如下图所示:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_tsp.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;确定约束条件和边界条件&#34;&gt;确定约束条件和边界条件&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;假设用cl表示从A出发到当前节点的的路径长度,fl表示当前最短的环路长度.k表示路径中第k个节点,$x_k$表示第k个点选择的城市编号(A-D编号为1-4),w为邻接矩阵.&#xA;则当k&amp;lt;n时,如果$x&lt;em&gt;k$添加到当前路径已经比当前最优路径大,则不用继续搜索,因此约束条件可以表示为:&#xA;$$cl+w[x&lt;/em&gt;{k-1},x_k] &amp;lt;= fl$$&#xA;当k=n时,也就是叶子节点,如果该回路比当前最优回路短,则此条回路比之前更好,即&#xA;若&#xA;$$cl+w[x_{k-1},x_k] + w[x_k,1] &amp;lt; fl$$&#xA;则更新fl(如果需要记录最有路径,还要更新记录)&#xA;$$fl = cl+w[x_{k-1},x_k] + w[x_k,1]$$&lt;/p&gt;&#xA;&#xA;&lt;p&gt;上面便是约束条件和边界条件&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;深度优先搜索和剪枝&#34;&gt;深度优先搜索和剪枝&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;搜索的问题就比较简单了,按照DFS的思路搜索就行了,当然特定的问题可以进行一些优化.然后搜索的过程使用约束条件的边界条件进行剪枝和更新最有路径.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;下面是伪代码:&#xA;&lt;img src=&#34;/media/archive/img_tsp_pseudocode.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;用java实现如下所示&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;/**&#xA; * 判断第k个数是否不同与前k-1个数&#xA; * @param k&#xA; * @return bool&#xA; */&#xA;private boolean nextValue(int k){&#xA;    int i = 0;&#xA;    while(i &amp;lt; k){&#xA;        if(x[k] == x[i]){&#xA;            return false;&#xA;        }&#xA;        i += 1;&#xA;    }&#xA;    return true;&#xA;}&#xA;&#xA;/**&#xA; * 第k条路径选择&#xA; * @param k&#xA; */&#xA;private void backUp(int k){&#xA;    if(k==N-1){&#xA;        for (int j=1;j&amp;lt;=N;j++){&#xA;            x[k] = Math.floorMod(x[k]+1, N);&#xA;            if(nextValue(k) &amp;amp;&amp;amp; cl + weight[x[k-1]][x[k]] + weight[x[k]][0] &amp;lt; fl) {//如果最短路径,更新最优解&#xA;                fl = cl + weight[x[k - 1]][x[k]] + weight[x[k]][0];&#xA;                for (int i = 0; i &amp;lt; N; i++) {&#xA;                    bestX[i] = x[i];&#xA;                }&#xA;            }&#xA;        }&#xA;    }else{&#xA;        for(int j=1; j&amp;lt;=N; j++){&#xA;            x[k] = Math.floorMod(x[k]+1, N);&#xA;            if(nextValue(k) &amp;amp;&amp;amp; cl+weight[x[k-1]][x[k]] &amp;lt;= fl){&#xA;                //此路可行&#xA;                cl += weight[x[k-1]][x[k]];&#xA;                backUp(k+1);&#xA;                cl -= weight[x[k-1]][x[k]];&#xA;            }&#xA;        }&#xA;&#xA;    }&#xA;}&#xA;public void solve(){&#xA;    int k = 1; //第0个顶点是固定的,从第一个顶点开始选择&#xA;    cl = 0;&#xA;    fl = Integer.MAX_VALUE;&#xA;    backUp(k);&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;完整代码下载见:&lt;a href=&#34;https://github.com/KDF5000/LeetCode/blob/master/java/BackTSP.java&#34;&gt;TSP回溯法&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 20 Nov 2015 14:46:39 +0000</pubDate>
    </item>
    <item>
      <title>回溯法解0/1背包问题</title>
      <link>http://openhex.cn/2015/11/11/回溯法解0-1背包问题.html</link>
      <description>&lt;p&gt;背包问题（Knap sack problem）是一种组合优化的NP完全问题。问题可以描述为：给定一组物品，每种物品都有自己的重量和价值，在限定的总重量内，我们如何选择，才能使得物品的总价值最高。问题的名称来源于如何选择最合适的物品放置于给定背包中。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;0/1背包问题是背包问题的一个特例,在选择物品时只有两种选择,选或不选.解决0/1背包问题就是一个寻找最优解的过程,可以使用动态规划的方法解决.当然最简单的方法就是枚举,列出所有的解,然后计算满足约束的解中最优的解,其实我们可以在枚举的过程中,通过一定的筛选方法去掉那些不可能的解.再进一步思考我们在枚举的过程中是否可以使用一个二叉树来表示选择的过程,这不就是决策树吗?!!通过一定的筛选其实就是剪枝的过程,那么到底满足什么条件时才进行剪枝呢?这个时候约束条件就派上用场了!&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;用$w_i$表示第i个物品的重量,$v_i$表示第i个物品的价值,$p_i=v_i/w_i$为单位i物品的价值,用M表示背包的容量, 则约束条件为:&#xA;$$\sum_{i=1}^{n} w_i \leq M$$&#xA;目标是求价值最大:&#xA;$$max\sum_{i=1}^{n} a_iw_i , a_i =0/1$$&#xA;贪心算法中,我们按照单位价值非递减的顺序排列即:$v_1 \leq v_2\leq v_3 &amp;hellip; \leq v_n$.剪枝时采取的策略是,如果该分支值能够获得的最大价值比当前记录的最大价值小或者相等的话可以直接剪掉,因为该分支并不能得到最优解(相等时可能有一个,但是为了简单我们只求一个).按照我们按照单位价值递增的排序的策略,我们可以定义一个求最大上界的方法,也就是求出某条路径按照贪心的策略获得最大的价值,如果最后一个商品加入后重量超标,则取部分商品放进包里,取最大价值.&#xA;代码如下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def boundF(self, cp, cw, k, M):&#xA;    &amp;quot;&amp;quot;&amp;quot;&#xA;    :param cp:&#xA;    :param cw:&#xA;    :param M:&#xA;    :return: maxValue&#xA;    &amp;quot;&amp;quot;&amp;quot;&#xA;    b = cp&#xA;    c = cw&#xA;    for i in range(k, self._n):&#xA;        c += self._weight[i]&#xA;        if c &amp;lt; M:&#xA;            b += self._value[i]&#xA;        else:&#xA;            return b + (1 - (c-M)/self._weight[i])*self._value[i]&#xA;    return b&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;有了裁剪分支的上界函数后,就可以通过回溯的方法遍历解的空间树,如果一个点满足约束条件就继续深度优先搜索,不满足的时候判断是否需要截枝进行回溯,思路还是简单的,具体过程如下&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt; def backKnap(self, M):&#xA;        cw = 0&#xA;        cp = 0&#xA;        k = 0&#xA;        fp = -1&#xA;        Y = [0 for i in range(self._n)]&#xA;        X = [0 for i in range(self._n)]&#xA;        while True:&#xA;            while k &amp;lt; self._n and cw + self._weight[k] &amp;lt;= M:&#xA;                cw = cw + self._weight[k]&#xA;                cp = cp + self._value[k]&#xA;                Y[k] = 1&#xA;                k += 1&#xA;            if k &amp;gt; self._n - 1:&#xA;                fp = cp&#xA;                k = self._n -1&#xA;                X = Y[:]&#xA;                self._solution.append(X)&#xA;            else:&#xA;                Y[k] = 0&#xA;            while self.boundF(cp, cw, k+1, M) &amp;lt;= fp: # 必须是k+1,若是k第一次探索, fp=-1,boundF()永远不会小于等于-1,会死循环&#xA;                while k != -1 and Y[k] != 1:&#xA;                    k -= 1&#xA;                if k == -1:&#xA;                    return X&#xA;                Y[k] = 0&#xA;                cw -= self._weight[k]&#xA;                cp -= self._value[k]&#xA;            k += 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;上述的程序输出最优解,同时用self._solution保存了所有遍历到叶子节点的解.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;完整代码:&lt;a href=&#34;https://github.com/KDF5000/LeetCode/blob/master/BackKnap.py&#34;&gt;BackKnap&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Wed, 11 Nov 2015 00:50:32 +0000</pubDate>
    </item>
    <item>
      <title>机器学习实战:二分k-均值聚类算法</title>
      <link>http://openhex.cn/2015/11/8/机器学习实战-二分k-均值聚类算法.html</link>
      <description>&lt;p&gt;前面介绍过k-means聚类算法,通过不断的更新簇质心直到收敛为止,但是这个收敛是局部收敛到了最小值,并没有考虑全局的最小值.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;那么一个聚类算法怎么才能称得上效果好呢?要想评价一个算法的好坏,首先需要有一个标准,这也是我们设计算法的时候要首先考虑的,我们设计算法的目的是什么,设计的算法要达到的什么效果,这样才能明确设计算法的目标.一种度量聚类算法效果的指标是SSE(Sum of Squard Error, 误差平方和),也就是前面介绍的计算数据点与质心的欧氏距离的平方和.SSE越小说明数据点越接近它们的质心,聚类效果越好.&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h4 id=&#34;二分k均值算法&#34;&gt;二分k均值算法&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;为了克服k-均值算法的局部最小值问题,有人提出了二分k均值算法.该算法首先将所有点作为一个簇,然后讲该簇一分为二.之后选择其中一个簇继续划分,选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值.上述基于SSE的划分过程不断重复,直到得到用户指定的簇数目为止.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;二分k均值算法的伪代码形式如下:&#xA;&amp;gt;&amp;gt;&#xA;将所有点看成一个簇&#xA;当簇数目小于k时&#xA;    对于每一个簇&#xA;       计算总误差&#xA;       在给定的簇上进行2均值聚类&#xA;       计算讲该簇一分为二之后的总误差&#xA;    选择使得总误差最小的那个簇进行划分操作&lt;/p&gt;&#xA;&#xA;&lt;p&gt;下面是python的实现&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def binKMeans(dataSet, k, distMeas=distEclud):&#xA;    &amp;quot;&amp;quot;&amp;quot;&#xA;    :param dataSet:&#xA;    :param k:&#xA;    :param distMeas:&#xA;    :return:&#xA;    选择一个初始的簇中心(取均值),加入簇中心列表&#xA;    计算每个数据点到簇中心的距离&#xA;    当簇的个数小于指定的k时&#xA;          对已经存在的每个簇进行2-均值划分,并计算其划分后总的SSE,找到最小的划分簇&#xA;          增加一个簇幷更新数据点的簇聚类&#xA;    &amp;quot;&amp;quot;&amp;quot;&#xA;    m = shape(dataSet)[0]&#xA;    clusterAssment = mat(zeros((m, 2)))&#xA;    # 创建一个初始簇, 取每一维的平均值&#xA;    centroid0 = mean(dataSet, axis=0).tolist()[0]&#xA;    centList = [centroid0]  # 记录有几个簇&#xA;    for j in range(m):&#xA;        clusterAssment[j, 1] = distMeas(mat(centroid0), dataSet[j, :]) ** 2&#xA;    while len(centList) &amp;lt; k:&#xA;        lowestSSE = inf&#xA;    #     # 找到对所有簇中单个簇进行2-means可以是所有簇的sse最小的簇&#xA;        for i in range(len(centList)):&#xA;            # 属于第i簇的数据&#xA;            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:, 0].A == i)[0], :]&#xA;            # print ptsInCurrCluster&#xA;            # 对第i簇进行2-means&#xA;            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)&#xA;            # 第i簇2-means的sse值&#xA;            sseSplit = sum(splitClustAss[:, 1])&#xA;            # 不属于第i簇的sse值&#xA;            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A != i), 1])&#xA;            if (sseSplit + sseNotSplit) &amp;lt; lowestSSE:&#xA;                bestCentToSplit = i&#xA;                bestNewCents = centroidMat&#xA;                bestClustAss = splitClustAss.copy()&#xA;                lowestSSE = sseNotSplit + sseSplit&#xA;        # 更新簇的分配结果&#xA;        #新增的簇编号&#xA;        bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList)&#xA;        #另一个编号改为被分割的簇的编号&#xA;        bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit  #&#xA;        # 更新被分割的的编号的簇的质心&#xA;        centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0]&#xA;        # 添加新的簇质心&#xA;        centList.append(bestNewCents[1, :].tolist()[0])&#xA;        # 更新原来的cluster assment&#xA;        clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss&#xA;    return mat(centList), clusterAssment&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;测试&#34;&gt;测试&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;给定一个测试文件,看看分类的效果,测试文件的内容如下,每行有两个值,一个是x坐标一个是y坐标,使用上面的算法看看聚类效果如何&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;3.275154    2.957587&#xA;-3.344465    2.603513&#xA;0.355083    -3.376585&#xA;1.852435    3.547351&#xA;-2.078973    2.552013&#xA;-0.993756    -0.884433&#xA;2.682252    4.007573&#xA;-3.087776    2.878713&#xA;-1.565978    -1.256985&#xA;2.441611    0.444826&#xA;-0.659487    3.111284&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;使用python的图形库matplotlib将测试文件的数据点绘制再二维平面中&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    dataMat = mat(loadDataSet(&#39;testSet2.txt&#39;))&#xA;    centroids, clusterAssment = binKMeans(dataMat, 4)&#xA;    pl.plot(centroids[:, 0], centroids[:, 1], &#39;ro&#39;)&#xA;    pl.plot(dataMat[:, 0], dataMat[:, 1], &#39;bo&#39;)&#xA;    pl.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;结果如下:&#xA;&lt;img src=&#34;/media/archive/img_figure_2.png&#34; alt=&#34;&#34; /&gt;&#xA;图中红色的点是四个簇的质心,可以看出2分k均值算法的聚类效果还是挺好的.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KDF5000/MLPractice/tree/master/ch10&#34;&gt;源码下载&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sun, 08 Nov 2015 22:50:54 +0000</pubDate>
    </item>
    <item>
      <title>机器学习实战:K-Means聚类算法</title>
      <link>http://openhex.cn/2015/11/7/机器学习实战-K-Means聚类算法.html</link>
      <description>&lt;h4 id=&#34;k-means聚类算法&#34;&gt;k-means聚类算法&lt;/h4&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;优点: 容易实现&#xA;缺点: 可能收敛到局部最小值,在大规模数据集上收敛较慢&#xA;使用数据类型: 数值型护具&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;k-均值是发现给定数据集的k个簇的算法.k有用户决定.每一个簇通过旗质心,即簇中所有点的中心描述.&#xA;工作流程: 首先随机确定k个初始点作为其质心.然后讲护具集中的每个点分配到一个簇中,也就是分配到距其最近的质心对应的簇.这一步完成时后,每个簇的质心更新为该簇所有点的平均值.&#xA;上述过程的伪代码如下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;创建k个点作为起始质心(随机选择)&#xA;当任意一个点的簇分配结果改变时:&#xA;    对数据集中的每个数据点&#xA;         对每个质心&#xA;              计算质心到数据点之间的距离&#xA;         讲数据点分配到距其最近的簇&#xA;    对每一个簇,计算簇中所有点的均值幷讲均值作为质心&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h5 id=&#34;起始质心&#34;&gt;起始质心&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;随机生成指定个数的起始质心,一般可能采取选择数据点中的几个点,本文使用所提供的数据点的各个维度的最大值和最小值随机生成基于最大和最小之间的数值,代码如下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# 随机取k个中心&#xA;def randCent(dataSet, k):&#xA;    n = shape(dataSet)[1]  # 列数&#xA;    centroids = mat(zeros((k, n))) # k行n列的矩阵 也就是取k个n维向量&#xA;    for j in range(n):&#xA;        minJ = min(dataSet[:, j])&#xA;        maxJ = max(dataSet[:, j])&#xA;        rangeJ = float(maxJ - minJ)&#xA;         # 生成j列向量&#xA;        centroids[:, j] = minJ + rangeJ * random.rand(k, 1) &#xA;&#xA;    return centroids&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;计算两点的距离&#34;&gt;计算两点的距离&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;计算两点有时候是两个向量(可以认为是高维的点)之间的距离有很多方法,在机器学习或者数据挖掘中经常需要计算两个向量的相似度,实际上也是计算两个向量的距离.计算距离的方法有很多,比如欧氏距离,曼哈顿距离,夹角余弦等等,本文采用的是欧式距离&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# 计算欧式距离&#xA;def distEclud(vecA, vecB):&#xA;    return sqrt(sum(power(vecA-vecB, 2)))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;k-means算法的实现&#34;&gt;k-means算法的实现&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;通过&lt;code&gt;randCent&lt;/code&gt;随机选择k个质心,然后计算每个数据点与各个质心的距离,分配到距离最小的质心所在的簇,然后队每个簇根据其均值重新计算质心,然后在队每个点进行距离起算,聚类直到所有点分配结果不在改变为止.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# k-means算法&#xA;def kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):&#xA;    &amp;quot;&amp;quot;&amp;quot;&#xA;    创建k个点作为起始质心(随机选择)&#xA;    当任意一个点的簇分配结果改变时:&#xA;        对数据集中的每个数据点&#xA;             对每个质心&#xA;                  计算质心到数据点之间的距离&#xA;             讲数据点分配到距其最近的簇&#xA;        对每一个簇,计算簇中所有点的均值幷讲均值作为质心&#xA;    &amp;quot;&amp;quot;&amp;quot;&#xA;    m = shape(dataSet)[0]&#xA;    # 第一列记录最近簇的索引,第二咧是距离&#xA;    clusterAssment = mat(zeros((m, 2)))  &#xA;    centroids = createCent(dataSet, k)&#xA;    clusterChanged = True&#xA;    while clusterChanged:&#xA;        clusterChanged = False&#xA;        for i in range(m):&#xA;            minDist = inf&#xA;            minIndex = -1&#xA;            for j in range(k):&#xA;                distJI = distMeas(centroids[j, :], dataSet[i, :])&#xA;                if distJI &amp;lt; minDist:&#xA;                    minDist = distJI&#xA;                    minIndex = j&#xA;            if clusterAssment[i, 0] != minIndex:&#xA;                clusterChanged = True&#xA;            clusterAssment[i, :] = minIndex, minDist ** 2&#xA;        # 更新质心的位置&#xA;        for cent in range(k):&#xA;            ptsInClust = dataSet[nonzero(clusterAssment[:, 0].A == cent)[0]]&#xA;            centroids[cent, :] = mean(ptsInClust, axis=0)&#xA;    return centroids, clusterAssment&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;测试&#34;&gt;测试&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;给定一个测试文件,看看分类的效果,测试文件的内容如下,每行有两个值,一个是x坐标一个是y坐标,使用上面的算法看看聚类效果如何&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;1.658985    4.285136&#xA;-3.453687    3.424321&#xA;4.838138    -1.151539&#xA;-5.379713    -3.362104&#xA;0.972564    2.924086&#xA;-3.567919    1.531611&#xA;0.450614    -3.302219&#xA;-3.487105    -1.724432&#xA;2.668759    1.594842&#xA;-3.156485    3.191137&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;使用python的图形库matplotlib将测试文件的数据点绘制再二维平面中&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    dataMat = mat(loadDataSet(&#39;testSet.txt&#39;))&#xA;    centroids, clusterAssment = kMeans(dataMat, 4)&#xA;    pl.plot(centroids[:, 0], centroids[:, 1], &#39;r&#39;)&#xA;    pl.plot(dataMat[:, 0], dataMat[:, 1], &#39;bo&#39;)&#xA;    pl.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;结果如下:&#xA;&lt;img src=&#34;/media/archive/img_figure_1.png&#34; alt=&#34;&#34; /&gt;&#xA;图中红色的点是四个簇的质心,可以看出k-means的聚类效果还是挺好的.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KDF5000/MLPractice/tree/master/ch10&#34;&gt;源码下载&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sat, 07 Nov 2015 23:11:32 +0000</pubDate>
    </item>
    <item>
      <title>常用推荐算法(策略)比较</title>
      <link>http://openhex.cn/2015/11/7/常用推荐算法-策略-比较.html</link>
      <description>&lt;h4 id=&#34;常用的推荐算法比较&#34;&gt;常用的推荐算法比较&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;在推荐系统简介中，我们给出了推荐系统的一般框架。很明显，推荐方法是整个推荐系统中最核心、最关键的部分，很大程度上决定了推荐系统性能的优劣。目前，主要的推荐方法包括：基于内容推荐、协同过滤推荐、基于关联规则推荐、基于效用推荐、基于知识推荐和组合推荐。&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;1-基于内容推荐&#34;&gt;1. 基于内容推荐&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;基于内容的推荐（Content-based Recommendation）是信息过滤技术的延续与发展，它是建立在项目的内容信息上作出推荐的，而不需要依据用户对项目的评价意见，更多地需要用机 器学习的方法从关于内容的特征描述的事例中得到用户的兴趣资料。在基于内容的推荐系统中，项目或对象是通过相关的特征的属性来定义，系统基于用户评价对象 的特征，学习用户的兴趣，考察用户资料与待预测项目的相匹配程度。用户的资料模型取决于所用学习方法，常用的有决策树、神经网络和基于向量的表示方法等。 基于内容的用户资料是需要有用户的历史数据，用户资料模型可能随着用户的偏好改变而发生变化。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;基于内容推荐方法的优点是：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;不需要其它用户的数据，没有冷开始问题和稀疏问题。&lt;/li&gt;&#xA;&lt;li&gt;能为具有特殊兴趣爱好的用户进行推荐。&lt;/li&gt;&#xA;&lt;li&gt;能推荐新的或不是很流行的项目，没有新项目问题。&lt;/li&gt;&#xA;&lt;li&gt;通过列出推荐项目的内容特征，可以解释为什么推荐那些项目。&lt;/li&gt;&#xA;&lt;li&gt;已有比较好的技术，如关于分类学习方面的技术已相当成熟。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;缺点是要求内容能容易抽取成有意义的特征，要求特征内容有良好的结构性，并且用户的口味必须能够用内容特征形式来表达，不能显式地得到其它用户的判断情况。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h5 id=&#34;2-协同过滤推荐&#34;&gt;2. 协同过滤推荐&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;协同过滤推荐（Collaborative Filtering Recommendation）技术是推荐系统中应用最早和最为成功的技术之一。它一般采用最近邻技术，利用用户的历史喜好信息计算用户之间的距离，然后 利用目标用户的最近邻居用户对商品评价的加权评价值来预测目标用户对特定商品的喜好程度，系统从而根据这一喜好程度来对目标用户进行推荐。协同过滤最大优 点是对推荐对象没有特殊的要求，能处理非结构化的复杂对象，如音乐、电影。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;协同过滤是基于这样的假设：为一用户找到他真正感兴趣的内容的好方法是首先找到与此用户有相似兴趣的其他用户，然后将他们感兴趣的内容推荐给此用户。其基本 思想非常易于理解，在日常生活中，我们往往会利用好朋友的推荐来进行一些选择。协同过滤正是把这一思想运用到电子商务推荐系统中来，基于其他用户对某一内 容的评价来向目标用户进行推荐。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;基于协同过滤的推荐系统可以说是从用户的角度来进行相应推荐的，而且是自动的，即用户获得的推荐是系统从购买模式或浏览行为等隐式获得的，不需要用户努力地找到适合自己兴趣的推荐信息，如填写一些调查表格等。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;和基于内容的过滤方法相比，协同过滤具有如下的优点：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;能够过滤难以进行机器自动内容分析的信息，如艺术品，音乐等。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;共享其他人的经验，避免了内容分析的不完全和不精确，并且能够基于一些复杂的，难以表述的概念（如信息质量、个人品味）进行过滤。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;有推荐新信息的能力。可以发现内容上完全不相似的信息，用户对推荐信息的内容事先是预料不到的。这也是协同过滤和基于内容的过滤一个较大的差别，基于内容的过滤推荐很多都是用户本来就熟悉的内容，而协同过滤可以发现用户潜在的但自己尚未发现的兴趣偏好。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;能够有效的使用其他相似用户的反馈信息，较少用户的反馈量，加快个性化学习的速度。&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;虽然协同过滤作为一种典型的推荐技术有其相当的应用，但协同过滤仍有许多的问题需要解决。最典型的问题有稀疏问题（Sparsity）和可扩展问题（Scalability）。&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;3-基于关联规则推荐&#34;&gt;3. 基于关联规则推荐&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;基于关联规则的推荐（Association Rule-based Recommendation）是以关联规则为基础，把已购商品作为规则头，规则体为推荐对象。关联规则挖掘可以发现不同商品在销售过程中的相关性，在零 售业中已经得到了成功的应用。管理规则就是在一个交易数据库中统计购买了商品集X的交易中有多大比例的交易同时购买了商品集Y，其直观的意义就是用户在购 买某些商品的时候有多大倾向去购买另外一些商品。比如购买牛奶的同时很多人会同时购买面包。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;算法的第一步关联规则的发现最为关键且最耗时，是算法的瓶颈，但可以离线进行。其次，商品名称的同义性问题也是关联规则的一个难点。&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;4-基于效用推荐&#34;&gt;4. 基于效用推荐&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;基于效用的推荐（Utility-based Recommendation）是建立在对用户使用项目的效用情况上计算的，其核心问题是怎么样为每一个用户去创建一个效用函数，因此，用户资料模型很大 程度上是由系统所采用的效用函数决定的。基于效用推荐的好处是它能把非产品的属性，如提供商的可靠性（Vendor Reliability）和产品的可得性（Product Availability）等考虑到效用计算中。考虑使用用户对商品的评论等&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;5-基于知识推荐&#34;&gt;5. 基于知识推荐&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;基于知识的推荐（Knowledge-based Recommendation）在某种程度是可以看成是一种推理（Inference）技术，它不是建立在用户需要和偏好基础上推荐的。基于知识的方法因 它们所用的功能知识不同而有明显区别。效用知识（Functional Knowledge）是一种关于一个项目如何满足某一特定用户的知识，因此能解释需要和推荐的关系，所以用户资料可以是任何能支持推理的知识结构，它可以 是用户已经规范化的查询，也可以是一个更详细的用户需要的表示。考虑利用用户浏览，购买，搜索建立用户的兴趣集。&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;6-组合推荐&#34;&gt;6. 组合推荐&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;由于各种推荐方法都有优缺点，所以在实际中，组合推荐（Hybrid Recommendation）经常被采用。研究和应用最多的是内容推荐和协同过滤推荐的组合。最简单的做法就是分别用基于内容的方法和协同过滤推荐方法 去产生一个推荐预测结果，然后用某方法组合其结果。尽管从理论上有很多种推荐组合方法，但在某一具体问题中并不见得都有效，组合推荐一个最重要原则就是通 过组合后要能避免或弥补各自推荐技术的弱点。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在组合方式上，有研究人员提出了七种组合思路：&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;加权（Weight）：加权多种推荐技术结果。&lt;/li&gt;&#xA;&lt;li&gt;变换（Switch）：根据问题背景和实际情况或要求决定变换采用不同的推荐技术。&lt;/li&gt;&#xA;&lt;li&gt;混合（Mixed）：同时采用多种推荐技术给出多种推荐结果为用户提供参考。&lt;/li&gt;&#xA;&lt;li&gt;特征组合（Feature combination）：组合来自不同推荐数据源的特征被另一种推荐算法所采用。&lt;/li&gt;&#xA;&lt;li&gt;层叠（Cascade）：先用一种推荐技术产生一种粗糙的推荐结果，第二种推荐技术在此推荐结果的基础上进一步作出更精确的推荐。&lt;/li&gt;&#xA;&lt;li&gt;特征扩充（Feature augmentation）：一种技术产生附加的特征信息嵌入到另一种推荐技术的特征输入中。&lt;/li&gt;&#xA;&lt;li&gt;元级别（Meta-level）：用一种推荐方法产生的模型作为另一种推荐方法的输入。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h5 id=&#34;7-主要推荐方法的对比&#34;&gt;7. 主要推荐方法的对比&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;各种推荐方法都有其各自的优点和缺点，见下表:&lt;/p&gt;&#xA;&#xA;&lt;table&gt;&#xA;&lt;thead&gt;&#xA;&lt;tr&gt;&#xA;&lt;th align=&#34;left&#34;&gt;算法&lt;/th&gt;&#xA;&lt;th align=&#34;left&#34;&gt;优点&lt;/th&gt;&#xA;&lt;th align=&#34;left&#34;&gt;缺点&lt;/th&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/thead&gt;&#xA;&#xA;&lt;tbody&gt;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;left&#34;&gt;基于内容推荐&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;推荐结果直观，容易解释；不需要领域知识&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;稀疏问题；新用户问题；复杂属性不好处理；要有足够数据构造分类器&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;left&#34;&gt;协同过滤推荐&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;新异兴趣发现、不需要领域知识；随着时间推移性能提高；推荐个性化、自动化程度高；能处理复杂的非结构化对象&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;稀疏问题；可扩展性问题；新用户问题；质量取决于历史数据集；系统开始时推荐质量差；&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;left&#34;&gt;基于规则推荐&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;能发现新兴趣点；不要领域知识&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;规则抽取难、耗时；产品名同义性问题；个性化程度低；&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;left&#34;&gt;基于效用推荐&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;无冷开始和稀疏问题；对用户偏好变化敏感；能考虑非产品特性&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;用户必须输入效用函数；推荐是静态的，灵活性差；属性重叠问题；&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&#xA;&lt;tr&gt;&#xA;&lt;td align=&#34;left&#34;&gt;基于知识推荐&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;能把用户需求映射到产品上；能考虑非产品属性&lt;/td&gt;&#xA;&lt;td align=&#34;left&#34;&gt;知识难获得；推荐是静态的&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&#xA;&lt;p&gt;**转自: **&lt;a href=&#34;http://liyonghui160com.iteye.com/blog/2082450&#34;&gt;http://liyonghui160com.iteye.com/blog/2082450&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sat, 07 Nov 2015 11:08:33 +0000</pubDate>
    </item>
    <item>
      <title>归并排序和快速排序比较</title>
      <link>http://openhex.cn/2015/10/18/归并排序和快速排序.html</link>
      <description>&lt;h4 id=&#34;1-归并排序和快速排序&#34;&gt;1 . 归并排序和快速排序&lt;/h4&gt;&#xA;&#xA;&lt;h5 id=&#34;1-1-归并排序&#34;&gt;1.1 归并排序&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;归并排序的思想就是讲数组分为两部分然后对两部分进行排序,然后讲排序后的两部分进行合并,主要的难度在于合并部分,合并的时候需要重新开一个临时数组保存合并的结果,然后再复制到原数组.&#xA;下面是归并排序的&lt;code&gt;python&lt;/code&gt;实现&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# coding:utf-8&#xA;__author__ = &#39;devin&#39;&#xA;&#xA;&#39;&#39;&#39;&#xA;data: array&#xA;low,high: index of array&#xA;&#39;&#39;&#39;&#xA;def merge_sort(data, low, high):&#xA;    if low &amp;lt; high:&#xA;        mid = (low + high)/2&#xA;        merge_sort(data, low, mid)&#xA;        merge_sort(data, mid+1, high)&#xA;        merge(data, low, mid, high)&#xA;&#xA;def merge(data, low, mid, high):&#xA;    temp = []&#xA;    i = low&#xA;    j = mid+1&#xA;    while i &amp;lt;= mid and j &amp;lt;= high:&#xA;        if data[i] &amp;lt;= data[j]:&#xA;            temp.append(data[i])&#xA;            i += 1&#xA;        else:&#xA;            temp.append(data[j])&#xA;            j += 1&#xA;    if i &amp;gt; mid:&#xA;        while j &amp;lt;= high:&#xA;            temp.append(data[j])&#xA;            j += 1&#xA;    else:&#xA;        while i &amp;lt;= mid:&#xA;            temp.append(data[i])&#xA;            i += 1&#xA;&#xA;    i = low&#xA;    j = 0&#xA;    while i &amp;lt;= high:&#xA;        data[i] = temp[j]&#xA;        i += 1&#xA;        j += 1&#xA;&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    data = [1, 3, 2, 6, 3, 7, 2, 12, 15, 11, 10, 131, 1]&#xA;    merge_sort(data, 0, len(data) - 1)&#xA;    print data&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h5 id=&#34;1-1-基于链表的归并排序&#34;&gt;1.1 基于链表的归并排序&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;基于链表的归并排序与一般使用的归并排序算法不同之处主要在于使用链表保存原数组元素的索引,开辟空间对索引排序,不改变原数组元素的顺序.该算法使用数组构造链表,排序后返回链表头索引值,也是原数组第一个元素的索引.这样相比普通的归并排序效率要好一些,省去了复制数组的麻烦,在大规模数据的情况下可以提高一定的性能.&#xA;下面是算法示例,使用&lt;code&gt;python&lt;/code&gt;实现.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# coding: utf-8&#xA;__author__ = &#39;devin&#39;&#xA;import random&#xA;&#xA;class MergeSortLink(object):&#xA;    def __init__(self, data, link):&#xA;        self.data = data&#xA;        self.link = link&#xA;&#xA;    def insert_sort(self, low, high):&#xA;        if low == high:&#xA;            return low&#xA;        head = low&#xA;        i = low + 1&#xA;        while i &amp;lt;= high:&#xA;            temp = self.data[i]&#xA;            p = head&#xA;            pre = p&#xA;            while p != -1:&#xA;                if temp &amp;gt;= self.data[p]:&#xA;                    pre = p&#xA;                    p = self.link[p]&#xA;                else:&#xA;                    break&#xA;            if p == -1:  # 插入尾部&#xA;                self.link[pre] = i&#xA;                self.link[i] = -1&#xA;            elif p == head:  # 插入头部&#xA;                self.link[i] = p&#xA;                head = i&#xA;            else:  # 插入中间&#xA;                self.link[pre] = i&#xA;                self.link[i] = p&#xA;            i += 1&#xA;        return head&#xA;&#xA;    def merge_sort_link(self, low, high):&#xA;        if high-low + 1 &amp;lt; 16:&#xA;            return self.insert_sort(low, high)&#xA;        # if low == high:&#xA;        #     return low&#xA;        else:&#xA;            mid = (low+high)/2&#xA;            q = self.merge_sort_link(low, mid)&#xA;            r = self.merge_sort_link(mid+1, high)&#xA;            return self.merge(q, r)&#xA;&#xA;    def merge(self, q, r):&#xA;        i = q&#xA;        j = r&#xA;        p = None&#xA;        k = 0&#xA;        while True:&#xA;            if self.data[i] &amp;lt;= self.data[j]:&#xA;                if p is None:&#xA;                    p = i&#xA;                else:&#xA;                    self.link[k] = i&#xA;                k = i&#xA;                i = self.link[i]&#xA;            else:&#xA;                if p is None:&#xA;                    p = j&#xA;                else:&#xA;                    self.link[k] = j&#xA;                k = j&#xA;                j = self.link[j]&#xA;            if i == -1 or j == -1:&#xA;                break&#xA;&#xA;        if i == -1:&#xA;            self.link[k] = j&#xA;        else:&#xA;            self.link[k] = i&#xA;        return p&#xA;&#xA;    def print_link(self, p):&#xA;        sorted_data = []&#xA;        while p != -1:&#xA;            sorted_data.append(self.data[p])&#xA;            p = self.link[p]&#xA;        print sorted_data&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    test_data = [random.randint(1, 100) for i in range(50)]&#xA;    print test_data&#xA;    link = [-1 for i in range(len(test_data))]  # -1表示链表结束,为了与索引0区分,所以不能用0&#xA;    sort_link = MergeSortLink(test_data, link)&#xA;    p = sort_link.merge_sort_link(0, len(test_data)-1)&#xA;    sort_link.print_link(p)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;1-2-快速排序&#34;&gt;1.2 快速排序&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;快速排序的思想很简单,在数组中选择一个数,将数组划分为小于和大于该数两个部分,然后在这两个部分进行递归快速排序,因此算法的核心就是划分数的选择.&#xA;下面示例代码使用&lt;code&gt;python&lt;/code&gt;实现.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# coding: utf-8&#xA;__author__ = &#39;devin&#39;&#xA;import random&#xA;&#xA;def partition(data, low, high):&#xA;    index = random.randint(low, high)&#xA;    temp = data[index]&#xA;    data[index] = data[low]&#xA;    data[low] = temp&#xA;    i = low&#xA;    j = high&#xA;    v = data[low]&#xA;    while True:&#xA;        while i &amp;lt;= high and data[i] &amp;lt;= v:&#xA;            i += 1&#xA;        while j &amp;gt;= low and data[j] &amp;gt; v:&#xA;            j -= 1&#xA;        if i &amp;lt; j:&#xA;            temp = data[i]&#xA;            data[i] = data[j]&#xA;            data[j] = temp&#xA;        else:&#xA;            break&#xA;    data[low] = data[j]&#xA;    data[j] = v&#xA;    return j&#xA;&#xA;&#xA;def quick_sort(data, low, high):&#xA;    if low &amp;lt; high:&#xA;        p = partition(data, low, high)&#xA;        quick_sort(data, low, p-1)&#xA;        quick_sort(data, p+1, high)&#xA;&#xA;if __name__ == &amp;quot;__main__&amp;quot;:&#xA;    test_data = [random.randint(1, 100) for i in range(40)]&#xA;    print test_data&#xA;    quick_sort(test_data, 0, len(test_data)-1)&#xA;    print test_data&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;2-算法测试&#34;&gt;2 算法测试&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;使用长读分别为100,200,300,400,500,600,700,800,900,1000的是个数组排列统计第一节中两个算法的时间复杂度&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;2-1-编写测试程序&#34;&gt;2.1 编写测试程序&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;为了方便测试,编写一个测试程序,程序的输入为数据规模因子,比如输入100,则测试的10个数组每个数组为&lt;strong&gt;i*100&lt;/strong&gt;,即100,200,300,400,500,600,700,800,900,1000.然后用&lt;code&gt;python&lt;/code&gt;的图形库&lt;code&gt;matplotlib&lt;/code&gt;输出三个算法的时间-规模折线图.&#xA;测试代码如下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;# coding: utf-8&#xA;__author__ = &#39;devin&#39;&#xA;import time&#xA;import random&#xA;from MergeSort import merge_sort&#xA;from MergeSortL import MergeSortLink&#xA;from QuickSort import quick_sort&#xA;import matplotlib.pyplot as plt&#xA;&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    factor = raw_input()&#xA;    data_scale = [i * int(factor) for i in range(1, 11)]&#xA;    merge_sort_time = []&#xA;    merge_sort_l_time = []&#xA;    quick_sort_time = []&#xA;    print data_scale&#xA;    for i in range(10):&#xA;        scale = data_scale[i]&#xA;        test_data = [random.randint(1, scale*2) for i in range(scale)]&#xA;&#xA;        data_1 = test_data&#xA;        start = time.time()&#xA;        merge_sort(data_1, 0, len(data_1) - 1)&#xA;        end = time.time()&#xA;        merge_sort_time.append(end-start)&#xA;&#xA;        data_2 = test_data&#xA;        link = [-1 for i in range(len(data_2))]  # -1表示链表结束,为了与索引0区分,所以不能用0&#xA;        start = time.time()&#xA;        sort_link = MergeSortLink(test_data, link)&#xA;        p = sort_link.merge_sort_link(0, len(data_2)-1)&#xA;        end = time.time()&#xA;        # sort_link.print_link(p)&#xA;        merge_sort_l_time.append(end-start)&#xA;&#xA;        data_3 = test_data&#xA;        start = time.time()&#xA;        quick_sort(data_3, 0, len(data_3) - 1)&#xA;        end = time.time()&#xA;        # print data_3&#xA;        quick_sort_time.append(end-start)&#xA;&#xA;    print &amp;quot;Scale: &amp;quot;, data_scale&#xA;    print &amp;quot;Merge: &amp;quot;, merge_sort_time&#xA;    print &amp;quot;MergeL: &amp;quot;, merge_sort_l_time&#xA;    print &amp;quot;QuickSort: &amp;quot;, quick_sort_time&#xA;&#xA;    merge_sort_plot = plt.plot(data_scale, merge_sort_time, &#39;b&#39;, label=&#39;MergeSort&#39;)&#xA;    merge_sort_l_plot = plt.plot(data_scale, merge_sort_l_time, &#39;r&#39;, label=&#39;MergeSortLink&#39;)&#xA;    quick_sort_plot = plt.plot(data_scale, quick_sort_time, &#39;g&#39;, label=&#39;QuickSort&#39;)&#xA;&#xA;    quick_sort_plot = plt.plot(data_scale,&#xA;                               [quick_sort_time[0]*data_scale[i]/data_scale[0] for i in range(10)],&#xA;                               &#39;y&#39;, label=&#39;O(n)&#39;)&#xA;    max_time = max(merge_sort_time[9], merge_sort_l_time[9], quick_sort_time[9])&#xA;    plt.xlabel(&amp;quot;Scale&amp;quot;)&#xA;    plt.ylabel(&amp;quot;time&amp;quot;)&#xA;    plt.ylim(0, max_time * 1.2)&#xA;    plt.title(&#39;Algorithm Time &amp;amp; Data Scale&#39;)&#xA;    plt.legend()&#xA;    plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;2-2-测试分析&#34;&gt;2.2 测试分析&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;由于再规模因为为100时,运行时间很短,因此测试因子选择10000,测试结果如下图所示&#xA;&lt;img src=&#34;/images/archive/img_cmd.png&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/img_res.png&#34; alt=&#34;&#34; /&gt;&#xA;在折线图中,黄色实线代表时间复杂度为O(n), 蓝色是普通的归并排序,红色是基于链表的归并排序,绿色是快速排序,可以清楚的看到三种排序算法的时间复杂度明显大于O(n), 小于$O(n^2)$, 而且再数据规模较小的情况下,三者的排序所用时间差不多,数据规模较大是,快速排序和基于链表的归并排序明显优于普通的归并排序,快速排序效果最好,因此再大规模数据下快速排序的速度最快,他们的时间复杂度均为  $O(nlogn)$&lt;/p&gt;&#xA;</description>
      <pubDate>Sun, 18 Oct 2015 22:17:42 +0000</pubDate>
    </item>
    <item>
      <title>Ubuntu 下安装Ruby On Rails</title>
      <link>http://openhex.cn/2015/9/25/Ubuntu-下安装Ruby-On-Rails.html</link>
      <description>&lt;h3 id=&#34;ruby-on-rails的配置&#34;&gt;Ruby On Rails的配置&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Rails&lt;/code&gt;是一个优秀的&lt;code&gt;Ruby&lt;/code&gt;框架.结合了&lt;code&gt;PHP&lt;/code&gt;快速开发和&lt;code&gt;JAVA&lt;/code&gt;程序规整的特点,使用&lt;code&gt;MVC&lt;/code&gt;的设计模式,是一个用于开发数据库驱动的网络应用程序的完整框架.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Ruby On Rails&lt;/code&gt;的官网: &lt;a href=&#34;https://www.ruby-lang.org&#34;&gt;https://www.ruby-lang.org&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;安装&lt;code&gt;Ruby On Rails&lt;/code&gt;需要安装&lt;code&gt;Ruby&lt;/code&gt;,&lt;code&gt;gem&lt;/code&gt;,&lt;code&gt;Sqlite3&lt;/code&gt;,和&lt;code&gt;Rails&lt;/code&gt;. 本教程的平台使用的使用的&lt;code&gt;Ubuntu&lt;/code&gt;,因此只介绍&lt;code&gt;Ubuntu&lt;/code&gt;下的安装.&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;安装-ruby&#34;&gt;安装&lt;code&gt;Ruby&lt;/code&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;安装&lt;code&gt;Ruby&lt;/code&gt;可以参考&lt;code&gt;Ruby&lt;/code&gt;的&lt;a href=&#34;https://www.ruby-lang.org/en/documentation/installation/&#34;&gt;官方教程&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;再&lt;code&gt;Ubuntu&lt;/code&gt;下可以使用&lt;code&gt;sudo apt-get install ruby-fully&lt;/code&gt;进行快速的安装,但是使用这种方式安装的最新版本是&lt;code&gt;1.9.3&lt;/code&gt;,而最新版本的&lt;code&gt;Ruby&lt;/code&gt;是&lt;code&gt;2.2.3&lt;/code&gt;,因此本教程使用编译源码进行安装.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;到&lt;code&gt;Ruby&lt;/code&gt;下载最新的&lt;a href=&#34;https://cache.ruby-lang.org/pub/ruby/2.2/ruby-2.2.3.tar.gz&#34;&gt;源码&lt;/a&gt;,解压到指定的位置,进入解压后的目录,执行下面的命令:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$./configuren&#xA;$ make &#xA;$ sudo make install&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;耐心的等待一段时间,执行下面的命令:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ ruby -v&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果出现下图中的结果,说明安装成功了.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_ruby-v.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;安装-gem&#34;&gt;安装&lt;code&gt;Gem&lt;/code&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Gem&lt;/code&gt;是&lt;code&gt;Ruby&lt;/code&gt;的包管理工具,相当于&lt;code&gt;Python&lt;/code&gt;的&lt;code&gt;pip&lt;/code&gt;,&lt;code&gt;PHP&lt;/code&gt;的&lt;code&gt;Composer&lt;/code&gt;和&lt;code&gt;PEAR&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;可以去&lt;code&gt;RubyGem&lt;/code&gt;的&lt;a href=&#34;https://rubygems.org/&#34;&gt;官网&lt;/a&gt;查看更多的信息.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;安装&lt;code&gt;Gem&lt;/code&gt;也非常简单,下载最新的&lt;a href=&#34;https://rubygems.org/rubygems/rubygems-2.4.8.tgz&#34;&gt;RubyGem&lt;/a&gt;,然后解压到指定目录,进入目录,执行下面的命令即可&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo ruby setup.rb &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果出现下面的结果说明安装成功&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_gem-v.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;安装-sqlite3&#34;&gt;安装&lt;code&gt;sqlite3&lt;/code&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;要想使用&lt;code&gt;Rails&lt;/code&gt;,需要安装&lt;code&gt;Sqlite3&lt;/code&gt;,大多数的&lt;code&gt;Unix&lt;/code&gt;系统已经安装的有了,可以使用&lt;strong&gt;sqlite3 -verion&lt;/strong&gt;检查是否安装了&lt;code&gt;sqlite3&lt;/code&gt;，如果已经安装，则直接进入下一步，如果没有安装则，运行下面的命令进行安装&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo apt-get install sqlite3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;安装-rails&#34;&gt;安装&lt;code&gt;Rails&lt;/code&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;终于到了安装&lt;code&gt;Rails&lt;/code&gt;的时候,当然这也是最后一步,安装&lt;code&gt;Rainls&lt;/code&gt;使用&lt;code&gt;Gem&lt;/code&gt;安装即可,非常方便,再控制台下执行下面的命令即可.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo gem install rails&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;安装完成后执行下面的命令检查&lt;code&gt;Rails&lt;/code&gt;是否安装成功.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$rails -v&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果出现下图,恭喜你安装成功了.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_rails-v.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;测试-rails&#34;&gt;测试&lt;code&gt;Rails&lt;/code&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;使用下面的命令创建一个新的&lt;code&gt;Rails&lt;/code&gt;项目&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo rails new course   # course是项目的名称&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;然后启动&lt;code&gt;Rails Server&lt;/code&gt;,使用下面的命令&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo rails server&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果非常不幸出现了下面的错误信息,则说明没有&lt;code&gt;js&lt;/code&gt;的运行环境,有两种解决方案.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;在Gemfile里添加下面两行,然后执行&lt;code&gt;bundle install&lt;/code&gt;即可.&#xA;&amp;gt;&#xA;gem &amp;lsquo;execjs&amp;rsquo;&lt;br /&gt;&#xA;gem &amp;lsquo;therubyracer&amp;rsquo;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;安装&lt;code&gt;Node.js&lt;/code&gt;,&#xA;&amp;gt; $sudo apt-get install nodejs&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_rails-server-failed.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;再次启动&lt;code&gt;Rails server&lt;/code&gt;,如果出现下面的信息则说明启动成功.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_server_start.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;然后访问&lt;strong&gt;&lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;&lt;/strong&gt;,出现下面的页面则说明安装&lt;code&gt;Rails&lt;/code&gt;成功.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_server.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt; 在启动&lt;code&gt;Rails server&lt;/code&gt;的过程中可能还会出现一个缺少&lt;code&gt;libreadlinedev&lt;/code&gt;(确切的名字不太记得了)的错误,这时只需按照提示安装缺失库,然后重新编译安装&lt;code&gt;Ruby&lt;/code&gt;即可.&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 25 Sep 2015 23:39:46 +0000</pubDate>
    </item>
    <item>
      <title>Python Rq的使用</title>
      <link>http://openhex.cn/2015/8/31/Python-Rq的使用.html</link>
      <description>&lt;p&gt;&lt;code&gt;Rq&lt;/code&gt;的介绍和安装可以参考&lt;a href=&#34;http://kdf5000.github.io/2015/08/23/Ubuntu-14-04-%E4%B8%8B%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8Python-rq%E6%A8%A1%E5%9D%97/&#34;&gt;Ubuntu 14.04 下安装使用Python rq模块&lt;/a&gt;，此文章详细的介绍了安装&lt;code&gt;Rq&lt;/code&gt;的全部过程，文章最后给出了&lt;code&gt;Rq&lt;/code&gt;官方文档的地址。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Rq&lt;/code&gt;官方文档详细的介绍了&lt;code&gt;Queues&lt;/code&gt;, &lt;code&gt;Workers&lt;/code&gt;，&lt;code&gt;Results Jobs&lt;/code&gt;等每个组件的功能和使用，但是并没有指出这几个组件之间到底有什么关系，如果联合起来使用，对于像我这种菜鸟级的码农，一开始还真有点摸不着头脑，思考一番，懂了之后才发现原来是那么的简单&amp;hellip;下面是我自己对&lt;code&gt;Rq&lt;/code&gt;的整体认识图（windows 画图画的有点粗糙望见谅）&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_rq_model.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;用白话解释就是：有一个大工厂，这个工厂有个大仓库（Redis），有一个或者几个管理员（CreateJob），一些工人(Worker)。管理员负责产生任务，把任务放到仓库里特定的位置（特定的Queue），然后工人自己去队列去任务，默默的完成任务，完成成功后按照要求放到特定的位置（可能是仓库也可能是其他地方）。如果任务比较多或者想在短期内完成任务，那么工厂就可以招聘更多的工人去完成这些任务。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;使用&lt;code&gt;Rq&lt;/code&gt;模块最简单的就是只需要一个管理员，一个仓库，一个工人（工人都是一样的，可以复制很多出来）&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;建仓库&#34;&gt;建仓库&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;仓库就是我们安装的时候使用的redis，具体建造过程参见&lt;a href=&#34;http://kdf5000.github.io/2015/08/23/Ubuntu-14-04-%E4%B8%8B%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8Python-rq%E6%A8%A1%E5%9D%97/&#34;&gt;Ubuntu 14.04 下安装使用Python rq模块&lt;/a&gt;中安装&lt;code&gt;Redis&lt;/code&gt;一节&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h4 id=&#34;招管理员&#34;&gt;招管理员&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;仓库建好后，我们就需要一个或者几个管理员来生成不同的任务（Job），放到仓库里即可，&lt;code&gt;Job&lt;/code&gt;的调度不用担心，&lt;code&gt;Rq&lt;/code&gt;模块会自己处理，其实就是不同的工人自发的从仓库里特定的区域拿任务即可。&#xA;一个管理员的主要任务就是产生任务，如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#连接redis&#xA;redis_conn = Redis(host=&#39;192.168.0.108&#39;, port=6379)&#xA;q = Queue(connection=redis_conn, async=True)  # 设置async为False则入队后会自己执行 不用调用perform&#xA;&#xA;with open(&amp;quot;companies.json&amp;quot;, &#39;r&#39;) as f:&#xA;    i = 0&#xA;    for line in f:&#xA;        job = q.enqueue(parse_company, line.strip())&#xA;        i += 1&#xA;        print i, &amp;quot;:&amp;quot;, job.id&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;这个管理员的工作就是从文件&lt;code&gt;Companies.json&lt;/code&gt;读取每一行内容，将每一行内容放到仓库(Redis)默认的位置（Queue），并且指定那一类的&lt;code&gt;worker&lt;/code&gt;去完成这些任务，也就是&lt;code&gt;parse_company&lt;/code&gt;，其实只是指定工人所要做的工作流程，并不是一个工人实体。&lt;code&gt;parse_company&lt;/code&gt;的流程如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def parse_company(json_data):&#xA;    try:&#xA;        obj = json.loads(json_data)&#xA;        company_data = obj[&#39;Company&#39;]&#xA;        new_company_id = insert_company_info(company_data)  #插入公司数据到数据库&#xA;        if new_company_id is None:&#xA;            conn.rollback()&#xA;            with open(&#39;error.txt&#39;, &#39;a&#39;) as ferr:&#xA;                ferr.write(json_data)&#xA;            return None&#xA;        # 股东结构&#xA;        partners = company_data[&#39;Partners&#39;]&#xA;        for val in partners:&#xA;            id = insert_parter_info(new_company_id, val)  #插入股东信息到数据库&#xA;            if id is None:&#xA;                cursor.close()&#xA;                conn.rollback()&#xA;                conn.close()&#xA;                with open(&#39;error.txt&#39;, &#39;a&#39;) as ferr:&#xA;                    ferr.write(json_data)&#xA;                return None&#xA;        cursor.close()&#xA;        conn.commit()&#xA;        conn.close()&#xA;        print &#39;success!&#39;&#xA;        with open(&#39;success.txt&#39;, &#39;a&#39;) as fsu:&#xA;            fsu.write(json_data)&#xA;        return True&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;负责这个任务的工人所要做的工作也比较简单，就是解析每个任务内容（一行json文本），然后插入到数据库中。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;其实管理员的工作还是比较繁重的，既要将大的任务分解成小的任务，又要指定那一类工人（那个工艺流程）去做这些事，这也正是管理员工资比工人工资高的地方吧，虽然不出体力，但是脑力劳动还是比较强的。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;招聘工人&#34;&gt;招聘工人&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;招聘工人其实比较简单了，当然需要付出的，在计算机世界就是要么多买一些计算机或者多开几个线程，然后还是培训这些工人，告诉他们负责那个工艺流程。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;将上一节中指定的工艺流程的文件拷贝一份，放到需要完成任务的计算机上，当然在该计算机也要安装&lt;code&gt;Rq&lt;/code&gt;模块，到此时一个工人的培训已经结束了（有点填鸭式教育的感觉），让工人开始工作只要一个指令即可。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#在parse_company文件所在目录下执行&#xA;$rqworker -u &amp;quot;redis://192.168.0.108:6379&amp;quot;  #-u后面的地址是仓库（redis）的地址&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;这个指令需要告诉工人去哪个仓库取任务，就这么简单，想要招几个工人就招几个工人&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;验收产品&#34;&gt;验收产品&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;每个工人都是按照特定的工艺流程进行的，每个工艺流程指定了产品的输出位置，到相应的位置验收产品即可。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;整个工厂的工艺流程见&lt;a href=&#34;https://github.com/KDF5000/ParseCompany&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Mon, 31 Aug 2015 12:17:11 +0000</pubDate>
    </item>
    <item>
      <title>Ubuntu 14.04 安装图形监控工具Graphite</title>
      <link>http://openhex.cn/2015/8/28/Ubuntu-14-04-安装图形监控工具Graphite.html</link>
      <description>&lt;h3 id=&#34;什么是-graphite&#34;&gt;什么是&lt;code&gt;graphite&lt;/code&gt;?&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;先看看百度百科是怎么介绍&#xA;&amp;gt;&#xA;Graphite 是一个Python写的web应用，采用django框架，Graphite用来进行收集服务器所有的即时状态，用户请求信息，Memcached命中率，RabbitMQ消息服务器的状态，Unix操作系统的负载状态，Graphite服务器大约每分钟需要有4800次更新操作，Graphite采用简单的文本协议和绘图功能可以方便地使用在任何操作系统上。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;百度百科讲的还算是比较清楚了，&lt;code&gt;Graphite&lt;/code&gt;是用来监控系统的， 比如操作系统，缓存服务系统等，但是监控的数据怎么得到呢？&lt;code&gt;Graphite&lt;/code&gt;并不负责，它只负责显示，数据哪里来人家不care，你只要按照他的数据格式给它，&lt;code&gt;Graphite&lt;/code&gt;就可以机智的用漂亮的页面显示给你，不过不用担心，&lt;code&gt;graphite&lt;/code&gt;的安装的一系列套件，提供了&lt;code&gt;API&lt;/code&gt;去传数据给它，而且数据如何存储的也不用我们担心，只管发数据给它就行。说的这么好，到底怎么安装呢？&lt;/p&gt;&#xA;&#xA;&lt;p&gt;先别急，事情总不是那么完美，&lt;code&gt;Graphite&lt;/code&gt;不支持&lt;code&gt;windows&lt;/code&gt;，因此对于只使用&lt;code&gt;Windows&lt;/code&gt;的&lt;code&gt;Coder&lt;/code&gt;就有点小失落了，不过没关系，相信作为程序员都是有办法的，这些都是小事情。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;下面就进入&lt;code&gt;Graphite&lt;/code&gt;的世界！&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;安装&#34;&gt;安装&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;操作系统：Ubuntu 14.04&#xA;Python ：2.7.6&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;安装-graphite-的环境&#34;&gt;安装&lt;code&gt;graphite&lt;/code&gt;的环境&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Graphite&lt;/code&gt;的需要的支持环境如下：&#xA;* a UNIX-like Operating System&#xA;* Python 2.6 or greater&#xA;* Pycairo&#xA;* Django 1.4 or greater&#xA;* django-tagging 0.3.1 or greater&#xA;* Twisted 8.0 or greater (10.0+ recommended)&#xA;* zope-interface (often included in Twisted package dependency)&#xA;* pytz&#xA;* fontconfig and at least one font package (a system package usually)&#xA;* A WSGI server and web server. Popular choices are:&#xA;    * Apache with mod_wsgi&#xA;    * gunicorn with nginx&#xA;    * uWSGI with nginx&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Ubuntu&lt;/code&gt;已经安装了&lt;code&gt;python&lt;/code&gt;，所以不需要安装再安装了，只用确保版本大于等于&lt;strong&gt;2.6&lt;/strong&gt;即可。这里我们服务器选择&lt;code&gt;Apache&lt;/code&gt;，如果已经安装了就不用安装了，只用安装WSGI的模块&lt;code&gt;libapache2-mod-wsgi&lt;/code&gt;。&#xA;下面是安装所有支持环境的命令，建议一个一个安装，可以查看每个安装成功与否。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo apt-get update&#xA;$ sudo apt-get install apache2 libapache2-mod-wsgi python-django python-twisted python-cairo python-pip python-django-tagging&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;安装-graphite-三大组件&#34;&gt;安装&lt;code&gt;Graphite&lt;/code&gt;三大组件&lt;/h4&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;whisper（数据库）&lt;/li&gt;&#xA;&lt;li&gt;carbon（监控数据，默认端口2003，外部程序StatsD通过这个端口，向Graphite输送采样的数据）&lt;/li&gt;&#xA;&lt;li&gt;graphite-web（网页UI）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;使用&lt;code&gt;pip&lt;/code&gt;命令可以快速的安装&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo pip install whisper&#xA;$sudo pip install carbon&#xA;$sudo pip install graphite-web&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;安装完成后默认在&lt;strong&gt;/opt/graphite&lt;/strong&gt;目录&lt;/p&gt;&#xA;&#xA;&lt;p&gt;然后使用&lt;code&gt;Pip&lt;/code&gt;安装&lt;code&gt;pytz&lt;/code&gt;，用于转换&lt;code&gt;TIME_ZONE&lt;/code&gt;，后面会介绍&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo pip install pytz&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;配置-graphite&#34;&gt;配置&lt;code&gt;graphite&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;进入&lt;strong&gt;/opt/graphite/conf&lt;/strong&gt;目录，使用给的&lt;code&gt;example配置&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo cp carbon.conf.example carbon.conf &#xA;$ sudo cp storage-schemas.conf.example storage-schemas.conf &#xA;$ sudo cp graphite.wsgi.example graphite.wsgi  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;为-apache-添加-graphite-的虚拟主机&#34;&gt;为&lt;code&gt;apache&lt;/code&gt;添加&lt;code&gt;Graphite&lt;/code&gt;的虚拟主机&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;安装&lt;code&gt;graphite&lt;/code&gt;的时候会生成一个&lt;code&gt;/opt/graphite/example&lt;/code&gt;的文件夹，里面有一个配置好的虚拟主机文件，将其复制到&lt;code&gt;Apache&lt;/code&gt;放置虚拟主机的配置文件的地方，默认是&lt;strong&gt;/etc/apache2/sites-available&lt;/strong&gt;文件夹&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo cp /opt/graphite/examples/example-graphite-vhost.conf    /etc/apache2/sites-available/graphite-vhost.conf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;然后在编辑修改监听端口为8008以及一个WSGISocketPrefix的默认目录，修改后如下：&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_graphite_install_vhost.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在&lt;strong&gt;/etc/apache2/sites-enable&lt;/strong&gt;下建立该配置文件的软链接&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$cd /etc/apache2/sites-enable&#xA;$sudo ln -s ../sites-available/graphite-vhost.conf   graphite-vhost.conf &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;初始化数据库&#34;&gt;初始化数据库&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;初始化 &lt;code&gt;graphite&lt;/code&gt;需要的数据库，修改 &lt;code&gt;storage&lt;/code&gt; 的权限，用拷贝的方式创建 &lt;code&gt;local_settings.py&lt;/code&gt;文件（中间会问你是不是要创建一个superuser，选择no，把&amp;lt;用户名&amp;gt;改成你当前的Ubuntu的用户名，这是为了让carbon有权限写入whisper数据库，其实carbon里面也可以指定用户的，更新：graphite需要admin权限的用户才能创建User Graph，所以superuser是很重要的，可以使用 python manage.py createsuperuser创建）：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ cd /opt/graphite/webapp/graphite/&#xA;&#xA;$ sudo python manage.py syncdb&#xA;$ sudo chown -R &amp;lt;用户名&amp;gt;:&amp;lt;用户名&amp;gt; /opt/graphite/storage/  &#xA;$ sudo cp local_settings.py.example local_settings.py&#xA;&#xA;$ sudo /etc/init.d/apache2 restart  #重启apache&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;上面代码中的用户名为&lt;code&gt;Apache&lt;/code&gt;对应的用户，一般为&lt;code&gt;www-data&lt;/code&gt;，可以使用下面的代码获得，在&lt;code&gt;apache&lt;/code&gt;的&lt;code&gt;web&lt;/code&gt;根目录（默认：&lt;strong&gt;var/www/html&lt;/strong&gt;）穿件&lt;code&gt;control.php&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;    echo exec(&amp;quot;whoami&amp;quot;);&#xA;?&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;在浏览器访问&lt;code&gt;http://localhost/control.php&lt;/code&gt;既可以看到对应的用户名&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;启动carbon&#34;&gt;启动Carbon&lt;/h4&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ cd /opt/graphite/&#xA;&#xA;$ sudo ./bin/carbon-cache.py start&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;此时在浏览器访问&lt;strong&gt;&lt;a href=&#34;http://localhost:8008&#34;&gt;http://localhost:8008&lt;/a&gt;&lt;/strong&gt;，看到下面页面说明配置成功&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_graphite_index_page.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;如果出现没有权限访问的错误页面，可以修改&lt;code&gt;Apache&lt;/code&gt;配置文件/etc/pache2/apache2.conf,找到下图中的位置，注释掉&lt;strong&gt;Require  all denied&lt;/strong&gt; ，然后重启&lt;code&gt;Apache&lt;/code&gt;再次访问。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_apache_directory.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;修改-graphite-默认的时区&#34;&gt;修改&lt;code&gt;Graphite&lt;/code&gt;默认的时区&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;打开&lt;strong&gt;/opt/graphite/webapp/graphite/setting.py&lt;/strong&gt;，找到&lt;code&gt;TIME_ZONE&lt;/code&gt;，默认是&lt;strong&gt;UTC&lt;/strong&gt;，将其修改为&lt;strong&gt;Asia/Shanghai&lt;/strong&gt;&#xA;，然后找到&lt;code&gt;USE_TZ&lt;/code&gt;，没有的话自己在文件末尾添加，设置为&lt;strong&gt;True&lt;/strong&gt;。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;发送数据到-graphite&#34;&gt;发送数据到&lt;code&gt;graphite&lt;/code&gt;&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;发送数据的方法比较多，科一参考官方文档&lt;a href=&#34;http://graphite.readthedocs.org/en/latest/feeding-carbon.html&#34;&gt;Feeding In Your Data&lt;/a&gt;，此外，在&lt;strong&gt;/opt/graphite/examples&lt;/strong&gt;下提供了一份通过&lt;code&gt;Socket&lt;/code&gt;发送数据的例子&lt;strong&gt;examples-client.py&lt;/strong&gt;。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Graphite官方文档：&lt;a href=&#34;http://graphite.readthedocs.org/en/latest/index.html&#34;&gt;&lt;strong&gt;Graphite官方文档&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 28 Aug 2015 13:35:36 +0000</pubDate>
    </item>
    <item>
      <title>Ubuntu 14.04 安装PHP环境和反向代理</title>
      <link>http://openhex.cn/2015/8/24/Ubuntu-14-04-安装PHP环境和反向代理.html</link>
      <description>&lt;h4 id=&#34;安装apache&#34;&gt;安装Apache&lt;/h4&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo apt-get install apache2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;安装php&#34;&gt;安装php&lt;/h4&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo apt-get install php5 libapache2-mod-php5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;安装mysql&#34;&gt;安装mysql&lt;/h4&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo apt-get install mysql-server&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;反向代理&#34;&gt;反向代理&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;经过测试最小的配置。。。。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;启用apache的mod_proxy 模块&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo a2enmod mod_proxy&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;修改配置文件 /etc/apache2/sites-enabled/000-default.conf （此文件是默认的80端口的配置文件，也可以添加在自己想添加的虚拟主机配置文件），在&lt;VirtualHost&gt;&lt;/VirtualHost&gt;内添加下面的代码&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;ProxyPassReverse    /      http://192.168.0.3:8006/&#xA;ProxyPass           /      http://192.168.0.3:8006/&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如下所示&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_apache-proxy.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;然后重启apache服务器&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo service apache2 restart&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果不成功，则尝试进行下面的操作&#xA;* 重新load apache&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo service apache2 reload&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;在最开始配置代理的地方添加下面两句&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;ProxyPreserveHost On&#xA;ProxyRequests On&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;添加代理的外部访问权限，在配置虚拟主机的地方添加下面几句&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;Proxy *&amp;gt;&#xA;   Order deny,allow&#xA;   Allow from all&#xA;&amp;lt;/Proxy&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;记得每次修改都要重启&lt;code&gt;Apache&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Mon, 24 Aug 2015 20:47:55 +0000</pubDate>
    </item>
    <item>
      <title>Ubuntu 14.04 下安装使用Python rq模块</title>
      <link>http://openhex.cn/2015/8/23/Ubuntu-14-04-下安装使用Python-rq模块.html</link>
      <description>&lt;p&gt;&lt;code&gt;rq&lt;/code&gt; 是&lt;code&gt;Python&lt;/code&gt;的一个第三方模块，使用&lt;code&gt;rq&lt;/code&gt;可以方便快速的实现&lt;code&gt;Python&lt;/code&gt;的队列操作，实现多态电脑的分布式架构。其中 *R*是&lt;code&gt;Redis&lt;/code&gt;的意思，*Q*是&lt;code&gt;Queue&lt;/code&gt;的首字母，&lt;code&gt;rq&lt;/code&gt;使用&lt;code&gt;Redis&lt;/code&gt;和&lt;code&gt;Queue&lt;/code&gt;实现分布式，分别实现了&lt;code&gt;Master&lt;/code&gt;和&lt;code&gt;Worker&lt;/code&gt;，通过&lt;code&gt;Redis&lt;/code&gt;存储任务队列。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;ubuntu14-04-安装rq&#34;&gt;Ubuntu14.04 安装rq&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;假设已经安装了&lt;code&gt;Python&lt;/code&gt;和&lt;code&gt;pip&lt;/code&gt;，本文通过`&lt;code&gt;pip&lt;/code&gt;来安装&lt;code&gt;rq&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo pip install rq &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;安装-redis&#34;&gt;安装&lt;code&gt;Redis&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;rq&lt;/code&gt;模块使用&lt;code&gt;redis&lt;/code&gt;保存队列信息，因此可以保证多台机器同时读取同一个队列，也就是多个`&lt;code&gt;worker&lt;/code&gt;同时工作，这也就达到了我们的目的。在&lt;code&gt;Ubuntu&lt;/code&gt; 下安装&lt;code&gt;Redis&lt;/code&gt;比较简单，使用下面的命令即可，该命令除了安装 &lt;code&gt;Redis&lt;/code&gt;外，也会好心地帮你安装了&lt;code&gt;redis-cli&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo apt-get install redis-server&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;安装完成后可以尝试启动一下&lt;code&gt;Reids&lt;/code&gt;，检查是否安装成功。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ redis-server&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;上面的命令会使用默认的设置启动&lt;code&gt;Redis&lt;/code&gt;服务，如果你看到下面漂亮启动界面说明安装成功了。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;不过还没完额，使用下面命令看看我们可以看到什么&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ netstat -an | grep 6379&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;结果：&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_rq-redis-bind.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;因为&lt;code&gt;Redis&lt;/code&gt;默认使用的端口是6379，该命令可以查看6379端口监听的ip ，可以看到 &lt;code&gt;Redis&lt;/code&gt;默认绑定的是&lt;code&gt;127.0.0.1&lt;/code&gt;，可以在&lt;code&gt;/etc/redis/redis.conf&lt;/code&gt;中看到该设置。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_rq-redis-redis-conf.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Redis&lt;/code&gt;的默认配置绑定了&lt;code&gt;127.0.0.1&lt;/code&gt;，注释掉&lt;strong&gt;bind 127.0.0.1&lt;/strong&gt;即可。然后重启&lt;code&gt;Redis&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo /etc/init.d/redis-server restart&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;再次执行&lt;code&gt;netstat -an | grep 6379&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_rq-redis-redis-netstat.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;可以看到改变了 ，&lt;code&gt;Redis&lt;/code&gt;已经可以接受同一个局域网内的&lt;code&gt;redis cli&lt;/code&gt;连接了&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;安装-rq-dashboard&#34;&gt;安装&lt;code&gt;rq-dashboard&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;rq-dashboard&lt;/code&gt;是一个监控&lt;code&gt;rq&lt;/code&gt;执行状况的&lt;code&gt;python&lt;/code&gt;库，它可以显示当前有哪些&lt;code&gt;Queue&lt;/code&gt;，每个&lt;code&gt;Queue&lt;/code&gt;有多少&lt;code&gt;Job&lt;/code&gt;，以及有多少&lt;code&gt;Worker&lt;/code&gt;处于工作状态，还显示了失败的&lt;code&gt;Job&lt;/code&gt;。可以使用&lt;code&gt;pip&lt;/code&gt;方便的安装&lt;code&gt;Dashboard&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo pip install rq-dashboard&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;安装成功后，使用下面的命令启动&lt;code&gt;rq-dashboard&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$rq-dashboard -u &amp;quot;redus://192.168.0.107:6379&amp;quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;其中&lt;code&gt;-u&lt;/code&gt;参数是需要使用的&lt;code&gt;Redis&lt;/code&gt;连接地址，启动成功后可以看到下面的信息&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_rq-rqdashboard-start.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;可以看出&lt;code&gt;Rq dashboard&lt;/code&gt;的版本信息，以及运行的地址端口，也就是我们可以通过浏览器访问，默认的端口是*9181*，&lt;code&gt;IP&lt;/code&gt;地址是启动&lt;code&gt;rq-dashboard&lt;/code&gt;的机器&lt;code&gt;ip&lt;/code&gt;，在同一局域网的电脑访问&lt;code&gt;http://192.168.0.107:9181&lt;/code&gt;，其中&lt;code&gt;192.168.0.107&lt;/code&gt;是启动&lt;code&gt;rq-dashboard&lt;/code&gt;的电脑&lt;code&gt;ip&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_rq-rqdashboard-web.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Rq-dashboard&lt;/code&gt;是一个很有用的工具，可以图形化的监控&lt;code&gt;rq&lt;/code&gt;的工作状态，但是美中不足，不能控制&lt;code&gt;worker&lt;/code&gt;的工作，不过相信应该很快就会支持这些功能了。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;rq-的使用&#34;&gt;&lt;code&gt;rq&lt;/code&gt;的使用&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;参考&lt;a href=&#34;http://python-rq.org/&#34;&gt;官方文档&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sun, 23 Aug 2015 14:00:16 +0000</pubDate>
    </item>
    <item>
      <title>爬虫学习资料汇总</title>
      <link>http://openhex.cn/2015/8/12/爬虫学习资料汇总.html</link>
      <description>&lt;p&gt;收集写爬虫的相关技术资料以及有用的代码库&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;技术博客&#34;&gt;技术博客&lt;/h4&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Scrapy官方英文文档&lt;/strong&gt;: &lt;a href=&#34;http://doc.scrapy.org/en/latest/index.html&#34;&gt;http://doc.scrapy.org/en/latest/index.html&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Scrapy官方中文文档&lt;/strong&gt;: &lt;a href=&#34;http://scrapy-chs.readthedocs.org/zh_CN/latest/intro/tutorial.html&#34;&gt;http://scrapy-chs.readthedocs.org/zh_CN/latest/intro/tutorial.html&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.tuicool.com/articles/ZbEFnya&#34;&gt;使用Scrapy抓取数据&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.ituring.com.cn/article/114408#&#34;&gt;Scrapy抓取豆瓣电影&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.zhihu.com/question/20899988&#34;&gt;知乎：如何入门爬虫&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.tuicool.com/articles/7JVvaa&#34;&gt;Scrapy爬虫笔记【8-Scrapy核心操作+爬豆瓣图片+数据库链接】&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://cuiqingcai.com/1052.html&#34;&gt;Python爬虫学习系列教程&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://phantomjs.org/&#34;&gt;PhantomJs&lt;/a&gt; : 模拟浏览器解析js，js引擎&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://casperjs.org/&#34;&gt;CasperJs&lt;/a&gt;: 以及&lt;code&gt;phantomjs&lt;/code&gt;的js引擎相比&lt;code&gt;Phantomjs&lt;/code&gt;更加简洁易用&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.tuicool.com/articles/beeMNj/&#34;&gt;PhantomJs快速入门&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h4 id=&#34;开源项目&#34;&gt;开源项目&lt;/h4&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/chineking/cola&#34;&gt;Cola&lt;/a&gt; 一个高水平的分布式爬虫框架&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/grangier/python-goose&#34;&gt;Goose &lt;/a&gt; Html Content / Article Extractor&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/darkrho/scrapy-redis&#34;&gt;scrapy-redis&lt;/a&gt; 基于&lt;code&gt;Redis&lt;/code&gt;和&lt;code&gt;Scrapy&lt;/code&gt;的分布式爬虫框架&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/nvie/rq&#34;&gt;nvie/rq&lt;/a&gt; &lt;code&gt;Python&lt;/code&gt;实现的一个简单的任务队列&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://billmill.org/bloomfilter-tutorial/&#34;&gt;Bloom Filter&lt;/a&gt;: 一个高效Url过滤，去重库&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/scrapy/scrapyd&#34;&gt;Scrapyd&lt;/a&gt;: 部署监控&lt;code&gt;scrapy&lt;/code&gt;的工具&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/scrapy/scrapyd-client&#34;&gt;scrapy-client&lt;/a&gt;: 与&lt;code&gt;Scrapyd&lt;/code&gt;结合发布调用&lt;code&gt;addversion.json&lt;/code&gt;发布&lt;code&gt;Spider&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/scrapinghub/scrapyjs&#34;&gt;ScrapyJs&lt;/a&gt;: &lt;code&gt;scrapy&lt;/code&gt;官方提供的&lt;code&gt;JS&lt;/code&gt;解决方案&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;</description>
      <pubDate>Wed, 12 Aug 2015 14:28:28 +0000</pubDate>
    </item>
    <item>
      <title>Ubuntu下Scrapy的安装</title>
      <link>http://openhex.cn/2015/8/11/Ubuntu下Scrapy的安装.html</link>
      <description>&lt;p&gt;最近在学习爬虫，早就听说&lt;code&gt;Python&lt;/code&gt;写爬虫极爽（貌似pythoner说python都爽，不过也确实，python的类库非常丰富，不用重复造轮子），还有一个强大的框架&lt;code&gt;Scrapy&lt;/code&gt;，于是决定尝试一下。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;要想使用&lt;code&gt;Scrapy&lt;/code&gt;第一件事，当然是安装&lt;code&gt;Scrapy&lt;/code&gt;，尝试了&lt;code&gt;Windows&lt;/code&gt;和&lt;code&gt;Ubuntu&lt;/code&gt;的安装，本文先讲一下 &lt;code&gt;Ubuntu&lt;/code&gt;的安装，比&lt;code&gt;Windows&lt;/code&gt;的安装简单太多了。。。抽时间也会详细介绍一下怎么在&lt;code&gt;Windows&lt;/code&gt;下进行安装。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;http://scrapy-chs.readthedocs.org/zh_CN/latest/intro/install.html#ubuntu-9-10&#34;&gt;官方介绍&lt;/a&gt;，在安装&lt;code&gt;Scrapy&lt;/code&gt;前需要安装一系列的依赖.&#xA;*  &lt;code&gt;Python 2.7&lt;/code&gt;： &lt;code&gt;Scrapy&lt;/code&gt;是&lt;code&gt;Python&lt;/code&gt;框架，当然要先安装&lt;code&gt;Python&lt;/code&gt; ，不过由于&lt;code&gt;Scrapy&lt;/code&gt;暂时只支持 &lt;code&gt;Python2.7&lt;/code&gt;，因此首先确保你安装的是&lt;code&gt;Python 2.7&lt;/code&gt;&#xA;*  &lt;code&gt;lxml&lt;/code&gt;：大多数&lt;code&gt;Linux&lt;/code&gt;发行版自带了&lt;code&gt;lxml&lt;/code&gt;&#xA;*  &lt;code&gt;OpenSSL&lt;/code&gt;：除了&lt;code&gt;windows&lt;/code&gt;之外的系统都已经提供&#xA;*  &lt;code&gt;Python Package&lt;/code&gt;: pip and setuptools. 由于现在&lt;code&gt;pip&lt;/code&gt;依赖&lt;code&gt;setuptools&lt;/code&gt;,所以安装&lt;code&gt;pip&lt;/code&gt;会自动安装&lt;code&gt;setuptools&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;有上面的依赖可知，在非windows的环境下安装 Scrapy的相关依赖是比较简单的，只用安装&lt;code&gt;pip&lt;/code&gt;即可。&lt;code&gt;Scrapy&lt;/code&gt;使用&lt;code&gt;pip&lt;/code&gt;完成安装。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h4 id=&#34;检查-scrapy-依赖是否安装&#34;&gt;检查&lt;code&gt;Scrapy&lt;/code&gt;依赖是否安装&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;你可能会不放心自己的电脑是否已经安装了，上面说的已经存在的依赖，那么你可以使用下面的方法检查一下，本文使用的是&lt;code&gt;Ubuntu 14.04&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;检查-python-的版本&#34;&gt;检查&lt;code&gt;Python&lt;/code&gt;的版本&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ python --version&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;如果看到下面的输出，说明&lt;code&gt;Python&lt;/code&gt;的环境已经安装，我这里显示的是&lt;code&gt;Python 2.7.6&lt;/code&gt;，版本也是&lt;code&gt;2.7&lt;/code&gt;的满足要求。如果没有出现下面的信息，那么请读者自行百度安装&lt;code&gt;Python&lt;/code&gt;，本文不介绍&lt;code&gt;Python&lt;/code&gt;的安装（网上一搜一堆）。&#xA;&lt;img src=&#34;/media/archive/img_scrapy_python.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;检查-lxml-和-openssl-是否安装&#34;&gt;检查&lt;code&gt;lxml&lt;/code&gt;和&lt;code&gt;OpenSSL&lt;/code&gt;是否安装&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;假设已经安装了&lt;code&gt;Python&lt;/code&gt;，在控制台输入&lt;code&gt;python&lt;/code&gt;，进入&lt;code&gt;Python&lt;/code&gt;的交互环境。&#xA;&lt;img src=&#34;/media/archive/img_scrapy_lxml_ssl.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;然后分别输入&lt;code&gt;import lxml&lt;/code&gt;和&lt;code&gt;import OpenSSL&lt;/code&gt;如果没有报错，说明两个依赖都已经安装。&#xA;&lt;img src=&#34;/media/archive/img_scrapy_lxml_openssl.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;安装-python-dev-和-libevent&#34;&gt;安装&lt;code&gt;python-dev&lt;/code&gt;和&lt;code&gt;libevent&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;python-dev&lt;/code&gt;是&lt;code&gt;linux&lt;/code&gt;上开发&lt;code&gt;python&lt;/code&gt;比较重要的工具，以下的情况你需要安装&#xA;* 你需要自己安装一个源外的python类库, 而这个类库内含需要编译的调用python api的c/c++文件&#xA;* 你自己写的一个程序编译需要链接libpythonXX.(a|so)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;libevent&lt;/code&gt;是一个时间出发的高性能的网络库，很多框架的底层都使用了&lt;code&gt;libevent&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;上面两个库是需要安装的，不然后面后报错。使用下面的指令安装&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$sudo apt-get install python-dev&#xA;$sudo apt-get install libevent-dev&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;安装-pip&#34;&gt;安装&lt;code&gt;pip&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;因为&lt;code&gt;Scrapy&lt;/code&gt;可以使用&lt;code&gt;pip&lt;/code&gt;方便的安装，因此我们需要先安装&lt;code&gt;pip&lt;/code&gt;，可以使用下面的指令安装&lt;code&gt;pip&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo apt-get install python-pip&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;使用-pip-安装-scrapy&#34;&gt;使用&lt;code&gt;pip&lt;/code&gt;安装&lt;code&gt;Scrapy&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;使用下面的指令安装&lt;code&gt;Scrapy&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ sudo pip install scrapy&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;记住一定要获得&lt;code&gt;root&lt;/code&gt;权限，否则会出现下面的错误。&#xA;&lt;img src=&#34;/media/archive/img_scrapy_exception.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;至此&lt;code&gt;scrapy&lt;/code&gt;安装完成，使用下面的命令检查&lt;code&gt;Scrapy&lt;/code&gt;是否安装成功。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;$ scrapy version&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;显示如下结果说明安装成功，此处的安装版本是&lt;code&gt;1.02&lt;/code&gt;&#xA;&lt;img src=&#34;/media/archive/img_scrapy_version.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Tue, 11 Aug 2015 23:06:56 +0000</pubDate>
    </item>
    <item>
      <title>PHP常见的五种设计模式——工厂模式</title>
      <link>http://openhex.cn/2015/8/8/PHP常见的五种设计模式——工厂模式.html</link>
      <description>&lt;p&gt;一直对设计模式有一种敬畏之心，每次想要看设计模式的时候就会想到&lt;code&gt;Erich Gamma&lt;/code&gt;，&lt;code&gt;Richard Helm&lt;/code&gt; ， &lt;code&gt;Ralph Johnson&lt;/code&gt;， &lt;code&gt;John Vlissides&lt;/code&gt;的黑皮&lt;code&gt;《设计模式》&lt;/code&gt;，基本都望而止步，要把那本书看完可不是一时半会的，而且在没有项目经验的情况下，个人感觉基本都是纸上谈兵。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;今天在&lt;code&gt;IBM Developerworks&lt;/code&gt;上看到一篇文章将&lt;code&gt;PHP&lt;/code&gt;中常用的五种设计模式，感觉还不错，而且只有五种五种五种（重要的强调三遍）！先从简单的入手，把这五种消灭了再说。以后慢慢学习其他的设计模式。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;工厂模式-factory-pattern&#34;&gt;工厂模式（&lt;code&gt;Factory Pattern&lt;/code&gt;）&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;工厂这个词的使用是非常形象，字面意思可以这样认为，这种模式下，我们有一个工厂，这个工厂生产很多一种或者几种产品（其实多种的情况是覆盖了一种的），但是每个产品怎么生产和包装的我们不知道，其实我们也不需要知道，知道的越多你就越迷糊，以后你的行为就受制于太多杂事，也就是我们常说的耦合度太高，因此我们就将所有的事情交给工厂负责，我们只用告诉工厂需要什么，工厂把产品交付给你就是了。一旦产品的工艺发生改变，工厂负责就好，你使用该产品的工艺不受影响。因此工厂模式可以大大的降低系统的耦合度，增强系统的稳定性，当然也会提高代码的复用率。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在实际的程序设计中，工厂相当于一个对外的接口，那么这个接口的返回类型是确定的，那么我们怎么通过这个工厂来生产不同的产品发回给客户呢？很简单，做一个所有产品的“模子”就可以，这个“模子”有每个产品的所有特征，但是不能用，需要具体的产品实现这些特性，就是我们常说的&lt;code&gt;Interface&lt;/code&gt;。&#xA;使用类图表示如下：&#xA;&lt;img src=&#34;/media/archive/img_Factory_Pattern.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;php-的实现&#34;&gt;&lt;code&gt;PHP&lt;/code&gt;的实现&lt;/h5&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;编写一个接口 &lt;code&gt;Product.php&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;/**&#xA;* Created by PhpStorm.&#xA;* User: Defei&#xA;* Date: 2015/8/8&#xA;* Time: 16:14&#xA;*/&#xA; &#xA;interface Product{&#xA;public function getName();&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;设计一个产品&lt;code&gt;A&lt;/code&gt;实现&lt;code&gt;Product&lt;/code&gt;接口&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;/**&#xA;* Created by PhpStorm.&#xA;* User: Defei&#xA;* Date: 2015/8/8&#xA;* Time: 16:16&#xA;*/&#xA;&#xA;class ProductA implements Product{&#xA; &#xA;public function getName(){&#xA;    // TODO: Implement getName() method.&#xA;    echo &#39;我是产品A&#39;;&#xA;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;设计产品&lt;code&gt;B&lt;/code&gt;实现&lt;code&gt;Product&lt;/code&gt;接口&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;/**&#xA;* Created by PhpStorm.&#xA;* User: Defei&#xA;* Date: 2015/8/8&#xA;* Time: 16:17&#xA;*/&#xA; &#xA;class ProductB implements Product{&#xA; &#xA;public function getName(){&#xA;    // TODO: Implement getName() method.&#xA;    echo &#39;我是产品B&#39;;&#xA;}&#xA; &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;建造一座工厂生产产品&lt;code&gt;A&lt;/code&gt;和&lt;code&gt;B&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;/**&#xA;* Created by PhpStorm.&#xA;* User: Defei&#xA;* Date: 2015/8/8&#xA;* Time: 16:18&#xA;*/&#xA;class ProductFactory{&#xA; &#xA;/**&#xA; * @param $product_name&#xA; * @return mixed&#xA; */&#xA;public function factory($product_name){&#xA;    return new $product_name; //PHP可以使用名字直接new一个同名的对象这个很方便&#xA;}&#xA; &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h5 id=&#34;测试&#34;&gt;测试&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;产品&lt;code&gt;A&lt;/code&gt;和&lt;code&gt;B&lt;/code&gt;已经设计好了，工厂也建好了，下一步就是测试一下这个工厂对的生产能力如何。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;/**&#xA;* Created by PhpStorm.&#xA;* User: Defei&#xA;* Date: 2015/8/8&#xA;* Time: 16:20&#xA;*/&#xA;include &#39;ProductFactory.php&#39;;&#xA;include &#39;Product.php&#39;;&#xA;include &#39;ProductA.php&#39;;&#xA;include &#39;ProductB.php&#39;;&#xA; &#xA;$factory = new  ProductFactory();&#xA;echo $factory-&amp;gt;factory(&#39;ProductA&#39;)-&amp;gt;getName().PHP_EOL;&#xA;echo $factory-&amp;gt;factory(&#39;ProductB&#39;)-&amp;gt;getName();&#xA; &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;输出结果如下：&#xA;&lt;img src=&#34;/media/archive/img_factory_pattern.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Sat, 08 Aug 2015 18:53:37 +0000</pubDate>
    </item>
    <item>
      <title>使用svn和markdown管理项目文档</title>
      <link>http://openhex.cn/2015/7/31/使用svn和markdown管理项目文档.html</link>
      <description>&lt;p&gt;文档管理，是每个项目非常重要又非常让人头疼的问题，每份文档可能会有不同的版本，而且可能还有有不同的人来共同编辑。除此还要考虑怎么才能方便的分享给项目组其他成员查看。也正是这些原因很多笔记记录软件推出了协同办公的功能，比如印象笔记，为知笔记等。但是把东西放在他们那里似乎不是很安全呵，毕竟是项目文档，都是比较私密的东西。也有一些团队协作平台提供了这个功能，不过他们都比较简单，只是上传下载文件。这两种方式似乎都不是很令人满意。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;最近做项目，内部使用&lt;code&gt;svn&lt;/code&gt;管理代码。就想是不是也可以使用svn来管理文档（当然有的公司确实是使用svn管理文档的），等等，这只是解决了文档的多人协作和版本控制的功能。那么怎么才能实现大家更方便的查看文档呢？借鉴现在比较流行的&lt;code&gt;Jekyll&lt;/code&gt;，&lt;code&gt;hexo&lt;/code&gt;搭建博客的思路，使用&lt;code&gt;markdown&lt;/code&gt;写博客，然后生成&lt;code&gt;html&lt;/code&gt;。因此借鉴这个思路，使用&lt;code&gt;svn&lt;/code&gt;管理项目文档，项目文档尽可能使用&lt;code&gt;markdown&lt;/code&gt;编写，然后写一个脚本定时解析&lt;code&gt;md&lt;/code&gt;文件生成&lt;code&gt;html&lt;/code&gt;，搭建一个服务器，将生成的&lt;code&gt;html&lt;/code&gt;部署到服务器，这样同一个局域网的组员就可以访问这些文档了。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;搭建svn&#34;&gt;搭建svn&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;svn&lt;/code&gt; 分为服务端和客户端，搭建详细教程参考&lt;a href=&#34;http://www.jb51.net/article/29005.htm&#34;&gt;&lt;code&gt;windows下svn服务器的搭建&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;解析-md-文件生成-html&#34;&gt;解析&lt;code&gt;md&lt;/code&gt;文件生成&lt;code&gt;html&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;markdown本来就是一种轻量级的标记语言，有很多开源的解析markdown文件的项目。本文使用一个开源的&lt;code&gt;PHP&lt;/code&gt;的开源项目&lt;a href=&#34;https://github.com/erusev/parsedown&#34;&gt;ParseDown&lt;/a&gt;，用来解析markdown生成html文件，在此项目的基础上，封装了一个类，实现解析指定目录下所有markdown文件，生成html文件（包括子目录下的markdown文档，按照原来的目录存放）。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;下面是指定目录下所有的&lt;code&gt;md&lt;/code&gt;文件生成&lt;code&gt;html&lt;/code&gt;文件的源码：&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;/**&#xA;* 解析指定目录下所有的md文件生成html文件&#xA;**/&#xA;require_once &#39;Parsedown.php&#39;;&#xA;class Parse2Html{&#xA;    private $files; //指定目录下的所有文件&#xA;&#xA;    function __construct($dir,$extension=&#39;md&#39;){&#xA;        if($dir==NULL ||!is_dir($dir)){&#xA;            echo &#39;请指定一个合法目录&#39;;&#xA;            exit;&#xA;        }&#xA;        $this-&amp;gt;files = self::getAllFiles($dir);&#xA;    }&#xA;&#xA;    /**&#xA;     * 解析目录下面的所有md文件输出到html文件&#xA;     */&#xA;    public function parse2Html(){&#xA;        foreach ($this-&amp;gt;files as $file ) {&#xA;            $down_text = file_get_contents($file);&#xA;            $file_name = pathinfo($file,PATHINFO_FILENAME);&#xA;            $title = iconv(&#39;gb2312&#39;,&#39;utf-8&#39;,$file_name);&#xA;            $html_content = $this-&amp;gt;html($down_text,$title);&#xA;&#xA;            file_put_contents(dirname($file).DIRECTORY_SEPARATOR.$file_name.&#39;.html&#39;,$html_content);&#xA;            echo $file_name.&#39;export successfully!&#39;.PHP_EOL;&#xA;        }&#xA;    }&#xA;&#xA;    /**&#xA;     * 将markdown内容转换为html&#xA;     * @param $down_text&#xA;     * @param $title&#xA;     * @return string&#xA;     */&#xA;    private function html($down_text, $title){&#xA;        $parsedown = Parsedown::instance();&#xA;        $parsedown-&amp;gt;setBreaksEnabled(true);&#xA;        $html_text = $parsedown-&amp;gt;text($down_text);&#xA;        $html_content = &amp;lt;&amp;lt;&amp;lt;CONTAINER&#xA;&amp;lt;!DOCTYPE html&amp;gt;&#xA;&amp;lt;html lang=&amp;quot;en&amp;quot;&amp;gt;&#xA;&amp;lt;head&amp;gt;&#xA;    &amp;lt;meta http-equiv=&#39;Content-Type&#39; content=&#39;text/html; charset=utf-8&#39; /&amp;gt;&#xA;    &amp;lt;meta charset=&amp;quot;UTF-8&amp;quot;&amp;gt;&#xA;    &amp;lt;title&amp;gt;$title&amp;lt;/title&amp;gt;&#xA;&amp;lt;/head&amp;gt;&#xA;&amp;lt;body&amp;gt;&#xA;    $html_text&#xA;&amp;lt;/body&amp;gt;&#xA;&amp;lt;/html&amp;gt;&#xA;CONTAINER;&#xA;        return $html_content;&#xA;    }&#xA;    /**&#xA;     * 获取指定目录下的所有文件，返回一个数组，数组元素为文件的实际路径&#xA;     * @param $dir&#xA;     * @return array&#xA;     */&#xA;    private static function getAllFiles($dir,$extension=&#39;md&#39;){&#xA;        $files = array();&#xA;        if(is_file($dir)){&#xA;            if(pathinfo($dir,PATHINFO_EXTENSION) == $extension){&#xA;                array_push($files,$dir);&#xA;            }&#xA;            return $files;&#xA;        }&#xA;        $folder = new DirectoryIterator($dir);&#xA;        foreach($folder as $file){&#xA;            if($file-&amp;gt;isFile()){&#xA;                if(pathinfo($file,PATHINFO_EXTENSION) == $extension){&#xA;                    array_push($files,$file-&amp;gt;getRealPath());&#xA;                }&#xA;                continue ;&#xA;            }&#xA;            if(!$file-&amp;gt;isDot()){&#xA;                $sub_files = self::getAllFiles($file-&amp;gt;getRealPath(),$extension);&#xA;                $files = array_merge($files,$sub_files);&#xA;            }&#xA;        }&#xA;        return $files;&#xA;    }&#xA;}&#xA;?&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;调用Parse2Html类的方法如下(index.php):&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;require_once &#39;Parse2Html.php&#39;;&#xA;$dir = dirname(__FILE__).DIRECTORY_SEPARATOR.&#39;Doc&#39;;&#xA;$parse2Html = new Parse2Html($dir);&#xA;$parse2Html-&amp;gt;parse2Html();&#xA;?&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;svn-建立文档仓库&#34;&gt;&lt;code&gt;svn&lt;/code&gt;建立文档仓库&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;新建一个&lt;code&gt;svn&lt;/code&gt;仓库，所有编写的&lt;code&gt;markdown&lt;/code&gt;文档都放进去，然后&lt;strong&gt;将在上一步的&lt;code&gt;index.php&lt;/code&gt;里指定该仓库的目录为解析的目录&lt;/strong&gt;。然后写一个&lt;code&gt;bat&lt;/code&gt;文件，调用&lt;code&gt;index.php&lt;/code&gt;文件生成&lt;code&gt;html&lt;/code&gt;文件，内容如下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;php F:\Boss\Doc\index.php  ；假设已经将php可执行程序添加到环境变量&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;此时运行&lt;code&gt;bat&lt;/code&gt;文件就可以解析&lt;code&gt;svn&lt;/code&gt;目录下的所有&lt;code&gt;md&lt;/code&gt;文件生成&lt;code&gt;html&lt;/code&gt;文档，也可以将该&lt;code&gt;bat&lt;/code&gt;文件添加到系统任务里，设定每天固定时间执行。&#xA;执行结果如下:&#xA;&lt;img src=&#34;/media/archive/img_parse2htmll2.png&#34; alt=&#34;&#34; /&gt;&#xA;其中&lt;code&gt;产品概述.html&lt;/code&gt;是由&lt;code&gt;产品概述.md&lt;/code&gt;文档生成的&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;部署到-apache-服务器&#34;&gt;部署到&lt;code&gt;apache&lt;/code&gt;服务器&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;这里使用的是&lt;code&gt;Apache&lt;/code&gt;服务器，当然也可以是其他服务器，比如&lt;code&gt;IIS&lt;/code&gt;，只要可以解析html文件就可以。将网站的目录设置为&lt;code&gt;svn&lt;/code&gt;文档仓库的根目录，在根目录下编写一个&lt;code&gt;index.php&lt;/code&gt;文件，用于罗列该目录下所有想要显示的文件。内容如下&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;header(&amp;quot;content-type:text/html;charset=gb2312&amp;quot;);&#xA;/**&#xA; * 获取指定目录下的所有文件，返回一个数组，数组元素为文件的实际路径&#xA; * @param $dir&#xA; * @return array&#xA; */&#xA;function getAllFiles($dir,$extension){&#xA;    $files = array();&#xA;    if(is_file($dir)){&#xA;        if(in_array(pathinfo($dir,PATHINFO_EXTENSION),$extension)){&#xA;            array_push($files,$dir);&#xA;        }&#xA;        return $files;&#xA;    }&#xA;    $folder = new DirectoryIterator($dir);&#xA;    foreach($folder as $file){&#xA;        if($file-&amp;gt;isFile()){&#xA;            if(in_array(pathinfo($file,PATHINFO_EXTENSION) ,$extension) ){&#xA;                array_push($files,$file-&amp;gt;getRealPath());&#xA;            }&#xA;            continue ;&#xA;        }&#xA;        if(!$file-&amp;gt;isDot()){&#xA;            $sub_files = getAllFiles($file-&amp;gt;getRealPath(),$extension);&#xA;            $files = array_merge($files,$sub_files);&#xA;        }&#xA;    }&#xA;    return $files;&#xA;}&#xA;$files = getAllFiles(dirname(__FILE__),array(&#39;html&#39;,&#39;doc&#39;,&#39;docx&#39;));&#xA;$root_len = strlen(dirname(__FILE__));&#xA;foreach ($files as $file) {&#xA;    $name = substr($file,$root_len+1,strlen($file)-strlen(pathinfo($file,PATHINFO_EXTENSION))-$root_len-1-1);&#xA;    echo &amp;quot;&amp;lt;a href=&#39;&amp;quot;.substr($file,strlen($_SERVER[&#39;DOCUMENT_ROOT&#39;])).&amp;quot;&#39;/&amp;gt;&amp;quot;.$name.&amp;quot;&amp;lt;/a&amp;gt;&amp;lt;/br&amp;gt;&amp;quot;;&#xA;}&#xA;?&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;此时访问大家好的网站，就可以看到如下结果:&#xA;&lt;img src=&#34;/media/archive/img_parse2html_1.png&#34; alt=&#34;&#34; /&gt;&#xA;&lt;img src=&#34;/media/archive/img_parse2html_3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 31 Jul 2015 22:07:38 +0000</pubDate>
    </item>
    <item>
      <title>Laravel入门教程之一--安装</title>
      <link>http://openhex.cn/2015/7/27/Laravel入门教程之一-安装.html</link>
      <description>&lt;p&gt;&lt;code&gt;Laravel&lt;/code&gt;是一套简洁、优雅的&lt;code&gt;PHP Web&lt;/code&gt;开发框架(&lt;code&gt;PHP Web Framework&lt;/code&gt;)。它可以让你从面条一样杂乱的代码中解脱出来；它可以帮你构建一个完美的网络&lt;code&gt;APP&lt;/code&gt;，而且每行代码都可以简洁、富于表达力。&#xA;在安装&lt;code&gt;Laravel&lt;/code&gt;之前需要先安装&lt;code&gt;PHP&lt;/code&gt;环境，    &lt;code&gt;Laravel&lt;/code&gt;的安装是通过&lt;a href=&#34;https://getcomposer.org/&#34;&gt;&lt;code&gt;Composer&lt;/code&gt;&lt;/a&gt;安装的，所以必须先安装&lt;code&gt;PHP&lt;/code&gt;和&lt;code&gt;Composer&lt;/code&gt;。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;安装-php&#34;&gt;安装&lt;code&gt;PHP&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;本教程使用最常用的&lt;code&gt;PHP&lt;/code&gt;集成环境&lt;code&gt;XAMPP&lt;/code&gt;，&lt;code&gt;XAMPP&lt;/code&gt;已经集成了&lt;code&gt;Apache&lt;/code&gt;，&lt;code&gt;PHP&lt;/code&gt;，&lt;code&gt;MySQL&lt;/code&gt;等开发环境，非常方便，而且搭建简单。&#xA;从&lt;code&gt;XAMPP&lt;/code&gt;的&lt;a href=&#34;https://www.apachefriends.org/zh_cn/index.html&#34;&gt;官网&lt;/a&gt;下载安装包即可，本教程是在&lt;code&gt;Windows&lt;/code&gt;下进行的，所以下载&lt;code&gt;Windows&lt;/code&gt;版即可。&#xA;下载完成后，进行安装，基本上是一路&lt;code&gt;Next&lt;/code&gt;。安装完成后在开始里会出现一个&lt;code&gt;XAMPP&lt;/code&gt;的文件夹，点击里面的&lt;code&gt;XAMPP Control Panel&lt;/code&gt;既可启动&lt;code&gt;Xampp&lt;/code&gt;的控制面板，&lt;code&gt;Apache&lt;/code&gt;,&lt;code&gt;MySQL&lt;/code&gt;等启动停止都可以在这里控制。如下图所示&#xA;&lt;img src=&#34;/media/archive/img_PHP_ENV_1.png&#34; alt=&#34;&#34; /&gt;&#xA;启动&lt;code&gt;Apache&lt;/code&gt;，在浏览器里输入&lt;code&gt;http://localhost&lt;/code&gt;，如果出现如下页面则说明配置成功&#xA;&lt;img src=&#34;/media/archive/img_PHP_ENV_2.png&#34; alt=&#34;&#34; /&gt;&#xA;本教程安装&lt;code&gt;PHP&lt;/code&gt;的介绍到此为止，网上有很多关于&lt;code&gt;XAMPP&lt;/code&gt;安装以及配置的教程，可以自行搜索。如果安装过程中出现问题，可以评论留言。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;安装-composer&#34;&gt;安装&lt;code&gt;Composer&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Composer&lt;/code&gt;是&lt;code&gt;PEAR&lt;/code&gt;之后的一个很好用的包管理工具，也是安装&lt;code&gt;Laravel&lt;/code&gt;的必备工具，详细的安装可以参考鄙人的博客&lt;a href=&#34;http://kdf5000.github.io/2015/07/26/PHP%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86Ciomposer%E7%9A%84%E5%AE%89%E8%A3%85/&#34;&gt;PHP依赖管理Composer&lt;/a&gt;。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;安装-laravel&#34;&gt;安装&lt;code&gt;Laravel&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Laravel&lt;/code&gt;的安装有两种方法，一种是安装&lt;code&gt;Laravel&lt;/code&gt;到本地环境，将&lt;code&gt;Laravel&lt;/code&gt;加入系统变量，使用&lt;code&gt;Laravel&lt;/code&gt;内置的命令创建项目。另一种是使用&lt;code&gt;Composer&lt;/code&gt;的&lt;code&gt;create-project&lt;/code&gt;命令创建项目。当然两种方式各有各的优点，第一种方法使用更加简单，第二种方法不需要安装&lt;code&gt;Laravel&lt;/code&gt;环境&#xA;。应该有其他比较大的差一点，暂时还不知道，等日后研究一下分享给大家，如果知道的大家分享一下。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h5 id=&#34;使用-composer-命令创建-laravel-项目&#34;&gt;使用&lt;code&gt;Composer&lt;/code&gt;命令创建&lt;code&gt;Laravel&lt;/code&gt;项目&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;使用&lt;code&gt;Composer&lt;/code&gt;创建比较简单，代开命令行，输入下面指令即可&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;composer create-project laravel/laravel learnlaravel5 5.0.22&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;其中&lt;code&gt;learnlaravel5&lt;/code&gt;是创建的项目的名字，&lt;code&gt;5.0.22&lt;/code&gt;是使用的&lt;code&gt;Laravel&lt;/code&gt;版本号，也可以不指定，默认应该是使用最新版本。这个过程会下载很多组件，时间可能会比较久，留意一下命令行下载过程如果出现错误，立刻停止(&lt;code&gt;Ctrl + C&lt;/code&gt;)，看看错误原因是什么。&#xA;创建完成后，可以在&lt;code&gt;Apache&lt;/code&gt;创建一个虚拟主机，将目录只想新建的&lt;code&gt;Laravel&lt;/code&gt;项目的&lt;code&gt;Public&lt;/code&gt;目录下。&#xA;* 修改&lt;code&gt;http-vhosts.conf&lt;/code&gt;，该文件在&lt;code&gt;xampp安装目录\apache\conf\extra&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;Listen 8008  #监听的端口&#xA;&amp;lt;VirtualHost *:8008&amp;gt;&#xA;    ##ServerAdmin webmaster@dummy-host.example.com&#xA;    DocumentRoot &amp;quot;F:/CodingDemo/Laravel/LearnLaravel/learnlaravel5/public&amp;quot;  #laravel项目的public目录&#xA;    ##ServerName dummy-host.example.com&#xA;    ##ServerAlias www.dummy-host.example.com&#xA;    ErrorLog &amp;quot;logs/laravel.com-error.log&amp;quot;&#xA;    CustomLog &amp;quot;logs/laravel.com-access.log&amp;quot; common&#xA;&amp;lt;/VirtualHost&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;该配置指定了&lt;code&gt;apache&lt;/code&gt;服务器监听&lt;code&gt;8008&lt;/code&gt;是访问&lt;code&gt;Laravel&lt;/code&gt;项目。此时启动&lt;code&gt;Apache&lt;/code&gt;，访问&lt;code&gt;localhost:8008&lt;/code&gt;，可能出现下面的&lt;code&gt;Access forbidden&lt;/code&gt;错误页面。&#xA;&lt;img src=&#34;/media/archive/img_laravel_2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;出现上面的访问禁止的原因主要是&lt;code&gt;Apache&lt;/code&gt;配置拒绝了所有的请求，最简单的就是注释掉，打开&lt;code&gt;xampp安装目录\apache\conf\httpd.conf&lt;/code&gt;，修改下面的地方，注释掉&lt;code&gt;Require all denied&lt;/code&gt;，将&lt;code&gt;AloolwOverride&lt;/code&gt;改为&lt;code&gt;All&lt;/code&gt;。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;Directory /&amp;gt;&#xA;AllowOverride All&#xA;#Require all denied&#xA;&amp;lt;/Directory&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;再次访问&lt;code&gt;localhost:8008&lt;/code&gt;，出现下面的页面，说明安装成功。&#xA;&lt;img src=&#34;/media/archive/img_laravel_3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;</description>
      <pubDate>Mon, 27 Jul 2015 12:11:58 +0000</pubDate>
    </item>
    <item>
      <title>PHP依赖管理Ciomposer的安装</title>
      <link>http://openhex.cn/2015/7/26/PHP依赖管理Ciomposer的安装.html</link>
      <description>&lt;p&gt;对于现代语言，包管理基本上市标配，通过包管理可以大大提高项目的开发效率，也使得项目的结构更加的清晰。&lt;code&gt;Java&lt;/code&gt;有&lt;code&gt;Maven&lt;/code&gt;，&lt;code&gt;Python&lt;/code&gt;有&lt;code&gt;PIP&lt;/code&gt;，&lt;code&gt;Ruby&lt;/code&gt;有&lt;code&gt;gem&lt;/code&gt;，&lt;code&gt;NodeJs&lt;/code&gt;有&lt;code&gt;PEAR&lt;/code&gt;，不过&lt;code&gt;PEAR&lt;/code&gt;坑不少：&#xA;* 依赖处理容易出问题&#xA;* 配置复杂&#xA;* 难用的命令行接口&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Composer&lt;/code&gt;的出现，是的&lt;code&gt;PHP&lt;/code&gt;的依赖管理更加容易，相比&lt;code&gt;PEAR&lt;/code&gt;简单易用，而且是开源的，提交自己的包夜很容易&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;安装-composer&#34;&gt;安装&lt;code&gt;Composer&lt;/code&gt;&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Composer&lt;/code&gt;的需要&lt;code&gt;PHP 5.3.2+&lt;/code&gt;才能运行&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;windows-下安装&#34;&gt;&lt;code&gt;Windows&lt;/code&gt;下安装&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;进入&lt;code&gt;Composer&lt;/code&gt;的&lt;a href=&#34;https://getcomposer.org/download/&#34;&gt;官网&lt;/a&gt;下载&lt;code&gt;Windows Installer&lt;/code&gt;进行安装，安装的前提是你已经安装了&lt;code&gt;PHP&lt;/code&gt;，安装过程中会让你选择&lt;code&gt;PHP&lt;/code&gt;的可执行程序路径，也可以选择将&lt;code&gt;Composer&lt;/code&gt;组件加入到右键快捷键，如下图所示&#xA;&lt;img src=&#34;/media/archive/img_composer_1.png&#34; alt=&#34;&#34; /&gt;&#xA;安装成功后，打开命令行，执行&lt;code&gt;composer --version&lt;/code&gt;，如果出现下图所示，则说明安装成功。&#xA;&lt;img src=&#34;/media/archive/img_composer_2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;修改&lt;code&gt;Composer&lt;/code&gt;的配置文件，指定获取包的镜像服务地址。&lt;/p&gt;&#xA;&#xA;&lt;!--more --&gt;&#xA;&#xA;&lt;p&gt;打开命令行，输入下面的命令即可。此方法是修改全局配置，也可以针对单个项目进行配置。参考&lt;a href=&#34;http://pkg.phpcomposer.com/&#34;&gt;http://pkg.phpcomposer.com/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;composer config -g repositories.packagist composer http://packagist.phpcomposer.com&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;类-unix-的安装&#34;&gt;类&lt;code&gt;Unix&lt;/code&gt;的安装&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;这里介绍一种全局安装的方法，其他放发可以参考&lt;a href=&#34;http://docs.phpcomposer.com/00-intro.html&#34;&gt;http://docs.phpcomposer.com/00-intro.html&lt;/a&gt;。&#xA;此方法将&lt;code&gt;Composer&lt;/code&gt;放在系统的&lt;code&gt;PATH&lt;/code&gt;里，这样就可一再任何地方使用&lt;code&gt;Composer&lt;/code&gt;命令。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;curl -sS https://getcomposer.org/installer | php&#xA;mv composer.phar /usr/local/bin/composer&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Sun, 26 Jul 2015 22:22:28 +0000</pubDate>
    </item>
    <item>
      <title>码农14-19期</title>
      <link>http://openhex.cn/2015/7/24/码农14-19期.html</link>
      <description>&lt;h4 id=&#34;14期-web-app不权威指南&#34;&gt;14期：Web.App不权威指南&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_ituring_cover_14.png&#34; alt=&#34;&#34; /&gt;&#xA;&lt;strong&gt;下载地址：&lt;/strong&gt; &lt;a href=&#34;/images/archive/pdf_码农_第14期.pdf&#34;&gt;码农第14期&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;!--more --&gt;&#xA;&#xA;&lt;h4 id=&#34;15期-cto长成记&#34;&gt;15期：CTO长成记&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_ituring_cover_15.png&#34; alt=&#34;&#34; /&gt;&#xA;&lt;strong&gt;下载地址：&lt;/strong&gt; &lt;a href=&#34;/images/archive/pdf_码农_第15期.pdf&#34;&gt;码农第15期&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;16期-进击的java&#34;&gt;16期：进击的java&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_ituring_cover_16.png&#34; alt=&#34;&#34; /&gt;&#xA;&lt;strong&gt;下载地址：&lt;/strong&gt; &lt;a href=&#34;/images/archive/pdf_码农_第16期.pdf&#34;&gt;码农第16期&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;17期-如何成为一位数据科学家&#34;&gt;17期：如何成为一位数据科学家&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_ituring_cover_17.png&#34; alt=&#34;&#34; /&gt;&#xA;&lt;strong&gt;下载地址：&lt;/strong&gt; &lt;a href=&#34;/images/archive/pdf_码农_第17期.pdf&#34;&gt;码农第17期&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;18期-hello-node&#34;&gt;18期：Hello Node&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_ituring_cover_18.png&#34; alt=&#34;&#34; /&gt;&#xA;&lt;strong&gt;下载地址：&lt;/strong&gt; &lt;a href=&#34;/images/archive/pdf_码农_第18期.pdf&#34;&gt;码农第18期&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;19期-android好手&#34;&gt;19期：Android好手&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/img_ituring_cover_19.png&#34; alt=&#34;&#34; /&gt;&#xA;&lt;strong&gt;下载地址：&lt;/strong&gt; &lt;a href=&#34;/images/archive/pdf_码农_第19期.pdf&#34;&gt;码农第19期&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 24 Jul 2015 22:31:25 +0000</pubDate>
    </item>
    <item>
      <title>一个专注于互联网人士的书籍交流平台</title>
      <link>http://openhex.cn/2015/7/24/一个专注于互联网人士的书籍交流平台.html</link>
      <description>&lt;h4 id=&#34;想法起源&#34;&gt;想法起源&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;由于以前一直做项目，学习了&lt;code&gt;PHP&lt;/code&gt;，&lt;code&gt;Android&lt;/code&gt;，&lt;code&gt;Python&lt;/code&gt;等编程语言，但都是以项目为主，很多系统性的东西都没有掌握，基本处于知道&lt;code&gt;How&lt;/code&gt;不知道&lt;code&gt;Why&lt;/code&gt;的阶段，而且当时学习数据结构和算法课程的时候也是水过去了，所以一直想抽时间补充一些计算机学科的基础知识，学习一些偏基础的东西，进段时间就想制定一个读书计划。说干就干，首先先从算法搞起，于是各种百度，豆瓣读书，看了很多评论和推荐，有的说黑皮的数据结构入门不错，有的说黑皮的算法导论不错，有的说&lt;code&gt;Sedgewick&lt;/code&gt;的算法（第四版）很好。为什么选个书都这么难呢？为什么不能有一个地方专门为某个特定的行业提供书籍交流呢？&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;想法的巩固&#34;&gt;想法的巩固&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;今天又突然看到码农19期184页巩朋&lt;strong&gt;《程序员必读书单1.0》&lt;/strong&gt;这篇文章，巩工程师以个人的经验将程序员需要掌握的知识分为三大类&amp;rdquo;&lt;strong&gt;入门-必读-延伸&lt;/strong&gt;&amp;ldquo;，然后详细的介绍了每一类应该读什么书籍，以及为什么选择这些书籍，都是发自肺腑的为读者推荐。现实中也是这样的，喜欢读书的人，也喜欢分享他们读过的书籍，并且对于他们感兴趣的领域有自己独特体验，每个行业的从业人员都是一步步成长起来的，尤其是互联网行业，可谓真的是活到老学到老，如果能够有一个社区让那些有经验的&amp;rdquo;长者&amp;rdquo;分享他们认为的书单，大家一块讨论交流，那么我们读书的过程既不会毫无方向，也能够找到书友不至于那么的枯燥。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;产品设计&#34;&gt;产品设计&lt;/h4&gt;&#xA;&#xA;&lt;h5 id=&#34;定位&#34;&gt;定位&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;专注于互联网人士的书籍交流平台&lt;/p&gt;&#xA;&#xA;&lt;!--more --&gt;&#xA;&#xA;&lt;h5 id=&#34;目标用户&#34;&gt;目标用户&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;喜欢读书的互联网人士或者想要入行的人士&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;产品功能设计&#34;&gt;产品功能设计&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;略&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;产品技术分析&#34;&gt;产品技术分析&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;略&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;注&lt;/strong&gt;：现在只是有个想法，希望感兴趣的认识能够积极参与讨论&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 24 Jul 2015 21:33:56 +0000</pubDate>
    </item>
    <item>
      <title>为应用程序添加缓存</title>
      <link>http://openhex.cn/2015/7/24/为应用程序添加缓存.html</link>
      <description>&lt;h3 id=&#34;服务端添加缓存&#34;&gt;服务端添加缓存&lt;/h3&gt;&#xA;&#xA;&lt;h4 id=&#34;1-背景&#34;&gt;1. 背景&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;由于项目进度赶得比较紧，而且第一次自己设计系统的架构，刚开始考虑的并不完全，主要想着先把系统的功能实现了再说。因此刚开始设计系统的时候并没有考虑缓存的问题，但是对已一个web系统，缓存不仅可以大大的减少数据库的压力，也可以很大程度的提高系统的响应时间。现在系统的功能完成的基本差不多了，因此现在需要为系统添加缓存，但是由于系统功能已经完成的差不多了，代码写的也很多了，所以现在添加缓存确实显得比较困难了。&#xA;    先说说我们当前系统的整体框架结构吧。&#xA;    &lt;img src=&#34;/media/archive/jiagou.png&#34; alt=&#34;Alt text&#34; /&gt;&#xA;    从图中可以清楚的看出，我们目前采用的架构比较简单，所有的请求都通过&lt;code&gt;.htaccess&lt;/code&gt;文件转发到&lt;code&gt;index.php&lt;/code&gt;然后在&lt;code&gt;index.php&lt;/code&gt;文件中启动转发函数，通过请求的将请求分配到特定的&amp;rsquo;Server&amp;rsquo;执行特定的&lt;code&gt;Action&lt;/code&gt;，在此&lt;code&gt;Action&lt;/code&gt;里调用一个服务从数据库里取出数据返回给客户端。下面是当前系统的类图(为了显示出调用的方法，所以有点不规范)。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;p&gt;&lt;img src=&#34;/media/archive/Main.png&#34; alt=&#34;Alt text&#34; /&gt;&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;2-添加缓存&#34;&gt;2. 添加缓存&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;添加缓存的方法有很多，最让人想到的就是直接在&lt;code&gt;dao&lt;/code&gt;层添加，在从数据库取数据之前，首先先从缓存取，如果缓存没有的话再从数据库取，然后放到缓存里。另一种策略是在&lt;code&gt;service&lt;/code&gt;层添加缓存，也就说从将从&lt;code&gt;dao&lt;/code&gt;层取出的取出的数据进行拼装之后放到缓存里。&#xA;    首先分析一下两种方案的优缺点：&#xA;    第一种方案：&#xA;    优点：首先在&lt;code&gt;dao&lt;/code&gt;层添加缓存对每条&lt;code&gt;sql&lt;/code&gt;的执行结果进行缓存，缓存的粒度比较小，缓存的命中率会比较高。&#xA;    缺点：每个&lt;code&gt;sql&lt;/code&gt;查询数据不一样，则可能本来是同一个信息，但是可能仅仅是多了一个字段确又多了一条缓存记录，这是很大的浪费，毕竟缓存的成本还是比较高的。其次如果直接在&lt;code&gt;dao&lt;/code&gt;层进行缓存，有些地方其实是不想进行缓存的，那么这样就比较不方便控制了。&#xA;    第二种方案：&#xA;    优点：在&lt;code&gt;service&lt;/code&gt;进行缓存，首先是在原来系统的基础上添加代码比较少，其次对拼装好的数据缓存由于&lt;code&gt;service&lt;/code&gt;的参数一般变化不大，缓存的数据就会比较少，命中率也不错。&#xA;    缺点：缓存的粒度比较大，命中率不会那么理想，而且缓存的重用性也比较差。&#xA;    综合上面的分析基本可以知道无论是在&lt;code&gt;dao&lt;/code&gt;层还是&lt;code&gt;service&lt;/code&gt;层进行缓存都既有优点又有缺点，最后经过考虑结合两者的优点，抽象出一个缓存层，在缓存层里控制是否进行缓存，以及缓存的逻辑，同时将缓存的对象改为数据库对应的一个一个对象，这样缓存对象的重用性以及命中率都有很不错的效果。&#xA;    下面是添加缓存后的类图：&#xA;    &lt;img src=&#34;/media/archive/cache2.jpg&#34; alt=&#34;Alt text&#34; /&gt;&#xA;    在原来的框架基础上再&lt;code&gt;service&lt;/code&gt;里直接调用&lt;code&gt;cacheServer&lt;/code&gt;加载指定的&lt;code&gt;cache&lt;/code&gt;，原来的&lt;code&gt;service&lt;/code&gt;逻辑不用改变，只有叫原来调用&lt;code&gt;dao&lt;/code&gt;的改为调用&lt;code&gt;cache&lt;/code&gt;即可。&#xA;    到此缓存基本已经添加完成，编码也完成了，使用起来目前感觉还可以，又不合适的地方希望读者可以多多交流。&#xA;    最后感谢&lt;a href=&#34;http://www.crackedzone.com/server-need-a-new-datastore-layar.html&#34;&gt;http://www.crackedzone.com/server-need-a-new-datastore-layar.html&lt;/a&gt;文章的博主，给了我很到的启发。&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 24 Jul 2015 18:49:59 +0000</pubDate>
    </item>
    <item>
      <title>PHP与Redis的仿Twitter实例</title>
      <link>http://openhex.cn/2015/7/24/PHP与Redis的仿Twitter实例.html</link>
      <description>&lt;p&gt;译者注：&#xA;原文位于Redis官网&lt;a href=&#34;http://redis.io/topics/twitter-clone&#34;&gt;http://redis.io/topics/twitter-clone&lt;/a&gt;&#xA;Redis是NoSQL数据库中一个知名数据库，在新浪微博中亦有部署，适合固定数据量的热数据的访问。&#xA;作为入门，这是一篇很好的教材，简单描述了如何使用KV数据库进行数据库的设计。新的项目www.xiayucha.com亦采用Redis + MySQL进行开发，考虑Redis文档比较少，故翻译了此文。&#xA;其他参考资料：&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&#34;http://redis.readthedocs.org/en/latest/index.html&#34;&gt;Redis命令参考中文版(Redis Command Reference&lt;/a&gt;&#xA;&lt;a href=&#34;http://try.redis-db.com/&#34;&gt;Try Redis&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我会在此文中描述如何使用&lt;code&gt;PHP&lt;/code&gt;以及仅使用&lt;code&gt;Redis&lt;/code&gt;来设计实现一个简单的&lt;code&gt;Twitter&lt;/code&gt;克隆。很多编程社区常认为 &lt;code&gt;KV&lt;/code&gt; 储存是一个特别的数据库，在 &lt;code&gt;web&lt;/code&gt; 应用中不能替代关系数据库。本文尝试证明这恰恰相反。这个&lt;code&gt;twitter&lt;/code&gt;克隆名为&lt;code&gt;Retwis&lt;/code&gt;，结构简单，性能优异，能很轻易地用&lt;code&gt;N&lt;/code&gt;&lt;code&gt;个web&lt;/code&gt;服务器和&lt;code&gt;Redis&lt;/code&gt;服务器以分布式架构。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;在此获取源码&lt;a href=&#34;http://code.google.com/p/redis/downloads/list&#34;&gt;http://code.google.com/p/redis/downloads/list&lt;/a&gt;&#xA;我们使用PHP作为例子是因为它能被每个人读懂，也能使用Ruby、Python、Erlang或其他语言获取同样(或者更佳)的效果。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;*注意*：&lt;code&gt;Retwis-RB&lt;/code&gt;是一个由&lt;code&gt;Daniel Lucraft&lt;/code&gt;用&lt;code&gt;Ruby&lt;/code&gt;与&lt;code&gt;Sinatra&lt;/code&gt;写的&lt;code&gt;Retwis&lt;/code&gt;分支！此文全部代码在本页尾部的&lt;code&gt;Git repository&lt;/code&gt;链接里。此文以PHP为例，但是&lt;code&gt;Ruby&lt;/code&gt;程序员也能检出其他源码。他们很相似。注意&lt;code&gt;Retwis-J&lt;/code&gt;是&lt;code&gt;Retwis&lt;/code&gt;的一个分支，由&lt;code&gt;Costin Leau&lt;/code&gt;以&lt;code&gt;Java&lt;/code&gt;和&lt;code&gt;Spring&lt;/code&gt;框架写成。源码能在&lt;code&gt;GitHub&lt;/code&gt;找到，并且在&lt;code&gt;springsource.org&lt;/code&gt;有综合的文档。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;key-value-数据库基础&#34;&gt;&lt;code&gt;Key-value&lt;/code&gt; 数据库基础&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;KV&lt;/code&gt;数据的精髓，是能够把&lt;code&gt;value&lt;/code&gt;储存在&lt;code&gt;key&lt;/code&gt;里，此后该数据仅能够通过确切的&lt;code&gt;key&lt;/code&gt;来获取，无法搜索一个值。确切的来讲，它更像一个大型&lt;code&gt;HASH/&lt;/code&gt;字典，但它是持久化的，比如，当你的程序终止运行，数据不会消失。比如我们能用&lt;code&gt;SET&lt;/code&gt;命令以&lt;code&gt;key foo&lt;/code&gt; 来储存值&lt;code&gt;bar&lt;/code&gt;&#xA; &amp;gt;&#xA; SET foo bar&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Redis&lt;/code&gt;会永久储存我们的数据，所以之后我们可以问&lt;code&gt;Redis&lt;/code&gt;：“储存在&lt;code&gt;key foo&lt;/code&gt;里的数据是什么？”，&lt;code&gt;Redis&lt;/code&gt;会返回一个值：&lt;code&gt;bar&lt;/code&gt;&#xA; &amp;gt;&#xA; GET foo =&amp;gt; bar&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;KV&lt;/code&gt;数据库提供的其他常见操作有:&lt;code&gt;DEL&lt;/code&gt;，用于删除指定的&lt;code&gt;key&lt;/code&gt;和关联的&lt;code&gt;value&lt;/code&gt;；&#xA;&lt;code&gt;SET-if-not-exists&lt;/code&gt; (在&lt;code&gt;Redis&lt;/code&gt;上称为&lt;code&gt;SETNX&lt;/code&gt;)仅会在&lt;code&gt;key&lt;/code&gt;不存在的时候设置一个值；&#xA;&lt;code&gt;INCR&lt;/code&gt;能够对指定的&lt;code&gt;key&lt;/code&gt;里储存的数字进行自增。&#xA;&amp;gt;&#xA; SET foo 10&#xA; INCR foo =&amp;gt; 11&#xA; INCR foo =&amp;gt; 12&#xA; INCR foo =&amp;gt; 13&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;原子操作&#34;&gt;原子操作&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;目前为止它是相当简单的，但是&lt;code&gt;INCR&lt;/code&gt;有些不同。设想一下，为什么要提供这个操作？毕竟我们自己能用以下简单的命令实现这个功能：&#xA; &amp;gt;&#xA; x = GET foo&#xA; x = x + 1&#xA; SET foo x&lt;/p&gt;&#xA;&#xA;&lt;p&gt;问题在于要使上面的操作正常进行，同时只能有一个客户端操作&lt;code&gt;x&lt;/code&gt;的值。看看如果两台电脑同时操作这个值会发生什么：&#xA;&amp;gt;&#xA; x = GET foo (返回10)&#xA; y = GET foo (返回10)&#xA; x = x + 1 (x现在是11)&#xA; y = y + 1 (y现在是11)&#xA; SET foo x (foo现在是11)&#xA; SET foo y (foo现在是11)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;问题发生了！我们增加了值两次，本应该从&lt;code&gt;10&lt;/code&gt;变成&lt;code&gt;12&lt;/code&gt;，现在却停留在了&lt;code&gt;11&lt;/code&gt;。这是因为用&lt;code&gt;GET&lt;/code&gt;和&lt;code&gt;SET&lt;/code&gt;来实现&lt;code&gt;INCR&lt;/code&gt;不是一个原子操作(&lt;code&gt;atomic operation&lt;/code&gt;)。所以&lt;code&gt;Redis\memcached&lt;/code&gt;之类提供了一个原子的&lt;code&gt;INCR&lt;/code&gt;命令，服务器会保护&lt;code&gt;get-increment-set&lt;/code&gt;操作，以防止同时的操作。让&lt;code&gt;Redis&lt;/code&gt;与众不同的是它提供了更多类似&lt;code&gt;INCR&lt;/code&gt;的方案，用于解决模型复杂的问题。因此你可以不使用任何SQL数据库、仅用Redis写一个完整的web应用，而不至于抓狂。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;超越-ke-value-数据库&#34;&gt;超越&lt;code&gt;Ke-Value&lt;/code&gt;数据库&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;本节我们会看到构建一个Twitter克隆所需Redis的功能。首先需要知道的是，Redis的值不仅仅可以是字符串(String)。&#xA;Redis的值可以是列表(Lists)也可以是集合(Sets)，在操作更多类型的值时也是原子的，所以多方操作同一个KEY的值也是安全的。&#xA;让我们从一个Lists开始：&#xA;&amp;gt;&#xA; LPUSH mylist a (现在mylist含有一个元素:&amp;lsquo;a&amp;rsquo;的list)&#xA; LPUSH mylist b (现在mylist含有元素&amp;rsquo;b,a&amp;rsquo;)&#xA; LPUSH mylist c (现在mylist含有&amp;rsquo;c,b,a&amp;rsquo;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;LPUSH的意思是Left Push， 就是把一个元素加在列表(list)的左边(或者说头上)。在PUSH操作之前，如果mylist这个键(key)不存在，Redis会自动创建一个空的list。就像你能想到的一样，同样有RPUSH操作可以把元素加在列表(list)的右边(尾部)。这对我们复制一个twitter非常有用，例如我们可以把用户的更新储存在username:updates里。当然，我们也有相应的操作来获取数据或者信息。比如LRANGE返回列表(list)的一个范围内的元素，或者所有元素&#xA;&amp;gt; LRANGE mylist 0 1 =&amp;gt; c,b&lt;/p&gt;&#xA;&#xA;&lt;p&gt;LRANGE使用从零开始的索引(zero-based indexes)，第一个元素的索引是0，第二个是1，以此类推。该命令的参数是：&#xA;&amp;gt;&#xA;LRANGE key first-index last-index&lt;/p&gt;&#xA;&#xA;&lt;p&gt;参数last index可以是负数，具有特殊的意义：-1是列表(list)的最后一个元素，-2是倒数第二个，以此类推。所以，如果要获取整个list，我们能使用以下命令：&#xA;&amp;gt;&#xA; LRANGE mylist 0 -1 =&amp;gt; c,b,a&lt;/p&gt;&#xA;&#xA;&lt;p&gt;其他重要的操作有LLEN，返回列表(list)的长度，LTRIM类似于LRANGE，但不仅仅会返回指定范围内的元素，而且还会原子地把列表(list)的值设置这个新的值。我们将会使用这些list操作，但是注意阅读Redis文档来浏览所有redis支持的list操作。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;数据类型-集合-set&#34;&gt;数据类型：集合(set)&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;除了列表(list)，Redis还提供了集合(sets)的支持，是不排序(unsorted)的元素集合。&#xA;它能够添加、删除、检查元素是否存在，并且获取两个结合之间的交集。当然它也能请求获取集合（set）里一个或者多个元素。&#xA;几个例子可以使概念更为清晰。记住：SADD是往集合(set)里添元素；SREM是从集合(set)里删除元素；SISMEMBER是检测一个元素是否包含在集合里；SINTER用于显示两个集合的交集。&#xA;其他操作有，SCARD用于获取集合的基数(集合中元素的数量)；SMEMBERS返回集合中所有的元素&#xA;&amp;gt;&#xA; SADD myset a&#xA; SADD myset b&#xA; SADD myset foo&#xA; SADD myset bar&#xA; SCARD myset =&amp;gt; 4&#xA; SMEMBERS myset =&amp;gt; bar,a,foo,b&lt;/p&gt;&#xA;&#xA;&lt;p&gt;注意SMEMBERS不会以我们添加的顺序返回元素，因为集合(Sets)是一个未排序的元素集合。如果你要储存顺序，最好使用列表(Lists)取而代之。以下是基于集合的一些操作：&#xA;&amp;gt;&#xA; SADD mynewset b&#xA; SADD mynewset foo&#xA; SADD mynewset hello&#xA; SINTER myset mynewset =&amp;gt; foo,b&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SINTER能够返回集合之间的交集，但并不仅限于两个集合(Sets)，你能获取4个、5个甚至1000个集合(sets)的交集。最后，让我们看下SISMEMBER是如何工作的：&#xA;&amp;gt;&#xA; SISMEMBER myset foo =&amp;gt; 1&#xA; SISMEMBER myset notamember =&amp;gt; 0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Okay，我觉得我们可以开始coding啦！&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;先决条件&#34;&gt;先决条件&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;如果你还没下载，请前往&lt;a href=&#34;http://code.google.com/p/redis/downloads/list&#34;&gt;http://code.google.com/p/redis/downloads/list&lt;/a&gt;下载Retwis的源码。它包含几个PHP文件，是个简单的tar.gz文件。实现的非常简单，你会在里面找到PHP客户端(redis.php)，用于redis与PHP的交互。该库由Ludovico Magnocavallo(&lt;a href=&#34;http://qix.it/&#34;&gt;http://qix.it/&lt;/a&gt; )编写，你可以在自己的项目中免费使用。但如果要更新库的版本请下载Redis的发行版。(注意:现在有更好的PHP库了，请检查我们的客户端页面&lt;a href=&#34;http://redis.io/clients&#34;&gt;http://redis.io/clients&lt;/a&gt;)你需要的另一个东西是正常运行的Redis服务器。仅需要获取源码、用make编译、用./redis-server就完工了，点儿也不须配置就可以在你的电脑上运行Retwis。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;数据结构规划&#34;&gt;数据结构规划&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;当使用关系数据库的时候，这一步往往是在设计数据表、索引的表单里处理。我们没有表，那我们设计什么呢？ 我们需要确认物体使用的key以及key采用的类型。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;让我们从用户这块开始设计。当然了，首先需要展示用户的username, userid, password, followers，自己follow的用户等。第一个问题是：如何在我们的系统中标识一个用户？username是个好主意，因为它是唯一的。不过它太大了，我们想要降低内存的使用。如果我们的数据库是关系数据库，我们能关联唯一ID到每一个用户。每一个对用户的引用都通过ID来关联。做起来很简单，因为我们有我们的原子的INCR命令！当我们创建一个新用户，我们假设这个用户叫&amp;rdquo;antirez&amp;rdquo;：&#xA;&amp;gt;&#xA; INCR global:nextUserId =&amp;gt; 1000&#xA; SET uid:1000:username antirez&#xA; SET uid:1000:password p1pp0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;我们使用global:nextUserId为键(Key)是为了给每个新用户分配一个唯一ID，然后用这个唯一ID来加入其他key，以识别保存用户的其他数据。这就是kv数据库的设计模式!请牢记于心，除了已经定义的KEY，我们还需要更多的来完整定义一个用户，比如有时需要通过用户名来获取用户ID，所以我们也需要设置这么一个键(Key)&#xA;&amp;gt;&#xA; SET username:antirez:uid 1000&lt;/p&gt;&#xA;&#xA;&lt;p&gt;一开始看上去这样很奇怪，但请记住我们只能通过key来获取数据!这不可能告诉Redis返回包含某值的Key，这也是我们的强处。&#xA;用关系数据库方式来讲，这个新实例强迫我们组织数据，以便于仅使用primary key访问任何数据。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;关注-被关注与更新&#34;&gt;关注\被关注与更新&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;这也是在我们系统中另一个重要需求.每个用户都有follower，也有follow的用户.对此我们有最佳的数据结构!那就是&amp;hellip;..集合(Sets).那就让我们在结构中加入两个新字段:&#xA;&amp;gt;&#xA; uid:1000:followers =&amp;gt; Set of uids of all the followers users&#xA; uid:1000:following =&amp;gt; Set of uids of all the following users&lt;/p&gt;&#xA;&#xA;&lt;p&gt;另一个重要的事情是我们需要有个地方来放用户主页上的更新。这个要以时间顺序排序，最新的排在旧的前面。所以，最佳的类型是列表(List)。&#xA;基本上每个更新都会被LPUSH到该用户的updates key.多亏了LRANGE，我们能够实现分页等功能。请注意更新(updates)和帖子(posts)讲的是同一个东西，实际上更新(updates)是有点小的帖子(posts)。&#xA;&amp;gt;&#xA; uid:1000:posts =&amp;gt; a List of post ids, every new post is LPUSHed here.&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;验证&#34;&gt;验证&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;OK，除了验证，或多或少我们已经有了关于该用户的一切东西。我们处理验证用一个简单而健壮(鲁棒)的办法:我们不使用PHP的session或者其他类似方式。&#xA;我们的系统必须是能够在不同不同服务器上分布式部署的，所以一切状态都必须保存在Redis里。所以我们所需要的一个保存在已验证用户cookie里的随机字符串。&#xA;包含同样随机字符串的一个key告诉我们用户的ID。我们需要使用两个key来保证这个验证机制的健壮性:&#xA;&amp;gt;&#xA; SET uid:1000:auth fea5e81ac8ca77622bed1c2132a021f9&#xA; SET auth:fea5e81ac8ca77622bed1c2132a021f9 1000&lt;/p&gt;&#xA;&#xA;&lt;p&gt;为了验证一个用户，我们需要做一些简单的工作(login.php):&#xA;* 从登录表单获取用户的用户名和密码&#xA;* 检查是否存在一个键 username:&lt;username&gt;:uid&#xA;* 如果这个user id存在(假设1000)&#xA;* 检查 uid:1000:password 是否匹配，如果不匹配，显示错误信息&#xA;* 匹配则设置cookie为字符串&amp;rdquo;fea5e81ac8ca77622bed1c2132a021f9&amp;rdquo;(uid:1000:auth的值)&#xA;实例代码:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;    include(&amp;quot;retwis.php&amp;quot;);   &#xA;   &#xA;     # Form sanity checks   &#xA;    if (!gt(&amp;quot;username&amp;quot;) || !gt(&amp;quot;password&amp;quot;))   &#xA;        goback(&amp;quot;You need to enter both username and password to login.&amp;quot;);   &#xA;      &#xA;    # The form is OK, check if the username is available   &#xA;    $username = gt(&amp;quot;username&amp;quot;);   &#xA;    $password = gt(&amp;quot;password&amp;quot;);   &#xA;    $r = redisLink();   &#xA;    $userid = $r-&amp;gt;get(&amp;quot;username:$username:id&amp;quot;);   &#xA;    if (!$userid)   &#xA;     goback(&amp;quot;Wrong username or password&amp;quot;);   &#xA;    $realpassword = $r-&amp;gt;get(&amp;quot;uid:$userid:password&amp;quot;);   &#xA;    if ($realpassword != $password)   &#xA;     goback(&amp;quot;Wrong useranme or password&amp;quot;);   &#xA;    &#xA;    # Username / password OK, set the cookie and redirect to index.php   &#xA;    $authsecret = $r-&amp;gt;get(&amp;quot;uid:$userid:auth&amp;quot;);   &#xA;    setcookie(&amp;quot;auth&amp;quot;,$authsecret,time()+3600*24*365);   &#xA;    header(&amp;quot;Location: index.php&amp;quot;);   &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;每次用户登录都会运行，但我们需要一个函数isLoggedIn用于检验一个用户是否已经验证。&#xA;这些是isLoggedIn的逻辑步骤&#xA;* 从用户获取cookie里auth的值。如果没有cookie，该用户未登录。我们称这个cookie为&lt;authcookie&gt;&#xA;* 检查auth:&lt;authcookie&gt;是否存在，存在则获取值(例子里是1000)&#xA;* 为了再次确认，检查uid:1000:auth是否匹配&#xA;* 用户已验证，在全局变量$User中载入一点信息&#xA;也许代码比描述更短:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt; function isLoggedIn() {   &#xA;      global $User, $_COOKIE;   &#xA;    &#xA;      if (isset($User)) return true;   &#xA;    &#xA;      if (isset($_COOKIE[&#39;auth&#39;])) {   &#xA;          $r = redisLink();   &#xA;          $authcookie = $_COOKIE[&#39;auth&#39;];   &#xA;          if ($userid = $r-&amp;gt;get(&amp;quot;auth:$authcookie&amp;quot;)) {   &#xA;               if ($r-&amp;gt;get(&amp;quot;uid:$userid:auth&amp;quot;) != $authcookie) return false;   &#xA;               loadUserInfo($userid);   &#xA;               return true;   &#xA;           }   &#xA;       }   &#xA;       return false;   &#xA;   }   &#xA;     &#xA;   function loadUserInfo($userid) {   &#xA;       global $User;   &#xA;     &#xA;       $r = redisLink();   &#xA;       $User[&#39;id&#39;] = $userid;   &#xA;       $User[&#39;username&#39;] = $r-&amp;gt;get(&amp;quot;uid:$userid:username&amp;quot;);   &#xA;       return true;   &#xA;   }   &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;把loadUserInfo作为一个独立函数对于我们的应用而言有点杀鸡用牛刀了，但是对于复杂的应用而言这是一个不错的模板。&#xA;作为一个完整的验证，还剩下logout还没实现。在logout的时候我们怎么做呢？&#xA;很简单，仅仅改变uid:1000:auth里的随机字符串，删除旧的auth:&lt;oldauthstring&gt;并增加一个新的auth:&lt;newauthstring&gt;&#xA;&lt;em&gt;重要&lt;/em&gt; :logout过程解释了为什么我们不仅仅查找auth:&lt;randomstring&gt;而是再次检查了uid:1000:auth。真正的验证字符串是后者，auth:&lt;randomstring&gt;是易变的.&#xA;假设程序中有BUGs或者脚本被意外中断，那么就有可能有多个auth:&lt;something&gt;指向同一个用户id。&#xA;logout代码如下:(logout.php)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;    include(&amp;quot;retwis.php&amp;quot;);   &#xA;      &#xA;    if (!isLoggedIn()) {   &#xA;        header(&amp;quot;Location: index.php&amp;quot;);   &#xA;        exit;   &#xA;    }   &#xA;      &#xA;    $r = redisLink();   &#xA;    $newauthsecret = getrand();   &#xA;    $userid = $User[&#39;id&#39;];   &#xA;    $oldauthsecret = $r-&amp;gt;get(&amp;quot;uid:$userid:auth&amp;quot;);   &#xA;    &#xA;    $r-&amp;gt;set(&amp;quot;uid:$userid:auth&amp;quot;,$newauthsecret);   &#xA;    $r-&amp;gt;set(&amp;quot;auth:$newauthsecret&amp;quot;,$userid);   &#xA;    $r-&amp;gt;delete(&amp;quot;auth:$oldauthsecret&amp;quot;);   &#xA;    &#xA;    header(&amp;quot;Location: index.php&amp;quot;);   &#xA;       &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;以上是我们所描述过的，应该比较易于理解。&#xA;更新(Updates)&#xA;更新，或者称为帖子(posts)的实现则更为简单。为了在数据库里创建一个新的帖子，我们做了以下工作:&#xA; INCR global:nextPostId =&amp;gt; 10343&#xA; SET post:10343 &amp;ldquo;$owner_id|$time|I&amp;rsquo;m having fun with Retwis&amp;rdquo;&#xA;就像你看到的一样，帖子的用户id和时间直接储存在了字符串里。&#xA;在这个例子中我们不需要根据时间或者用户id来查找帖子，所以把他们紧凑地挤在一个post字符串里更佳。&#xA;在新建一个帖子之后，我们获得了帖子的id。需要LPUSH这个帖子的id到每一个follow了作者的用户里去，当然还有作者的帖子列表。&#xA;update.php这个文件展示了这个工作是如何完成的:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;    include(&amp;quot;retwis.php&amp;quot;);   &#xA;      &#xA;    if (!isLoggedIn() || !gt(&amp;quot;status&amp;quot;)) {   &#xA;        header(&amp;quot;Location:index.php&amp;quot;);   &#xA;        exit;   &#xA;    }   &#xA;      &#xA;    $r = redisLink();   &#xA;    $postid = $r-&amp;gt;incr(&amp;quot;global:nextPostId&amp;quot;);   &#xA;    $status = str_replace(&amp;quot;\n&amp;quot;,&amp;quot; &amp;quot;,gt(&amp;quot;status&amp;quot;));   &#xA;    $post = $User[&#39;id&#39;].&amp;quot;|&amp;quot;.time().&amp;quot;|&amp;quot;.$status;   &#xA;    $r-&amp;gt;set(&amp;quot;post:$postid&amp;quot;,$post);   &#xA;    $followers = $r-&amp;gt;smembers(&amp;quot;uid:&amp;quot;.$User[&#39;id&#39;].&amp;quot;:followers&amp;quot;);   &#xA;    if ($followers === false) $followers = Array();   &#xA;    $followers[] = $User[&#39;id&#39;]; /* Add the post to our own posts too */  &#xA;    &#xA;    foreach($followers as $fid) {   &#xA;     $r-&amp;gt;push(&amp;quot;uid:$fid:posts&amp;quot;,$postid,false);   &#xA;    }   &#xA;    # Push the post on the timeline, and trim the timeline to the   &#xA;    # newest 1000 elements.   &#xA;    $r-&amp;gt;push(&amp;quot;global:timeline&amp;quot;,$postid,false);   &#xA;    $r-&amp;gt;ltrim(&amp;quot;global:timeline&amp;quot;,0,1000);   &#xA;    &#xA;    header(&amp;quot;Location: index.php&amp;quot;);   &#xA;       &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;函数的核心是foreach。 通过SMEMBERS获取当前用户的所有follower，然后循环会把帖子(post)LPUSH到每一个用户的 uid:&lt;userid&gt;:posts里&#xA;注意我们同时维护了一个所有帖子的时间线。为此我们还需要LPUSH到global:timeline里。&#xA;面对这个现实，你是否开始觉得:SQL里面用ORDER BY来按时间排序有一点儿奇怪? 我确实是这么想的。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;分页&#34;&gt;分页&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;现在很清楚，我们能用LRANGE来获取帖子的范围，并在屏幕上显示。代码很简单:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;  function showPost($id) {   &#xA;    $r = redisLink();   &#xA;    $postdata = $r-&amp;gt;get(&amp;quot;post:$id&amp;quot;);   &#xA;    if (!$postdata) return false;   &#xA;  &#xA;    $aux = explode(&amp;quot;|&amp;quot;,$postdata);   &#xA;    $id = $aux[0];   &#xA;    $time = $aux[1];   &#xA;    $username = $r-&amp;gt;get(&amp;quot;uid:$id:username&amp;quot;);   &#xA;    $post = join(array_splice($aux,2,count($aux)-2),&amp;quot;|&amp;quot;);   &#xA;    $elapsed = strElapsed($time);   &#xA;    $userlink = &amp;quot;&amp;lt;a class=\&amp;quot;username\&amp;quot; href=\&amp;quot;profile.php?u=&amp;quot;.urlencode($username).&amp;quot;\&amp;quot;&amp;gt;&amp;quot;.utf8entities($username).&amp;quot;&amp;lt;/a&amp;gt;&amp;quot;;   &#xA;&#xA;    echo(&#39;&amp;lt;div class=&amp;quot;post&amp;quot;&amp;gt;&#39;.$userlink.&#39; &#39;.utf8entities($post).&amp;quot;&amp;lt;br&amp;gt;&amp;quot;);   &#xA;    echo(&#39;&amp;lt;i&amp;gt;posted &#39;.$elapsed.&#39; ago via web&amp;lt;/i&amp;gt;&amp;lt;/div&amp;gt;&#39;);   &#xA;    return true;   &#xA;  }   &#xA;   &#xA; function showUserPosts($userid,$start,$count) {   &#xA;     $r = redisLink();   &#xA;     $key = ($userid == -1) ? &amp;quot;global:timeline&amp;quot; : &amp;quot;uid:$userid:posts&amp;quot;;   &#xA;     $posts = $r-&amp;gt;lrange($key,$start,$start+$count);   &#xA;     $c = 0;   &#xA;     foreach($posts as $p) {   &#xA;         if (showPost($p)) $c++;   &#xA;         if ($c == $count) break;   &#xA;     }   &#xA;     return count($posts) == $count+1;   &#xA; }   &#xA;  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;当showUserPosts获取帖子的范围并传递给showPost时，showPost会简单输出一篇帖子的HTML代码。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Following users 关注的用户&#xA;如果用户id 1000 (antirez)想要follow用户id1000的pippo，我们做到这个仅需两步SADD:&#xA;&amp;gt;&#xA;SADD uid:1000:following 1001&#xA;SADD uid:1001:followers 1000&lt;/p&gt;&#xA;&#xA;&lt;p&gt;再次注意这个相同的模式:在关系数据库里的理论里follow的用户和被follow的用户是一张包含类似following_id和follower_id的单独数据表。&#xA;用查询你能明确follow和被follow的每一个用户。在key-value数据里有一点特别，需要我们分别设置1000follow了1001并且1001被1000follow的关系。&#xA;这是需要付出的代价，但是另一方面讲，获取这些数据即简单又超快。并且这些是独立的集合，允许我们做一些有趣的事情，比如使用SINTER获取两个不同用户的集合。&#xA;这样我们也许可以在我们的twitter复制品中加入一个功能:当你访问某个人的资料页时显示&amp;rdquo;你和foobar有34个共同关注者&amp;rdquo;之类的东西。&#xA;你能够在follow.php中找到增加或者删除following/folloer关系的代码。它如你所见般平常。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;使它能够水平分割&#34;&gt;使它能够水平分割&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;亲爱的读者，如果你看到这里，你已经是一个英雄了，谢谢你。在讲到水平分割之前，看看单台服务器的性能是个不错的主意。&#xA;Retwis让人惊讶地快，没有任何缓存。在一台非常缓慢和高负载的服务器上，以100个线程并发请求100000次进行apache基准测试，平均占用5ms。&#xA;这意味着你可以仅仅使用一台linux服务器接受每天百万用户的访问，并且慢的跟个傻猴似的，就算用更新的硬件。&#xA;虽然，就算你有一堆用户，也许也不需要超过1台服务器来跑应用，但让我们假设我们是Twitter，需要处理海量的访问量呢?该怎么做?&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;hashing-the-key&#34;&gt;Hashing the key&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;第一件事是把KEY进行hash运算并基于hash在不同服务器上处理请求。有大量知名的hash算法，例如ruby客户端自带的consistent hashing&#xA;大致意思是你能把key转换成数字，并除以你的服务器数量&#xA; server_id = crc32(key) % number_of_servers&#xA;这里还有大量因为添加一台服务器产生的问题，但这仅仅是大致的意思，哪怕使用一个类似consistent hashing的更好索引算法，&#xA;是不是key就可以分布式访问了呢?所有用户数据都分布在不同的服务器上，没有inter-keys使用到(比如SINTER，否则你需要注意要在同一台服务器上进行)&#xA;这是Redis不像memcached一样强制指定索引算法的原因，需要应用来指定。另外，有几个key访问的比较频繁。&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;特殊的keys&#34;&gt;特殊的Keys&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;比如每次发布新帖，我们都需要增加global:nextPostId。单台服务器会有大量增加的请求。如何修复这个问题呢?一个简单的办法是用一台专门的服务器来处理增加请求。&#xA;除非你有大量的请求，否则矫枉过正了。另一个小技巧是ID并不需要真正地增加，只要唯一即可。这样你可以使用长度为不太可能发生碰撞的随机字符串(除了MD5这样的大小，几乎是不可能)。&#xA;完工，我们成功消除了水平分割带来的问题。&#xA;另一个问题是global:timeline。这里有个不是解决办法的解决办法，你可以分别保存在不同服务器上，并且在需要这些数据时从不同的服务器上取出来，或者用一个key来进行排序。&#xA;如果你确实每秒有这么多帖子，你能够再次用一台独立服务器专门处理这些请求。请记住，商用硬件的Redis能够以100000/s的速度写入数据。我猜测对于twitter这足够了。&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 24 Jul 2015 11:02:27 +0000</pubDate>
    </item>
    <item>
      <title>PHP下载文件</title>
      <link>http://openhex.cn/2015/7/24/PHP下载文件.html</link>
      <description>&lt;h3 id=&#34;php实现下载功能&#34;&gt;php实现下载功能&lt;/h3&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;$file = NAC_ROOT.&#39;/ident/theme/&#39;.$theme.&#39;/images/logo_src.gif&#39;;&#xA;        $fileTmp = pathinfo($file);&#xA;        $fileExt = $fileTmp[&#39;extension&#39;];&#xA;        $saveFileName = ($this-&amp;gt;themes[$theme].&#39;.&#39;.$fileExt);&#xA;        $fp=fopen($file,&amp;quot;r&amp;quot;);&#xA;        $file_size=filesize($file);&#xA;        &#xA;        //下载文件需要用到的头&#xA;        Header(&amp;quot;Content-type: application/octet-stream&amp;quot;); &#xA;        Header(&amp;quot;Accept-Ranges: bytes&amp;quot;); &#xA;        Header(&amp;quot;Accept-Length:&amp;quot;.$file_size); &#xA;        Header(&amp;quot;Content-Disposition: attachment; filename=&amp;quot;.$saveFileName); &#xA;        $buffer=1024;&#xA;        $file_count=0;&#xA;        //向浏览器返回数据&#xA;        while(!feof($fp) &amp;amp;&amp;amp; $file_count&amp;lt;$file_size){&#xA;            $file_con=fread($fp,$buffer);&#xA;            $file_count+=$buffer;&#xA;            echo $file_con;&#xA;        }&#xA;        fclose($fp);&#xA;        exit;&#xA; ?&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;</description>
      <pubDate>Fri, 24 Jul 2015 10:59:48 +0000</pubDate>
    </item>
    <item>
      <title>PHP生成缩略图</title>
      <link>http://openhex.cn/2015/7/24/PHP生成缩略图.html</link>
      <description>&lt;p&gt;最近在做一个app,需要保存一些图片的缩略图,由于时间比较紧迫所以直接从网上找了一些开源 的生成缩略图的代码, 只要找到两个个人感觉还不错的代码,下面直接贴出代码,其实原理不是很难,但是要考虑各种情况还是要花费一些时间的.&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h3 id=&#34;代码一&#34;&gt;代码一&lt;/h3&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;&amp;lt;?php&#xA; &#xA;/**&#xA; * 生成缩略图&#xA; * @author yangzhiguo0903@163.com&#xA; * @param string     源图绝对完整地址{带文件名及后缀名}&#xA; * @param string     目标图绝对完整地址{带文件名及后缀名}&#xA; * @param int        缩略图宽{0:此时目标高度不能为0，目标宽度为源图宽*(目标高度/源图高)}&#xA; * @param int        缩略图高{0:此时目标宽度不能为0，目标高度为源图高*(目标宽度/源图宽)}&#xA; * @param int        是否裁切{宽,高必须非0}&#xA; * @param int/float  缩放{0:不缩放, 0&amp;lt;this&amp;lt;1:缩放到相应比例(此时宽高限制和裁切均失效)}&#xA; * @return boolean&#xA; */&#xA;function img2thumb($src_img, $dst_img, $width = 75, $height = 75, $cut = 0, $proportion = 0)&#xA;{&#xA;    if(!is_file($src_img))&#xA;    {&#xA;        return false;&#xA;    }&#xA;    $ot = fileext($dst_img);&#xA;    $otfunc = &#39;image&#39; . ($ot == &#39;jpg&#39; ? &#39;jpeg&#39; : $ot);&#xA;    $srcinfo = getimagesize($src_img);&#xA;    $src_w = $srcinfo[0];&#xA;    $src_h = $srcinfo[1];&#xA;    $type  = strtolower(substr(image_type_to_extension($srcinfo[2]), 1));&#xA;    $createfun = &#39;imagecreatefrom&#39; . ($type == &#39;jpg&#39; ? &#39;jpeg&#39; : $type);&#xA; &#xA;    $dst_h = $height;&#xA;    $dst_w = $width;&#xA;    $x = $y = 0;&#xA;&#xA;    /**&#xA;     * 缩略图不超过源图尺寸（前提是宽或高只有一个）&#xA;     */&#xA;    if(($width&amp;gt; $src_w &amp;amp;&amp;amp; $height&amp;gt; $src_h) || ($height&amp;gt; $src_h &amp;amp;&amp;amp; $width == 0) || ($width&amp;gt; $src_w &amp;amp;&amp;amp; $height == 0))&#xA;    {&#xA;        $proportion = 1;&#xA;    }&#xA;    if($width&amp;gt; $src_w)&#xA;    {&#xA;        $dst_w = $width = $src_w;&#xA;    }&#xA;    if($height&amp;gt; $src_h)&#xA;    {&#xA;        $dst_h = $height = $src_h;&#xA;    }&#xA;    if(!$width &amp;amp;&amp;amp; !$height &amp;amp;&amp;amp; !$proportion)&#xA;    {&#xA;        return false;&#xA;    }&#xA;    if(!$proportion)&#xA;    {&#xA;        if($cut == 0)&#xA;        {&#xA;            if($dst_w &amp;amp;&amp;amp; $dst_h)&#xA;            {&#xA;                if($dst_w/$src_w&amp;gt; $dst_h/$src_h)&#xA;                {&#xA;                    $dst_w = $src_w * ($dst_h / $src_h);&#xA;                    $x = 0 - ($dst_w - $width) / 2;&#xA;                }&#xA;                else&#xA;                {&#xA;                    $dst_h = $src_h * ($dst_w / $src_w);&#xA;                    $y = 0 - ($dst_h - $height) / 2;&#xA;                }&#xA;            }&#xA;            else if($dst_w xor $dst_h)&#xA;            {&#xA;                if($dst_w &amp;amp;&amp;amp; !$dst_h)  //有宽无高&#xA;                {&#xA;                    $propor = $dst_w / $src_w;&#xA;                    $height = $dst_h  = $src_h * $propor;&#xA;                }&#xA;                else if(!$dst_w &amp;amp;&amp;amp; $dst_h)  //有高无宽&#xA;                {&#xA;                    $propor = $dst_h / $src_h;&#xA;                    $width  = $dst_w = $src_w * $propor;&#xA;                }&#xA;            }&#xA;        }&#xA;        else&#xA;        {&#xA;            if(!$dst_h)  //裁剪时无高&#xA;            {&#xA;                $height = $dst_h = $dst_w;&#xA;            }&#xA;            if(!$dst_w)  //裁剪时无宽&#xA;            {&#xA;                $width = $dst_w = $dst_h;&#xA;            }&#xA;            $propor = min(max($dst_w / $src_w, $dst_h / $src_h), 1);&#xA;            $dst_w = (int)round($src_w * $propor);&#xA;            $dst_h = (int)round($src_h * $propor);&#xA;            $x = ($width - $dst_w) / 2;&#xA;            $y = ($height - $dst_h) / 2;&#xA;        }&#xA;    }&#xA;    else&#xA;    {&#xA;        $proportion = min($proportion, 1);&#xA;        $height = $dst_h = $src_h * $proportion;&#xA;        $width  = $dst_w = $src_w * $proportion;&#xA;    }&#xA; &#xA;    $src = $createfun($src_img);&#xA;    $dst = imagecreatetruecolor($width ? $width : $dst_w, $height ? $height : $dst_h);&#xA;    $white = imagecolorallocate($dst, 255, 255, 255);&#xA;    imagefill($dst, 0, 0, $white);&#xA; &#xA;    if(function_exists(&#39;imagecopyresampled&#39;))&#xA;    {&#xA;        imagecopyresampled($dst, $src, $x, $y, 0, 0, $dst_w, $dst_h, $src_w, $src_h);&#xA;    }&#xA;    else&#xA;    {&#xA;        imagecopyresized($dst, $src, $x, $y, 0, 0, $dst_w, $dst_h, $src_w, $src_h);&#xA;    }&#xA;    $otfunc($dst, $dst_img);&#xA;    imagedestroy($dst);&#xA;    imagedestroy($src);&#xA;    return true;&#xA;}&#xA;function fileext($file)&#xA;{&#xA;    return pathinfo($file, PATHINFO_EXTENSION);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2 id=&#34;代码二&#34;&gt;代码二&lt;/h2&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;&amp;lt;?php&#xA;/**&#xA; * 功能：php生成缩略图片的类&#xA; */&#xA;    class ResizeImage{&#xA;        public $type;//图片类型&#xA;        public $width;//实际宽度&#xA;        public $height;//实际高度&#xA;        public $resize_width;//改变后的宽度&#xA;        public $resize_height;//改变后的高度&#xA;        public $cut;//是否裁图&#xA;        public $srcimg;//源图象 &#xA;        public $dstimg;//目标图象地址&#xA;        public $im;//临时创建的图象&#xA;        public $quality;//图片质量&#xA;        function resizeimage($img,$wid,$hei,$c,$dstpath,$quality=100){&#xA;            $this-&amp;gt;srcimg=$img;&#xA;            $this-&amp;gt;resize_width=$wid;&#xA;            $this-&amp;gt;resize_height=$hei;&#xA;            $this-&amp;gt;cut=$c;&#xA;            $this-&amp;gt;quality=$quality;&#xA;            $this-&amp;gt;type=strtolower(substr(strrchr($this-&amp;gt;srcimg,&#39;.&#39;),1));//图片的类型&#xA;            $this-&amp;gt;initi_img();//初始化图象&#xA;            $this -&amp;gt; dst_img($dstpath);//目标图象地址&#xA;            @$this-&amp;gt;width=imagesx($this-&amp;gt;im);&#xA;            @$this-&amp;gt;height=imagesy($this-&amp;gt;im);&#xA;            $this-&amp;gt;newimg();//生成图象&#xA;            @ImageDestroy($this-&amp;gt;im);&#xA;        }&#xA;        function newimg(){&#xA;            $resize_ratio=($this-&amp;gt;resize_width)/($this-&amp;gt;resize_height);//改变后的图象的比例&#xA;            @$ratio=($this-&amp;gt;width)/($this-&amp;gt;height);//实际图象的比例&#xA;            if(($this-&amp;gt;cut)==&#39;1&#39;){//裁图&#xA;                if($img_func===&#39;imagepng&#39;&amp;amp;&amp;amp;(str_replace(&#39;.&#39;,&#39;&#39;,PHP_VERSION)&amp;gt;=512)){  //针对php版本大于5.12参数变化后的处理情况&#xA;                    $quality=9;&#xA;                }&#xA;                if($ratio&amp;gt;=$resize_ratio){//高度优先&#xA;                    $newimg=imagecreatetruecolor($this-&amp;gt;resize_width,$this-&amp;gt;resize_height);&#xA;                    imagecopyresampled($newimg,$this-&amp;gt;im,0,0,0,0,$this-&amp;gt;resize_width,$this-&amp;gt;resize_height,(($this-&amp;gt;height)*$resize_ratio),$this-&amp;gt;height);&#xA;                    imagejpeg($newimg,$this-&amp;gt;dstimg,$this-&amp;gt;quality);&#xA;                }&#xA;                if($ratio&amp;lt;$resize_ratio){//宽度优先&#xA;                    $newimg=imagecreatetruecolor($this-&amp;gt;resize_width,$this-&amp;gt;resize_height);&#xA;                    imagecopyresampled($newimg,$this-&amp;gt;im,0,0,0,0,$this-&amp;gt;resize_width,$this-&amp;gt;resize_height,$this-&amp;gt;width,(($this-&amp;gt;width)/$resize_ratio));&#xA;                    imagejpeg($newimg,$this-&amp;gt;dstimg,$this-&amp;gt;quality);&#xA;                }&#xA;            }else{//不裁图&#xA;                if($ratio&amp;gt;=$resize_ratio){&#xA;                    $newimg=imagecreatetruecolor($this-&amp;gt;resize_width,($this-&amp;gt;resize_width)/$ratio);&#xA;                    imagecopyresampled($newimg,$this-&amp;gt;im,0,0,0,0,$this-&amp;gt;resize_width,($this-&amp;gt;resize_width)/$ratio,$this-&amp;gt;width,$this-&amp;gt;height);&#xA;                    imagejpeg($newimg,$this-&amp;gt;dstimg,$this-&amp;gt;quality);&#xA;                }&#xA;                if($ratio&amp;lt;$resize_ratio){&#xA;                    @$newimg=imagecreatetruecolor(($this-&amp;gt;resize_height)*$ratio,$this-&amp;gt;resize_height);&#xA;                    @imagecopyresampled($newimg,$this-&amp;gt;im,0,0,0,0,($this-&amp;gt;resize_height)*$ratio,$this-&amp;gt;resize_height,$this-&amp;gt;width,$this-&amp;gt;height);&#xA;                    @imagejpeg($newimg,$this-&amp;gt;dstimg,$this-&amp;gt;quality);&#xA;                }&#xA;            }&#xA;        }&#xA;        function initi_img(){//初始化图象&#xA;            if($this-&amp;gt;type==&#39;jpg&#39; || $this-&amp;gt;type==&#39;jpeg&#39;){&#xA;                $this-&amp;gt;im=imagecreatefromjpeg($this-&amp;gt;srcimg);&#xA;            }&#xA;            if($this-&amp;gt;type==&#39;gif&#39;){&#xA;                $this-&amp;gt;im=imagecreatefromgif($this-&amp;gt;srcimg);&#xA;            }&#xA;            if($this-&amp;gt;type==&#39;png&#39;){&#xA;                $this-&amp;gt;im=imagecreatefrompng($this-&amp;gt;srcimg);&#xA;            }&#xA;            if($this-&amp;gt;type==&#39;wbm&#39;){&#xA;                @$this-&amp;gt;im=imagecreatefromwbmp($this-&amp;gt;srcimg);&#xA;            }&#xA;            if($this-&amp;gt;type==&#39;bmp&#39;){&#xA;                $this-&amp;gt;im=$this-&amp;gt;ImageCreateFromBMP($this-&amp;gt;srcimg);&#xA;            }&#xA;        }&#xA;        function dst_img($dstpath){//图象目标地址&#xA;            $full_length=strlen($this-&amp;gt;srcimg);&#xA;            $type_length=strlen($this-&amp;gt;type);&#xA;            $name_length=$full_length-$type_length;&#xA;            $name=substr($this-&amp;gt;srcimg,0,$name_length-1);&#xA;            $this-&amp;gt;dstimg=$dstpath;&#xA;            //echo $this-&amp;gt;dstimg;&#xA;        }&#xA;         &#xA;        function ImageCreateFromBMP($filename){ //自定义函数处理bmp图片&#xA;            if(!$f1=fopen($filename,&amp;quot;rb&amp;quot;))returnFALSE;&#xA;            $FILE=unpack(&amp;quot;vfile_type/Vfile_size/Vreserved/Vbitmap_offset&amp;quot;,fread($f1,14));&#xA;            if($FILE[&#39;file_type&#39;]!=19778)returnFALSE;&#xA;            $BMP=unpack(&#39;Vheader_size/Vwidth/Vheight/vplanes/vbits_per_pixel&#39;.&#xA;                    &#39;/Vcompression/Vsize_bitmap/Vhoriz_resolution&#39;.&#xA;                    &#39;/Vvert_resolution/Vcolors_used/Vcolors_important&#39;,fread($f1,40));&#xA;            $BMP[&#39;colors&#39;]=pow(2,$BMP[&#39;bits_per_pixel&#39;]);&#xA;            if($BMP[&#39;size_bitmap&#39;]==0)$BMP[&#39;size_bitmap&#39;]=$FILE[&#39;file_size&#39;]-$FILE[&#39;bitmap_offset&#39;];&#xA;            $BMP[&#39;bytes_per_pixel&#39;]=$BMP[&#39;bits_per_pixel&#39;]/8;&#xA;            $BMP[&#39;bytes_per_pixel2&#39;]=ceil($BMP[&#39;bytes_per_pixel&#39;]);&#xA;            $BMP[&#39;decal&#39;]=($BMP[&#39;width&#39;]*$BMP[&#39;bytes_per_pixel&#39;]/4);&#xA;            $BMP[&#39;decal&#39;]-=floor($BMP[&#39;width&#39;]*$BMP[&#39;bytes_per_pixel&#39;]/4);&#xA;            $BMP[&#39;decal&#39;]=4-(4*$BMP[&#39;decal&#39;]);&#xA;            if($BMP[&#39;decal&#39;]==4)$BMP[&#39;decal&#39;]=0;&#xA;            $PALETTE=array();&#xA;            if($BMP[&#39;colors&#39;]&amp;lt;16777216)&#xA;            {&#xA;                $PALETTE=unpack(&#39;V&#39;.$BMP[&#39;colors&#39;],fread($f1,$BMP[&#39;colors&#39;]*4));&#xA;            }&#xA;            $IMG=fread($f1,$BMP[&#39;size_bitmap&#39;]);&#xA;            $VIDE=chr(0);&#xA;            $res=imagecreatetruecolor($BMP[&#39;width&#39;],$BMP[&#39;height&#39;]);&#xA;            $P=0;&#xA;            $Y=$BMP[&#39;height&#39;]-1;&#xA;            while($Y&amp;gt;=0)&#xA;            {&#xA;                $X=0;&#xA;                while($X&amp;lt;$BMP[&#39;width&#39;])&#xA;                {&#xA;                    if($BMP[&#39;bits_per_pixel&#39;]==24)&#xA;                        $COLOR=unpack(&amp;quot;V&amp;quot;,substr($IMG,$P,3).$VIDE);&#xA;                    elseif($BMP[&#39;bits_per_pixel&#39;]==16)&#xA;                    {&#xA;                        $COLOR=unpack(&amp;quot;n&amp;quot;,substr($IMG,$P,2));&#xA;                        $COLOR[1]=$PALETTE[$COLOR[1]+1];&#xA;                    }&#xA;                    elseif($BMP[&#39;bits_per_pixel&#39;]==8)&#xA;                    {&#xA;                        $COLOR=unpack(&amp;quot;n&amp;quot;,$VIDE.substr($IMG,$P,1));&#xA;                        $COLOR[1]=$PALETTE[$COLOR[1]+1];&#xA;                    }&#xA;                    elseif($BMP[&#39;bits_per_pixel&#39;]==4)&#xA;                    {&#xA;                        $COLOR=unpack(&amp;quot;n&amp;quot;,$VIDE.substr($IMG,floor($P),1));&#xA;                        if(($P*2)%2==0)$COLOR[1]=($COLOR[1]&amp;gt;&amp;gt;4);else$COLOR[1]=($COLOR[1]&amp;amp;0x0F);&#xA;                        $COLOR[1]=$PALETTE[$COLOR[1]+1];&#xA;                    }&#xA;                    elseif($BMP[&#39;bits_per_pixel&#39;]==1)&#xA;                    {&#xA;                        $COLOR=unpack(&amp;quot;n&amp;quot;,$VIDE.substr($IMG,floor($P),1));&#xA;                        if(($P*8)%8==0)$COLOR[1]=$COLOR[1]&amp;gt;&amp;gt;7;&#xA;                        elseif(($P*8)%8==1)$COLOR[1]=($COLOR[1]&amp;amp;0x40)&amp;gt;&amp;gt;6;&#xA;                        elseif(($P*8)%8==2)$COLOR[1]=($COLOR[1]&amp;amp;0x20)&amp;gt;&amp;gt;5;&#xA;                        elseif(($P*8)%8==3)$COLOR[1]=($COLOR[1]&amp;amp;0x10)&amp;gt;&amp;gt;4;&#xA;                        elseif(($P*8)%8==4)$COLOR[1]=($COLOR[1]&amp;amp;0x8)&amp;gt;&amp;gt;3;&#xA;                        elseif(($P*8)%8==5)$COLOR[1]=($COLOR[1]&amp;amp;0x4)&amp;gt;&amp;gt;2;&#xA;                        elseif(($P*8)%8==6)$COLOR[1]=($COLOR[1]&amp;amp;0x2)&amp;gt;&amp;gt;1;&#xA;                        elseif(($P*8)%8==7)$COLOR[1]=($COLOR[1]&amp;amp;0x1);&#xA;                        $COLOR[1]=$PALETTE[$COLOR[1]+1];&#xA;                    }&#xA;                    else&#xA;                        returnFALSE;&#xA;                    imagesetpixel($res,$X,$Y,$COLOR[1]);&#xA;                    $X++;&#xA;                    $P+=$BMP[&#39;bytes_per_pixel&#39;];&#xA;                }&#xA;                $Y--;&#xA;                $P+=$BMP[&#39;decal&#39;];&#xA;            }&#xA;            fclose($f1);&#xA;            return$res;&#xA;        }&#xA;    }&#xA;?&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Fri, 24 Jul 2015 10:56:20 +0000</pubDate>
    </item>
    <item>
      <title>提高PHP效率的50个编程技巧</title>
      <link>http://openhex.cn/2015/7/24/提高PHP效率的50个编程技巧.html</link>
      <description>&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;用单引号代替双引号来包含字符串，这样做会更快一些。因为PHP会在双引号包围的字符串中搜寻变量， 单引号则不会，注意：只有echo能这么做，它是一种可以把多个字符串当作参数的”函数”(译注：PHP手册中说echo是语言结构，不是真正的函数，故 把函数加上了双引号)。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果能将类的方法定义成static，就尽量定义成static，它的速度会提升将近4倍。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;$row[&amp;lsquo;id&amp;rsquo;] 的速度是$row[id]的7倍。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;echo 比 print 快，并且使用echo的多重参数(译注：指用逗号而不是句点)代替字符串连接，比如echo $str1,$str2。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;在执行for循环之前确定最大循环数，不要每循环一次都计算最大值，最好运用foreach代替。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;注销那些不用的变量尤其是大数组，以便释放内存。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;尽量避免使用&lt;strong&gt;get，&lt;/strong&gt;set，__autoload。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;require_once()代价昂贵。&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;include文件时尽量使用绝对路径，因为它避免了PHP去include_path里查找文件的速度，解析操作系统路径所需的时间会更少。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果你想知道脚本开始执行(译注：即服务器端收到客户端请求)的时刻，使用$_SERVER[&amp;lsquo;REQUEST_TIME&amp;rsquo;]要好于time()&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;函数代替正则表达式完成相同功能。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;str_replace函数比preg_replace函数快，但strtr函数的效率是str_replace函数的四倍。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果一个字符串替换函数，可接受数组或字符作为参数，并且参数长度不太长，那么可以考虑额外写一段替换代码，使得每次传递参数是一个字符，而不是只写一行代码接受数组作为查询和替换的参数。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;使用选择分支语句(译注：即switch case)好于使用多个if，else if语句。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;用@屏蔽错误消息的做法非常低效，极其低效。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;打开apache的mod_deflate模块，可以提高网页的浏览速度。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;数据库连接当使用完毕时应关掉，不要用长连接。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;错误消息代价昂贵。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;在方法中递增局部变量，速度是最快的。几乎与在函数中调用局部变量的速度相当。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;递增一个全局变量要比递增一个局部变量慢2倍。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;递增一个对象属性(如：$this-&amp;gt;prop++)要比递增一个局部变量慢3倍。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;递增一个未预定义的局部变量要比递增一个预定义的局部变量慢9至10倍。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;仅定义一个局部变量而没在函数中调用它，同样会减慢速度(其程度相当于递增一个局部变量)。PHP大概会检查看是否存在全局变量。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;方法调用看来与类中定义的方法的数量无关，因为我(在测试方法之前和之后都)添加了10个方法，但性能上没有变化。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;派生类中的方法运行起来要快于在基类中定义的同样的方法。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;调用带有一个参数的空函数，其花费的时间相当于执行7至8次的局部变量递增操作。类似的方法调用所花费的时间接近于15次的局部变量递增操作。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;Apache解析一个PHP脚本的时间要比解析一个静态HTML页面慢2至10倍。尽量多用静态HTML页面，少用脚本。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;除非脚本可以缓存，否则每次调用时都会重新编译一次。引入一套PHP缓存机制通常可以提升25%至100%的性能，以免除编译开销。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;尽量做缓存，可使用memcached。memcached是一款高性能的内存对象缓存系统，可用来加速动态Web应用程序，减轻数据库负载。对运算码 (OP code)的缓存很有用，使得脚本不必为每个请求做重新编译。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;当操作字符串并需要检验其长度是否满足某种要求时，你想当然地会使用strlen()函数。此函数执行起来相当快，因为它不做任何计算， 只返回在zval 结构(C的内置数据结构，用于存储PHP变量)中存储的已知字符串长度。但是，由于strlen()是函数，多多少少会有些慢，因为函数调用会经过诸多步 骤，如字母小写化(译注：指函数名小写化，PHP不区分函数名大小写). 哈希查找，会跟随被调用的函数一起执行。在某些情况下，你可以使用isset() 技巧加速执行你的代码。&#xA;&amp;gt;&#xA;&amp;gt;(举例如下)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;if (strlen($foo) &amp;lt; 5) { echo “Foo is too short”$$ }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;(与下面的技巧做比较)&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;&amp;gt;if (!isset($foo{5})) { echo “Foo is too short”$$ }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;调用isset()恰巧比strlen()快，因为与后者不同的是，isset()作为一种语言结构，意味着它的执行不需要函数查找和字母小写化。也就是说，实际上在检验字符串长度的顶层代码中你没有花太多开销。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;当执行变量$i的递增或递减时，$i++会比++$i慢一些。这种差异是PHP特有的，并不适用于其他语言，所以请不要修改你的C或 Java代码并指望它们能立即变快，没用的。++$i更快是因为它只需要3条指令(opcodes)，$i++则需要4条指令。后置递增实际上会产生一个 临时变量，这个临时变量随后被递增。而前置递增直接在原值上递增。这是最优化处理的一种，正如Zend的PHP优化器所作的那样。牢记这个优化处理不失为 一个好主意，因为并不是所有的指令优化器都会做同样的优化处理，并且存在大量没有装配指令优化器的互联网服务提供商(ISPs)和服务器。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;并不是事必面向对象(OOP)，面向对象往往开销很大，每个方法和对象调用都会消耗很多内存。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;并非要用类实现所有的数据结构，数组也很有用。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;不要把方法细分得过多，仔细想想你真正打算重用的是哪些代码?&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;当你需要时，你总能把代码分解成方法。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;尽量采用大量的PHP内置函数。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;如果在代码中存在大量耗时的函数，你可以考虑用C扩展的方式实现它们。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;评估检验(profile)你的代码。检验器会告诉你，代码的哪些部分消耗了多少时间。Xdebug调试器包含了检验程序，评估检验总体上可以显示出代码的瓶颈。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;mod_zip可作为Apache模块，用来即时压缩你的数据，并可让数据传输量降低80%。&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;在可以用file_get_contents替代file. fopen. feof. fgets等系列方法的情况下，尽量用 file_get_contents，因为他的效率高得多!但是要注意file_get_contents在打开一个URL文件时候的PHP版本问题;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;尽量的少进行文件操作，虽然PHP的文件操作效率也不低的;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;优化Select SQL语句，在可能的情况下尽量少的进行Insert. Update操作(在update上，我被恶批过);&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;尽可能的使用PHP内部函数(但是我却为了找个PHP里面不存在的函数，浪费了本可以写出一个自定义函数的时间，经验问题啊!);&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;循环内部不要声明变量，尤其是大变量：对象(这好像不只是PHP里面要注意的问题吧?);&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;多维数组尽量不要循环嵌套赋值;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;在可以用PHP内部字符串操作函数的情况下，不要用正则表达式;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;foreach效率更高，尽量用foreach代替while和for循环;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;用单引号替代双引号引用字符串;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;”用i+=1代替i=i+1。符合c/c++的习惯，效率还高”;&lt;/p&gt;&lt;/li&gt;&#xA;&#xA;&lt;li&gt;&lt;p&gt;对global变量，应该用完就unset()掉;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;</description>
      <pubDate>Fri, 24 Jul 2015 10:44:57 +0000</pubDate>
    </item>
    <item>
      <title>十个不错的C开源项目</title>
      <link>http://openhex.cn/2015/7/24/十个不错的C开源项目.html</link>
      <description>&lt;h3 id=&#34;1-webbench&#34;&gt;1. Webbench&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;　　Webbench 是一个在 linux 下使用的非常简单的网站压测工具。它使用 fork ()模拟多个客户端同时访问我们设定的 URL，测试网站在压力下工作的性能，最多可以模拟 3 万个并发连接去测试网站的负载能力。Webbench 使用C语言编写， 代码实在太简洁，源码加起来不到 600 行。&#xA;　　下载链接：&lt;a href=&#34;http://home.tiscali.cz/~cz210552/webbench.html&#34;&gt;http://home.tiscali.cz/~cz210552/webbench.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;2-tinyhttpd&#34;&gt;2. Tinyhttpd&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;　　tinyhttpd 是一个超轻量型 Http Server，使用C语言开发，全部代码只有 502 行(包括注释)，附带一个简单的 Client，可以通过阅读这段代码理解一个 Http Server 的本质。&#xA;　　载链接：&lt;a href=&#34;http://sourceforge.net/projects/tinyhttpd/&#34;&gt;http://sourceforge.net/projects/tinyhttpd/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;3-cjson&#34;&gt;3. cJSON&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;　　cJSON 是C语言中的一个 JSON 编解码器，非常轻量级，C文件只有 500 多行，速度也非常理想。&#xA;　　cJSON 也存在几个弱点，虽然功能不是非常强大，但 cJSON 的小身板和速度是最值得赞赏的。其代码被非常好地维护着，结构也简单易懂，可以作为一个非常好的C语言项目进行学习。&#xA;　　项目主页: &lt;a href=&#34;http://sourceforge.net/projects/cjson/&#34;&gt;http://sourceforge.net/projects/cjson/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;4-cmockery&#34;&gt;4. CMockery&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;　　cmockery 是 google 发布的用于C单元测试的一个轻量级的框架。它很小巧，对其他开源包没有依赖，对被测试代码侵入性小。cmockery 的源代码行数不到 3K，你阅读一下 will_return 和 mock 的源代码就一目了然了。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h4 id=&#34;主要特点&#34;&gt;主要特点：&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;1.免费且开源，google 提供技术支持；&#xA;2.轻量级的框架，使测试更加快速简单；&#xA;3.避免使用复杂的编译器特性，对老版本的编译器来讲，兼容性好;&#xA;4.并不强制要求待测代码必须依赖 C99 标准，这一特性对许多嵌入式系统的开发很有用&#xA;下载链接：&lt;a href=&#34;http://code.google.com/p/cmockery/downloads/list&#34;&gt;http://code.google.com/p/cmockery/downloads/list&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;5-libev&#34;&gt;5. Libev&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;　　libev 是一个开源的事件驱动库，基于 epoll，kqueue 等 OS 提供的基础设施。其以高效出名，它可以将 IO 事件，定时器，和信号统一起来，统一放在事件处理这一套框架下处理。基于 Reactor 模式，效率较高，并且代码精简（4.15 版本 8000 多行），是学习事件驱动编程的很好的资源。&#xA;　　下载链接：&lt;a href=&#34;http://software.schmorp.de/pkg/libev.html&#34;&gt;http://software.schmorp.de/pkg/libev.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;6-memcached&#34;&gt;6. Memcached&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;　　Memcached 是一个高性能的分布式内存对象缓存系统，用于动态 Web 应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提供动态数据库驱动网站的速度。Memcached 基于一个存储键/值对的 hashmap。Memcached-1.4.7 的代码量还是可以接受的，只有 10K 行左右。&#xA;　　下载地址：&lt;a href=&#34;http://memcached.org/&#34;&gt;http://memcached.org/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;7-lua&#34;&gt;7. Lua&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;　　Lua 很棒，Lua 是巴西人发明的，这些都令我不爽，但是还不至于脸红，最多眼红。&#xA;　　让我脸红的是 Lua 的源代码，百分之一百的 ANSI C，一点都不掺杂。在任何支持 ANSI C 编译器的平台上都可以轻松编译通过。我试过，真是一点废话都没有。Lua 的代码数量足够小，5.1.4 仅仅 1.5W 行，去掉空白行和注释估计能到 1W 行。&#xA;　　下载地址：&lt;a href=&#34;http://www.lua.org/&#34;&gt;http://www.lua.org/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;8-sqlite&#34;&gt;8. SQLite&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;　　SQLite 是一个开源的嵌入式关系数据库，实现自包容、零配置、支持事务的 SQL 数据库引擎。 其特点是高度便携、使用方便、结构紧凑、高效、可靠。足够小，大致 3 万行C代码，250K。&lt;/p&gt;&#xA;&#xA;&lt;p&gt;　　下载地址：&lt;a href=&#34;http://www.sqlite.org/&#34;&gt;http://www.sqlite.org/&lt;/a&gt; 。&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;9-unix-v6&#34;&gt;9. UNIX v6&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;　　UNIX V6 的内核源代码包括设备驱动程序在内约有 1 万行，这个数量的源代码，初学者是能够充分理解的。有一种说法是一个人所能理解的代码量上限为 1 万行，UNIX V6 的内核源代码从数量上看正好在这个范围之内。看到这里，大家是不是也有“如果只有 1 万行的话没准儿我也能学会”的想法呢？&#xA;　　另一方面，最近的操作系统，例如 Linux 最新版的内核源代码据说超过了 1000 万行。就算不是初学者，想完全理解全部代码基本上也是不可能的。&#xA;　　下载地址：&lt;a href=&#34;http://minnie.tuhs.org/cgi-bin/utree.pl?file=V6&#34;&gt;http://minnie.tuhs.org/cgi-bin/utree.pl?file=V6&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h3 id=&#34;10-netbsd&#34;&gt;10. NETBSD&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;　　NetBSD 是一个免费的，具有高度移植性的 UNIX-like 操作系统，是现行可移植平台最多的操作系统，可以在许多平台上执行，从 64bit alpha 服务器到手持设备和嵌入式设备。NetBSD 计划的口号是：”Of course it runs NetBSD”。它设计简洁，代码规范，拥有众多先进特性，使得它在业界和学术界广受好评。由于简洁的设计和先进的特征，使得它在生产和研究方面，都有卓越的表现，而且它也有受使用者支持的完整的源代码。许多程序都可以很容易地通过 NetBSD Packages Collection 获得。&#xA;　　下载地址：&lt;a href=&#34;http://www.netbsd.org/&#34;&gt;http://www.netbsd.org/&lt;/a&gt;&lt;/p&gt;&#xA;</description>
      <pubDate>Fri, 24 Jul 2015 10:39:56 +0000</pubDate>
    </item>
    <item>
      <title>附近的人-PHP计算两点球面距离</title>
      <link>http://openhex.cn/2015/7/24/附近的人-PHP计算两点球面距离.html</link>
      <description>&lt;pre&gt;&lt;code&gt;public static double getDistance(double lat1,double longt1 , double lat2,double longt2&#xA;            ) {&#xA;        double PI = 3.14159265358979323; // 圆周率&#xA;        double R = 6371229; // 地球的半径&#xA;&#xA;        double x, y, distance;&#xA;        x = (longt2 - longt1) * PI * R&#xA;                * Math.cos(((lat1 + lat2) / 2) * PI / 180) / 180;&#xA;        y = (lat2 - lat1) * PI * R / 180;&#xA;        distance = Math.hypot(x, y);&#xA;&#xA;        return distance;&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;</description>
      <pubDate>Fri, 24 Jul 2015 10:35:21 +0000</pubDate>
    </item>
    <item>
      <title>Python学习速记--正则表达式</title>
      <link>http://openhex.cn/2015/7/21/Python学习速记-正则表达式.html</link>
      <description>&lt;h4 id=&#34;字符描述&#34;&gt;字符描述&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;| 正则式| 字符|&#xA;|:------:||:--------|&#xA;|\s        |空白符|&#xA;|\d        |数字 |&#xA;|\w        |字母或者数字|&#xA;|.           |任意字符        |&#xA;|*          |任意多个字符(包括0个)|&#xA;|+         |至少一个       |&#xA;|?          |0个或1个      |&#xA;|{n}       |n个               |&#xA;|{n,m}   |n到m个         |&#xA;|[]|         []里是精确值|&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;例子&#34;&gt;例子&lt;/h5&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;\d{3}\-\d{3-8}&lt;/code&gt;匹配类似&lt;code&gt;010-12345678&lt;/code&gt;的字符串&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;[0-9a-zA-Z\_]&lt;/code&gt;匹配一个数字，字母或者下划线&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;[0-9a-zA-Z\_]+&lt;/code&gt;匹配至少有一个数字，字母或者下划线&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;[a-zA-Z\_][0-9a-zA-Z\_]*&lt;/code&gt;字母或者下划线开头 后面接任意的数字，字母或者下划线&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;[a-zA-Z\_][0-9a-zA-Z\_]{0,19}&lt;/code&gt;字母或者下划线开头 后面接0-19个数字，字母或者下划线&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;A|B&lt;/code&gt; 匹配&lt;code&gt;A&lt;/code&gt;或者&lt;code&gt;B&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;^&lt;/code&gt;标示行的开头,&lt;code&gt;^\d&lt;/code&gt;表示以数字开头&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;$&lt;/code&gt;行的结束，&lt;code&gt;\d$&lt;/code&gt;数字结尾&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h4 id=&#34;re模块&#34;&gt;re模块&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;python&lt;/code&gt;提供了&lt;code&gt;re&lt;/code&gt;模块，提供正则匹配常用的功能&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import re&#xA;&#xA;s = r&#39;ABC\123&#39;  #使用r前缀不用再字符串里转义&#xA;print(s) &#xA;&#xA;tel = r&#39;^\d{3}\-\d{3,8}$&#39;&#xA;if re.match(tel,&#39;010-12345783&#39;):&#xA;    print(&#39;ok&#39;)&#xA;else:&#xA;    print(&#39;None&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;切分字符串&#34;&gt;切分字符串&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;#切分字符串&#xA;s = &#39;a,b:   c  d&#39;&#xA;s_list = re.split(r&#39;[\s\,\:]+&#39;,s)&#xA;print(s_list)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;分组&#34;&gt;分组&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;&#xA;#分组&#xA;s = r&#39;(^\d{3})\-(\d{3,8}$)&#39;&#xA;res = re.match(s,&#39;010-2222222&#39;)&#xA;print(res.groups())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;非贪婪匹配&#34;&gt;非贪婪匹配&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;#非贪婪匹配&#xA;s = r&#39;(\d+)(0*)$&#39; #贪婪匹配&#xA;res = re.match(s,&#39;1230000&#39;) &#xA;print(res.groups())&#xA;&#xA;s2 = r&#39;(\d+?)(0*)$&#39; #加个? 非贪婪匹配&#xA;res2 = re.match(s2,&#39;1230000&#39;) &#xA;print(res2.groups())s&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Tue, 21 Jul 2015 12:27:23 +0000</pubDate>
    </item>
    <item>
      <title>Python学习速记--多线程</title>
      <link>http://openhex.cn/2015/7/21/Python学习速记-多线程.html</link>
      <description>&lt;h4 id=&#34;多线程&#34;&gt;多线程&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Python&lt;/code&gt;支持多线程，使用多线程也比较简单，直接使用&lt;code&gt;threading&lt;/code&gt;模块即可&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import time ,threading&#xA; &#xA;def my_thread():&#xA;    print(&#39;新的线程%s&#39;%threading.current_thread().name)&#xA;    n = 0&#xA;    while n&amp;lt;5:&#xA;        n = n + 1&#xA;        print(&#39;线程%s --&amp;gt;  %s&#39;%(threading.current_thread().name,n))&#xA;        time.sleep(1)&#xA;    print(&#39;线程%s结束&#39;% threading.current_thread().name)&#xA;&#xA;print(&#39;线程 %s 正在运行&#39; % threading.current_thread().name)&#xA;t = threading.Thread(target=my_thread, name=&#39;my_thread&#39;)&#xA;t.start()&#xA;t.join()&#xA;print(&#39;线程 %s 结束&#39; % threading.current_thread().name)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h4 id=&#34;锁&#34;&gt;锁&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;进程的资源线程是共享的，因此对于进程的变量，如果再线程中需要修改就需要加入锁的机制&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;import time,threading&#xA;&#xA;#全局变量&#xA;balance = 0&#xA;lock = threading.Lock()&#xA;def change_balance(n):&#xA;    global balance&#xA;    balance = balance + n&#xA;    balance = balance - n&#xA;&#xA;def run_thread(n):&#xA;    for i in range(100000):&#xA;        lock.acquire()&#xA;        try:&#xA;            change_balance(n)&#xA;        finally:&#xA;            lock.release()&#xA;        &#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    t1 = threading.Thread(target=run_thread, args=(5,))&#xA;    t2 = threading.Thread(target=run_thread, args=(8,))&#xA;    t1.start()&#xA;    t2.start()&#xA;    t1.join()&#xA;    t2.join()&#xA;    print(balance)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;threadlocal&#34;&gt;ThreadLocal&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;import threading&#xA;&#xA;local_school = threading.local()&#xA;&#xA;def process_student():&#xA;    std = local_school.student&#xA;    print(&#39;%s线程的学生是%s&#39;%(threading.current_thread().name,std))&#xA;&#xA;def process_thread(name):&#xA;    local_school.student = name&#xA;    process_student()&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    t1 = threading.Thread(target=process_thread,args=(&#39;Tom&#39;,))&#xA;    t2 = threading.Thread(target=process_thread,args=(&#39;Alice&#39;,))&#xA;    t1.start()&#xA;    t2.start()&#xA;    t1.join()&#xA;    t2.join()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Tue, 21 Jul 2015 12:26:25 +0000</pubDate>
    </item>
    <item>
      <title>Python学习速记--多进程</title>
      <link>http://openhex.cn/2015/7/21/Python学习速记-多进程.html</link>
      <description>&lt;h4 id=&#34;多进程&#34;&gt;多进程&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;python&lt;/code&gt;封装了操作系统底层的进程函数，&lt;code&gt;Linux&lt;/code&gt;下使用&lt;code&gt;fork&lt;/code&gt;开辟多进程，&lt;code&gt;windows&lt;/code&gt;下使用&lt;code&gt;multiprocessing&lt;/code&gt;模块使用多进程&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;windows-下使用多进程&#34;&gt;&lt;code&gt;windows&lt;/code&gt;下使用多进程&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;需要使用模块&lt;code&gt;multiproccessing&lt;/code&gt;中的Process&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from multiprocessing import Process&#xA;import os&#xA;&#xA;def proc_run(name):&#xA;    print(&#39;我是子进程%s(%s)&#39; %(name,os.getpid()))&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    print(&#39;即将启动一个子进程...&#39;)&#xA;    child = Process(target=proc_run,args=(&#39;child&#39;,))&#xA;    print(&amp;quot;开始启动子进程&amp;quot;)&#xA;    child.start()&#xA;    child.join() #等待子进程结束后再继续往下运行&#xA;    print(&amp;quot;子进程结束&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h5 id=&#34;进程池&#34;&gt;进程池&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Python&lt;/code&gt;也可以使用&lt;code&gt;multiprocessing&lt;/code&gt;中的线程池来管理子进程&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from multiprocessing import Pool&#xA;import os,time,random&#xA;&#xA;def long_time_task(name):&#xA;    print(&#39;任务%d(%s) 开始执行&#39; %(name,os.getpid()))&#xA;    start = time.time()&#xA;    time.sleep(random.random() * 3)&#xA;    end = time.time()&#xA;    print(&#39;任务%d(%s)执行了%.2f秒&#39;%(name,os.getpid(),end-start))&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    print(&#39;开始执行批量任务&#39;)&#xA;    p = Pool(4) #进程池只允许四个进程 默认是CPU的核数&#xA;    for i in range(5):# 五个进程&#xA;        p.apply_async(long_time_task,args=(i,))&#xA;    print(&#39;等待任务执行完毕&#39;)&#xA;    p.close() #关闭后不能再添加进程&#xA;    p.join() #等待所有子进程执行完毕&#xA;    print(&#39;任务执行完毕&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;进程间的通信&#34;&gt;进程间的通信&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;python&lt;/code&gt;进程的通信使用&lt;code&gt;multiprocessing&lt;/code&gt;模块提供的队列&lt;code&gt;(Queue)&lt;/code&gt;和管道&lt;code&gt;(Pipes)&lt;/code&gt;&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;from multiprocessing import Process,Queue&#xA;import os,time,random&#xA;&#xA;#写进程&#xA;def write(q):&#xA;    for val in [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;]:&#xA;        print(&#39;写字母%s&#39; %val)&#xA;        q.put(val)&#xA;        time.sleep(random.random())&#xA;&#xA;#读进程&#xA;def read(q):&#xA;    while True:&#xA;        val = q.get(True)&#xA;        print(&#39;读字母%s&#39;%val)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    #创建一个队列&#xA;    q = Queue()&#xA;    #开辟两个子进程&#xA;    pw = Process(target=write,args=(q,)) #写进程&#xA;    pr = Process(target=read,args=(q,))  #读进程&#xA;&#xA;    pw.start()&#xA;    pr.start()&#xA;&#xA;    pw.join()&#xA;    pr.terminate() #读进程是死循环 只能通过terminate结束&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Tue, 21 Jul 2015 12:25:31 +0000</pubDate>
    </item>
    <item>
      <title>Python学习速记--序列化</title>
      <link>http://openhex.cn/2015/7/21/Python学习速记-序列化.html</link>
      <description>&lt;h4 id=&#34;使用-pickle-模块序列化&#34;&gt;使用&lt;code&gt;pickle&lt;/code&gt;模块序列化&lt;/h4&gt;&#xA;&lt;pre&gt;&lt;code&gt;#pickle序列化对象&#xA;import pickle&#xA;d = {&#39;name&#39;:&#39;tom&#39;,&#39;age&#39;:22}&#xA;print(pickle.dumps(d))&#xA;f = open(&#39;seria.txt&#39;,&#39;wb&#39;)&#xA;pickle.dump(d,f)  #序列化到文件&#xA;f.close()&#xA;#反序列化&#xA;f = open(&#39;seria.txt&#39;,&#39;rb&#39;)&#xA;d2 = pickle.load(f)&#xA;f.close()&#xA;print(d2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h4 id=&#34;使用-json-序列化&#34;&gt;使用&lt;code&gt;JSON&lt;/code&gt;序列化&lt;/h4&gt;&#xA;&#xA;&lt;h5 id=&#34;序列化-dict&#34;&gt;序列化&lt;code&gt;dict&lt;/code&gt;&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;import json&#xA;#使用`json`序列化&#xA;d = {&#39;name&#39;:&#39;Tom&#39;,&#39;age&#39;:23}&#xA;f = open(&#39;json.txt&#39;,&#39;w&#39;)&#xA;json.dump(d,f) #序列化到文件中&#xA;f.close()&#xA;# 从文件中加载&#xA;f = open(&#39;json.txt&#39;,&#39;r&#39;)&#xA;d2 = json.load(f)&#xA;print(d2)&#xA;&#xA;# 序列化到字符串 &#xA;str = json.dumps(d) #序列化为字符串&#xA;print(str)&#xA;# 反序列化&#xA;d3 = json.loads(str)&#xA;print(d3)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;序列化类&#34;&gt;序列化类&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;# 序列化类&#xA;class Student(object):&#xA;    def __init__(self,name,age):&#xA;        self.name = name&#xA;        self.age = age&#xA;&#xA;#将student转化为dict&#xA;def student2dict(std):&#xA;    return {&#xA;        &#39;name&#39;:std.name,&#xA;        &#39;age&#39;:std.age&#xA;    }&#xA;&#xA;#将dict转化为student&#xA;def dict2student(d):&#xA;    return Student(d[&#39;name&#39;],d[&#39;age&#39;])&#xA;&#xA;stu = Student(&#39;kdf&#39;,23)&#xA;str = json.dumps(stu,default=student2dict) #指定序列化的函数&#xA;#str = json.dumps(stu,default=lambda obj: obj.__dict__) #通用的序列化，但是有的class没有__dict__变量&#xA;print(str)&#xA;#反序列化&#xA;stu2 = json.loads(str,object_hook=dict2student)&#xA;print(stu2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Tue, 21 Jul 2015 12:24:38 +0000</pubDate>
    </item>
    <item>
      <title>Python学习速记--操作文件和目录</title>
      <link>http://openhex.cn/2015/7/21/Python学习速记-操作文件和目录.html</link>
      <description>&lt;h4 id=&#34;操作文件和目录&#34;&gt;操作文件和目录&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;python有一个os模块封装了操作与底层相关的函数，比如查看环境变量，系统名字，操作目录等&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;查看系统参数&#34;&gt;查看系统参数&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;#查看系统参数&#xA;print(os.name) #系统名字&#xA;print(os.environ) #查看环境变量&#xA;print(os.environ.get(&#39;PATH&#39;)) #获取指定的环境变量值&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;操作文件和目录-1&#34;&gt;操作文件和目录&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;利用&lt;code&gt;os&lt;/code&gt;模块提供的功能可以操控目录，包括查看，添加，删除，遍历&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&lt;pre&gt;&lt;code&gt;#操作文件和目录&#xA;cur_path = os.path.abspath(&#39;.&#39;)# 获取当前目录&#xA;print(cur_path)&#xA;new_path = os.path.join(cur_path,&#39;test&#39;) # 通过join获取新的目录，不要自己组装，不同系统下的分隔符是不一样的&#xA;print(new_path)&#xA;os.mkdir(new_path) #创建一个目录&#xA;#os.rmdir(new_path) # 移除一个目录&#xA;#遍历目录&#xA;for x in os.listdir(&#39;.&#39;):&#xA;    if os.path.isdir(x):&#xA;        print(x)&#xA;    elif os.path.isfile(x)  and os.path.splitext(x) == &#39;.py&#39;:&#xA;        print(&#39;python file--&amp;gt;&#39;,x)&#xA;    else:&#xA;        print(&#39;file--&amp;gt;&#39;,x)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Tue, 21 Jul 2015 12:23:33 +0000</pubDate>
    </item>
    <item>
      <title>Python学习速记--IO操作</title>
      <link>http://openhex.cn/2015/7/21/Python学习速记-IO操作.html</link>
      <description>&lt;h4 id=&#34;文件读写&#34;&gt;文件读写&lt;/h4&gt;&#xA;&#xA;&lt;h5 id=&#34;读取文本文件&#34;&gt;读取文本文件&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;#IO操作--文件读取&#xA;f = open(&#39;test.txt&#39;,&#39;r&#39;) # 和c一样打开文件&#xA;#print(f.read())  # read()读取所有内容&#xA;&#xA;#print(f.readline()) # readline 读取一行&#xA;&#xA;#print(f.readlines()) # 读取所有内容到list,每一行一个单位&#xA;&#xA;#异常处理&#xA;try:&#xA;    f = open(&#39;test1.txt&#39;,&#39;r&#39;)&#xA;except IOError:&#xA;    print(&#39;打开文件失败&#39;)&#xA;finally:&#xA;    if f:&#xA;        f.close()&#xA;&#xA;#python提供了更简洁的方式&#xA;with open(&#39;test.txt&#39;,&#39;r&#39;) as f:&#xA;    print(f.read())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&#xA;&lt;h5 id=&#34;读取二进制文件&#34;&gt;读取二进制文件&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;读取文本文件是以UTF-8进行编码，读取二进制文件其实和读取文本文件是一样的，还是打开的模式不一样使用&lt;code&gt;rb&lt;/code&gt;打卡文件，读出来的内容是二进制数据&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#读取二进制文件&#xA;f = open(&#39;Penguins.jpg&#39;,&#39;rb&#39;) # 以rb打开&#xA;print(f.read())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;字符编码&#34;&gt;字符编码&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;读取文本文件默认是utf-8进行编码，也可以在打开文件的时候指定读取的编码，甚至可以自定遇到编码错误时如何处理错误，如下所示&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#字符编码&#xA;f = open(&#39;test.txt&#39;, &#39;r&#39;, encoding=&#39;gbk&#39;, errors=&#39;ignore&#39;)&#xA;print(f.read())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;写文件&#34;&gt;写文件&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;写文件与打开文件也比较类似，主要区别在于打开文件时的模式不同，写文件使用&lt;code&gt;w&lt;/code&gt;或者&lt;code&gt;wb&lt;/code&gt;，也可以指定编码，由于写文件不是立即写到磁盘，而是先写到缓冲区，随后写入，因此在写完后一定有节的调动&lt;code&gt;close()&lt;/code&gt;关闭文件，这样才真正写到磁盘，为了避免忘记写&lt;code&gt;close&lt;/code&gt;，最好使用&lt;code&gt;with&lt;/code&gt;打开文件。示例如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#写文件&#xA;with open(&#39;demo.txt&#39;,&#39;w&#39;)  as f:&#xA;    f.write(&#39;nihaoma&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h4 id=&#34;文件读写-1&#34;&gt;文件读写&lt;/h4&gt;&#xA;&#xA;&lt;h5 id=&#34;读取文本文件-1&#34;&gt;读取文本文件&lt;/h5&gt;&#xA;&lt;pre&gt;&lt;code&gt;#IO操作--文件读取&#xA;f = open(&#39;test.txt&#39;,&#39;r&#39;) # 和c一样打开文件&#xA;#print(f.read())  # read()读取所有内容&#xA;&#xA;#print(f.readline()) # readline 读取一行&#xA;&#xA;#print(f.readlines()) # 读取所有内容到list,每一行一个单位&#xA;&#xA;#异常处理&#xA;try:&#xA;    f = open(&#39;test1.txt&#39;,&#39;r&#39;)&#xA;except IOError:&#xA;    print(&#39;打开文件失败&#39;)&#xA;finally:&#xA;    if f:&#xA;        f.close()&#xA;&#xA;#python提供了更简洁的方式&#xA;with open(&#39;test.txt&#39;,&#39;r&#39;) as f:&#xA;    print(f.read())&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;读取二进制文件-1&#34;&gt;读取二进制文件&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;读取文本文件是以UTF-8进行编码，读取二进制文件其实和读取文本文件是一样的，还是打开的模式不一样使用&lt;code&gt;rb&lt;/code&gt;打卡文件，读出来的内容是二进制数据&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#读取二进制文件&#xA;f = open(&#39;Penguins.jpg&#39;,&#39;rb&#39;) # 以rb打开&#xA;print(f.read())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;字符编码-1&#34;&gt;字符编码&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;读取文本文件默认是utf-8进行编码，也可以在打开文件的时候指定读取的编码，甚至可以自定遇到编码错误时如何处理错误，如下所示&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#字符编码&#xA;f = open(&#39;test.txt&#39;, &#39;r&#39;, encoding=&#39;gbk&#39;, errors=&#39;ignore&#39;)&#xA;print(f.read())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;写文件-1&#34;&gt;写文件&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;写文件与打开文件也比较类似，主要区别在于打开文件时的模式不同，写文件使用&lt;code&gt;w&lt;/code&gt;或者&lt;code&gt;wb&lt;/code&gt;，也可以指定编码，由于写文件不是立即写到磁盘，而是先写到缓冲区，随后写入，因此在写完后一定有节的调动&lt;code&gt;close()&lt;/code&gt;关闭文件，这样才真正写到磁盘，为了避免忘记写&lt;code&gt;close&lt;/code&gt;，最好使用&lt;code&gt;with&lt;/code&gt;打开文件。示例如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#写文件&#xA;with open(&#39;demo.txt&#39;,&#39;w&#39;)  as f:&#xA;    f.write(&#39;nihaoma&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;</description>
      <pubDate>Tue, 21 Jul 2015 12:22:18 +0000</pubDate>
    </item>
    <item>
      <title>Python学习速记--面向对象</title>
      <link>http://openhex.cn/2015/7/21/Python学习速记-面向对象.html</link>
      <description>&lt;p&gt;python支持面向对象编程，定义一个类的方式如下:&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;class Student(object):&#xA;    pass&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;其中class是类的声明，Student是类名，后面括号里的object是该类继承的类，即父类&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#创建一个类&#xA;stu_instance = Student(&#39;jack&#39;,&#39;male&#39;)&#xA;print(stu_instance.name)&#xA;stu_instance.print_info()&#xA;&#xA;#添加私有变量&#xA;class Student2(object):&#xA;    def __init__(self,name,sex):&#xA;        self.__name = name&#xA;        self.__sex = sex&#xA;    def print_info(self):#self必须作为参数&#xA;        print(&#39;name:&#39;,self.__name,&#39;sex:&#39;,self.__sex)&#xA;    &#xA;    def set_name(self,name):&#xA;        self.__name = name&#xA;    def get_name(self):&#xA;        return self__name&#xA;&#xA;    def set_sex(self,sex):&#xA;        self.__sex = sex&#xA;    def get_sex(self):&#xA;        return self.__sex&#xA;&#xA;stu2 = Student2(&#39;jack&#39;,&#39;male&#39;)&#xA;print(stu2._Student2__name) #可以这样访问私有变量&#xA;stu2.print_info()&#xA;print(stu2.get_sex())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;</description>
      <pubDate>Tue, 21 Jul 2015 12:18:18 +0000</pubDate>
    </item>
    <item>
      <title>Python学习速记--python函数相关</title>
      <link>http://openhex.cn/2015/7/21/Python学习速记-python函数相关.html</link>
      <description>&lt;h4 id=&#34;python数据结构&#34;&gt;python数据结构&lt;/h4&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;list = [1,2,3] 方法：list.append(ele),list.insert(pos,ele),list.pop(),list.pop(pos)&lt;/li&gt;&#xA;&lt;li&gt;tuple = (1,2)或(1,2,[3,4])  方法：tuble元素不可变&lt;/li&gt;&#xA;&lt;li&gt;dict = {&amp;lsquo;Tom&amp;rsquo;:12,&amp;lsquo;Michael&amp;rsquo;:23}  方法：dict.get(ele),dict.pop(ele)&lt;/li&gt;&#xA;&lt;li&gt;set = set([1,2,3])  方法：add, remobe&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;h4 id=&#34;函数&#34;&gt;函数&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;函数名其实就是指向一个函数对象的引用，完全可以把函数名赋给一个变量，相当于给这个函数起了一个“别名”&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = abs #变量a指向abs函数&#xA;&amp;gt;&amp;gt;&amp;gt; a(-1) # 所以也可以通过a调用abs函数&#xA;1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;python中，自己定义一个函数使用def语句，依次写出函数名、括号、括号中的参数和冒号:，然后，在缩进块中编写函数体，函数的返回值用&lt;code&gt;return&lt;/code&gt;语句返回。&lt;/p&gt;&#xA;&#xA;&lt;!--more--&gt;&#xA;&lt;pre&gt;&lt;code&gt;#自己定义函数&#xA;def my_abs(x):&#xA;    if not isinstance(x,(int,float)):&#xA;        raise TypeError(&#39;参数类型不匹配&#39;)&#xA;    if x&amp;gt;0:&#xA;        return x&#xA;    else:&#xA;        return -x;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;python 中的函数可以返回多个值，返回的return语句中直接用&lt;code&gt;,&lt;/code&gt;隔开即可，接收变量同样&lt;code&gt;,&lt;/code&gt;隔开，如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#返回多个值&#xA;import math # 导入math包 &#xA;&#xA;def move(x, y, step, angle=0):&#xA;    nx = x + step * math.cos(angle)&#xA;    ny = y - step * math.sin(angle)&#xA;    return nx, ny&#xA;nx,ny = move(0,0,10)&#xA;print(nx,ny)&#xA;#其实返回的还是一个值，是一个tuple&#xA;r1 = move(0,0,10)&#xA;print(r1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;其实返回值是一个tuple&lt;/p&gt;&#xA;&#xA;&lt;h4 id=&#34;函数参数&#34;&gt;函数参数&lt;/h4&gt;&#xA;&#xA;&lt;p&gt;python支持默认参数，放在后面，而且默认参数必须指向不可对象，否则该默认参数的值在多次调用改变参数变量的值是默认参数的值也会放生变化。如下&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;def add_end(L=[]):&#xA;    L.append(&#39;end&#39;)&#xA;    return L;&#xA;print(add_end()) # L的默认值是[]&#xA;print(add_end()) # L的默认值是[&#39;end&#39;],所以结果是[&#39;end&#39;,&#39;end&#39;]&#xA;#修改如下，将默认参数设置为不可变对象，None&#xA;def add_end_2(L=None):&#xA;    if L is None:&#xA;        L = []&#xA;    L.append(&#39;end&#39;)&#xA;    return L&#xA;print(add_end_2()) #L的默认值是None&#xA;print(add_end_2()) #L的默认值是None&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;可变参数&#34;&gt;可变参数&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;pytho如何实现可变参数呢？&#xA;* 可以将list或tuple类型作为参数，这样就实现了输入可变参数，但是调用的时候必须先组装一个list或者tuple&#xA;* 以&lt;code&gt;*param&lt;/code&gt;的形式作为参数，这样调用的时候有几个参数就写几个参数&#xA;对比如下：&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#list形式的可变参数&#xA;def fun_params(list):&#xA;    sum = 0&#xA;    for num in list:&#xA;        sum = sum + num&#xA;    return sum&#xA;print(fun_params([1,2,3])) #参数必须是个list&#xA;# “指针”形式的可变参数&#xA;def fun_params_2(*numbers):&#xA;    sum = 0&#xA;    for num in numbers:&#xA;        sum = sum + num&#xA;    return sum&#xA;print(fun_params_2(1,2,3)) #参数不用组装list,直接有个参数就写几个参数,其实内部参数numbers接收的是一个tuple&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;对于已经有的list，如何调用第二种像是的可变参数函数呢？如下代码&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;list = [1,2,3]&#xA;print(fun_params_2(*list))#只用在list或者tuple变量前加一个*即可&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;*总结*：对于可变参数，允许输入0个或者任意多个参数，内部都是组装成一个tuple&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;关键字参数&#34;&gt;关键字参数&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;可变参数允许你传入0个或任意个参数，这些可变参数在函数调用时自动组装为一个tuple。而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#关键字参数&#xA;def dict_fun(name,age,**kw):&#xA;    print(&#39;name:&#39;,name,&#39;age:&#39;,age,&#39;other:&#39;,kw)&#xA;dict_fun(&#39;jack&#39;,23,city=&#39;beijing&#39;,sex=&#39;male&#39;)#以键值对的方式调用&#xA;#dict形式的调用&#xA;dict_demo = {&#39;city&#39;:&#39;bijing&#39;,&#39;sex&#39;:&#39;male&#39;}&#xA;dict_fun(&#39;jack&#39;,23,**dict_demo)#dict形式的调用,dict变量前加两个**&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h5 id=&#34;明名关键字参数&#34;&gt;明名关键字参数&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;和关键字参数不同的是，命名关键字参数制定了关键字参数的个数和名字，如下&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;#命名关键字参数&#xA;def kw_fun(name,age,*,city=&#39;Beijing&#39;,job):&#xA;    print(name,age,city,job)&#xA;kw_fun(&#39;tom&#39;,23,job=&amp;quot;engineer&amp;quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;还不知道什么时候会使用，，，&lt;/p&gt;&#xA;&#xA;&lt;h5 id=&#34;组合参数&#34;&gt;组合参数&lt;/h5&gt;&#xA;&#xA;&lt;p&gt;在Python中定义函数，可以用必选参数、默认参数、可变参数、关键字参数和命名关键字参数，这5种参数都可以组合使用，除了可变参数无法和命名关键字参数混合。但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数/命名关键字参数和关键字参数。&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;&#xA;def f1(a, b, c=0, *args, **kw):&#xA;    print(&#39;a =&#39;, a, &#39;b =&#39;, b, &#39;c =&#39;, c, &#39;args =&#39;, args, &#39;kw =&#39;, kw)&#xA;f1(1,2,3,4,5,name=&#39;jack&#39;)&#xA;&#xA;def f2(a, b, c=0, *, d,e, **kw):&#xA;    print(a,  b,c, d, e,kw)&#xA;&#xA;f2(1,2,3,d=4,e=5,name=&#39;jack&#39;)&#xA;&#xA;&#xA;args = [1,2,3,4]&#xA;ext = {&#39;name&#39;:&#39;jack&#39;}&#xA;f1(*args,**ext)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3 id=&#34;总结&#34;&gt;总结&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;Python&lt;/code&gt;的函数具有非常灵活的参数形态，既可以实现简单的调用，又可以传入非常复杂的参数。&#xA;默认参数一定要用不可变对象，如果是可变对象，程序运行时会有逻辑错误！&#xA;要注意定义可变参数和关键字参数的语法：&#xA;&lt;code&gt;*args&lt;/code&gt;是可变参数，&lt;code&gt;args&lt;/code&gt;接收的是一个&lt;code&gt;tuple&lt;/code&gt;；&#xA;&lt;code&gt;**kw&lt;/code&gt;是关键字参数，&lt;code&gt;kw&lt;/code&gt;接收的是一个&lt;code&gt;dict&lt;/code&gt;。&#xA;以及调用函数时如何传入可变参数和关键字参数的语法：&#xA;可变参数既可以直接传入：&lt;code&gt;func(1, 2, 3)&lt;/code&gt;，又可以先组装&lt;code&gt;list&lt;/code&gt;或&lt;code&gt;tuple&lt;/code&gt;，再通过&lt;code&gt;*args&lt;/code&gt;传入：&lt;code&gt;func(*(1, 2, 3))&lt;/code&gt;；&#xA;关键字参数既可以直接传入：&lt;code&gt;func(a=1, b=2)&lt;/code&gt;，又可以先组装&lt;code&gt;dict&lt;/code&gt;，再通过&lt;code&gt;**kw&lt;/code&gt;传入：&lt;code&gt;func(**{&#39;a&#39;: 1, &#39;b&#39;: 2})&lt;/code&gt;。&#xA;使用&lt;code&gt;*args&lt;/code&gt;和&lt;code&gt;**kw&lt;/code&gt;是&lt;code&gt;Python&lt;/code&gt;的习惯写法，当然也可以用其他参数名，但最好使用习惯用法。&#xA;命名的关键字参数是为了限制调用者可以传入的参数名，同时可以提供默认值。&#xA;定义命名的关键字参数不要忘了写分隔符*，否则定义的将是位置参数。&lt;/p&gt;&#xA;</description>
      <pubDate>Tue, 21 Jul 2015 09:01:02 +0000</pubDate>
    </item>
  </channel>
</rss>