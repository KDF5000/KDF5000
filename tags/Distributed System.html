<!DOCTYPE html>
<html>
<head lang="en">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <title>多进程 - OpenHex</title>
    <meta name="keywords" content="golang,c&#43;&#43;,distributed system,storage system,对象存储,分布式存储"/>
    <meta name="description" content="A place to rock the distributed world!"/>
    <link rel="stylesheet" href="/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="/css/prism.css"/>
    <link rel="stylesheet" href="/css/style.css"/>
</head>
<body class="post-tag" data-perma="post-tag-多进程">
<header id="header">
    <div class="container">
        <div class="header clearfix">
            <h3 id="site-title">
                <a href="/">OpenHex <sup>Just For Fun</sup></a>
            </h3>
            <nav id="site-nav">
                <ul class="nav nav-inverse nav-pills">
                    <li role="presentation" class="">
                        <a href="/" >Home
                        </a>
                    </li>
                    <li role="presentation" class="">
                        <a href="/archive" >Archive
                        </a>
                    </li>
                </ul>
            </nav>
        </div>
    </div>
</header>

<section id="main">
    <div class="container">
        <div id="article-list">
            <h2 class="tag text-center">Distributed System</h2>
            <article class="article">
                <div class="row">
                    <div class="col-md-10 col-md-offset-1 panel panel-default">
                        <header class="header">
                            <div class="meta">
                        <span class="date">
                            <span class="month">2</span>
                            <span class="day">17</span>
                        </span>
                            </div>
                            <h3 class="title">
                                <a href="/2020/2/17/Microsoft-Azure-Cloud-Storage-Giza.html">Microsoft Azure Cloud Storage: Giza</a>
                            </h3>
                        </header>
                        <section class="brief"><p>Microsoft Azure Cloud Storage Series:</p>

<p><a href="https://webcourse.cs.technion.ac.il/236802/Spring2018/ho/WCFiles/Azure_Cloud_Storage.pdf">Windows Azure Storage: A Highly Available Cloud Storage Service with Strong</a></p>

<p><a href="https://www.cs.princeton.edu/courses/archive/spring13/cos598C/atc12-final181.pdf">Erasure Coding in Windows</a> </p>

<p><a href="http://true/">Giza: Erasure Coding Objects across Global Data Centers</a></p>

<h1 id="overview"><strong>Overview</strong></h1>

<p>Giza是构建在Azure Blob Storage之上的一层服务，在此之前Azure BLOB Storage只支持单Datacenter(虽然可以通过异步复制在不同的Stamp之间同步数据，但只能是一个最终一致性的系统)，并且通过LRC的优化在保证可靠性和可用性的前提下将存储成本降到了足够低(1.3)，但是随着业务的发展，对存储的需求要求越来越高，对DC级别的故障容灾的需求越来越强烈，Giza正是为了解决这个问题，为了保证尽量利用现有的存储基础设施，微软选择了在原有存储系统之上构建多DC的存储系统，这就诞生了Giza：<strong>一个强一致的，支持多版本，使用EC编码并且跨全球数据中心的对象存储系统</strong>。</p>

<p>当前Azure在全球有38个不同的Region，数EB的数据存储量，为了保护用户的数据在磁盘、机器、机架故障的情况下数据依然完好无损，微软设计LRC[1]在保证数据高可用和高可靠的情况下极大的降低了存储成本。但是随着业务的发展，用户对数据安全性的要求越来越高，希望能够在地震、洪水等可能导致Region或者机房级别故障的情况下，数据依然是可以访问的，因此这就要求Azure需要将数据在其他Region也保存一份，这就导致了存储成功成倍的增加。使用EC在单机房非常有效的将存储成本将了下来，自然而然的如果能够将EC数据分布在多个Region或者Datacenter就能够在保证数据可靠性的情况下将低存储成本。但是将EC的多个分片数据分布在不同的Datacenter就会导致每次请求都要从不同的Datacenter获取数据，导致占用cross-dc的带宽占用并增加请求延迟，而且在Datacenter出现故障恢复后会产生大量的跨DC的数据修复。在Datacenter专线带宽有限且费用昂贵的情况下，这种跨Datacenter的EC方案并非最好的解决方案。</p>
</section>
                        <aside class="aside clearfix">
                            <a class="btn btn-primary btn-lg pull-right" href="/2020/2/17/Microsoft-Azure-Cloud-Storage-Giza.html">Read More</a>
                        </aside>
                    </div>
                </div>
            </article>
            
            <article class="article">
                <div class="row">
                    <div class="col-md-10 col-md-offset-1 panel panel-default">
                        <header class="header">
                            <div class="meta">
                        <span class="date">
                            <span class="month">7</span>
                            <span class="day">2</span>
                        </span>
                            </div>
                            <h3 class="title">
                                <a href="/2017/7/2/%e5%88%86%e5%b8%83%e5%bc%8f%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8%e6%af%94%e8%be%83-Ambry.html">分布式对象存储比较-Ambry</a>
                            </h3>
                        </header>
                        <section class="brief"><p>在前面的文章<a href="http://kdf5000.com/2017/06/25/对象存储比较-Haystack/">对象存储比较-Haystack</a>中详细介绍了对象存储的经典之作Haystack，该系统合并小文件，元数据分开管理的思想到现在依然是很多系统的基石，想TFS, FastDFS多少都借用了这个思想。去年(2016)Linkedid在数据管理会议SIGMOD发表了自己内部使用的对象存储系统<a href="http://dprg.cs.uiuc.edu/docs/SIGMOD2016-a/ambry.pdf">Ambry</a>，能发到 SIGMOD，想必也不是等闲之物，因此就拜读了一下。整体感觉相比Haystack考虑的比较全面，诸如数据的负载均衡，partition的异步同步，甚至数据的冷热全都考虑了进来，但是依然能看到Haystack的影子，只能说是在基础上更加的完善，可用性，维护性也更强了。</p>

<h2 id="动机">动机</h2>

<p>在介绍Ambry之前，先看一下为什么要重新自己设计一套对象存储系统，Linkedin的工程师想必也都是有经验的工程师，他们在分析了一下自己的业务场景，已经现在的解决方案，最后得出了一下的结论：
* 存储各种各样大小的对象(图片，文档，音视频)
* 各种不同的负载
* 现有系统大多针对小文件(100k一下)的文件存储进行了优化，但是不能适应各种大小的对象
* 现有系统不能很好的解决，数据访问倾斜，新加入节点导致数据分配不均衡等问题</p>

<p>因此他们决定在前人的肩膀上，重新设计一套适合自己的解决方案，他们认为他们的系统应该有如下的特点：
* 低延迟，高吞吐
* 跨多个数据中心
* 扩展性要强
* 能够很好的解决负载均衡的问题</p>

<h2 id="架构">架构</h2>

<p><img src="/media/archive/blog/images/ambry-arch.png" alt="ambry-arch.png" />
Ambry主要有Cluster Manager，Datanode，Frontend三个组件。在介绍每个组件负责的职责前，先介绍Ambry里的一个重要概念: partition.</p>

<p><strong>Partition:</strong> Partition是Ambry里的最小的存储单元，是一个虚拟的概念，相当于haystack里的Logical Volume,  Twitter BlobStore里的Virtual Bucket，Partition在实现的时候实际上是一个大的文件，存放着成千上万的小文件，并且作为replication的最小单位。因为存在replication，所以每个parition对应着多个位于不同节点，甚至不同机架和数据中心的物理文件，每个文件的大小通常100G，尽可能大，但是不能太大，因为移动数据的时候可能会消耗太多的时间。</p>

<p><strong>Cluster Manager:</strong> Cluster Manager负责管理整个集群的节点，磁盘等布局信息，跟踪partition的状态，以及映射的位置。不同的数据中心都有自己的cluster Manager，通过zookeeper保持状态的同步。</p>

<p><strong>Datanode:</strong> Datanaode是一个实际的物理节点，管理多个磁盘，存储真实的数据，每个datanode有多个partition，并且每个parition会在不同的节点有相应的副本。同时Datanode为每个partition都维护一个常驻内存的索引，Journals和布隆过滤，索引后面会详细介绍。</p>

<p><strong>Frontend:</strong> Frontend是直接客户端交互的组件，所有的请求都是发送给Frontend然后再有其转发到相应的Datanode节点，然后接受数据返回给客户端。Frontend主要有下面三个功能：
- 请求处理转发
- 安全检查。Frontend在接收到客户端的请求的时候会根据设置进行一点的权限之类的安全检查。
- 操作拦截，push到Kafaka。这个功能主要用于监控集群的请求信息，用于分析请求的模式等信息。</p>

<p>Frontend有一个重要的模块叫Router Library，主要用于请求的转发，以及执行相关的逻辑。该模块的的请求操作行为是基于规则的。用户可以设置，每个写入或者get特定partition的请求是写入或者读取几个datanode才算是成功。除了转发请求，该模块还负责将大对象分成固定大小的chunck，然后作为新的blob分发到不同的partition，同时使用一个额外的保存blob的编号等信息，保存在某个paitition，并将partition返回给客户端。Router Library同时还兼顾故障检测，请求代理的功能。</p>

<h2 id="索引-journal和bloom-filter">索引、Journal和Bloom Filter</h2>

<h3 id="索引">索引</h3>

<p>前面提到Datanode的主要作用就是存储不同的paitition，每个partition内保存了成千上万的小blob，每个blob在Partition的布局如下图：
<img src="/media/archive/blog/images/ambry-blob_layout.png" alt="ambry-blob_layout.png" />
当增加一个blob的时候直接append到特定的partition即可。那么当获取一个blob的时候怎么才能够快速地找到相应的数据呢？为此Ambry为每一个Partition创建了一个索引，保存了每个Blob在paitition文件中的偏移位置，大小等信息，不过建索引不是什么新鲜的东西，Haystack也有。但是Linkedin的工程师并没有就此止步，他们为了节约成本，减少索引占用的内存，Ambry讲索引分成不同的segment，然后只有经常访问的blob，也就是热索引才会常驻内存，然后那些冷数据通过在内存维护一个布隆过滤器，当访问冷数据的时候现在布隆过滤器里查找是否存在其对应的索引段，然后再决定加载那段索引 ，从而在保证用少量内存的情况下依然能够有较高的访问速度。
<img src="/media/archive/blog/images/ambry-index.png" alt="ambry-index.png" /></p>

<h3 id="replication和journal">replication和Journal</h3>

<p>replication是保证数据可靠性的一个有效方法，通常在保证数据的一致性上最经典的做法就是使用Paxos等协议，但是在对象存储的场景下，直接向所有或者多数的副本节点发送同步或者异步的请求就可以了，大大的简化了实现难度，并且保证了延迟，如果使用一致性协议则会大大的增加延迟(个人的猜测，也许不是这个原因)。Ambry的replication发生在put操作时候，客户端向Frontend发送put请求，然后根据贪心的策略选择replication的位置，选择的时候根据下面两个原则：
- 不能再同一个Datanode
- 处于多个不同的数据中心</p>

<p>副本的数量有管理员自己配置，选择好replicatio的位置后，异步的像这些节点发送请求，但是会根据管理员的配置等待指定的k(1，多数，all)个节点成功返回才向客户端返回成功的响应。</p>

<p>因为replication的操作是异步的，因此失败的情况是可能发生的。为了保证不同副本的一致性，Ambry使用了一个replication算法，周期性的在后台执行，每个replica都作为一个master从其他节点拉取丢失的blob，为了加快检测的速度，使用journal维护每个节点上每个parititon的lastOffset，然后分下面两个阶段同步副本的blob
- 请求lastOffset后的所有blob id，然后过滤掉本地已经有的blob
- 请求missing的blob</p>

<p><img src="/media/archive/blog/images/ambry-replication.png" alt="ambry-replication.png" /></p>

<h2 id="负载均衡">负载均衡</h2>

<p>负载均衡是Ambry相对于Haystack的一个重要的不同点，针对业务请求负载可能的倾斜，大量大的blobs以及集群节点的变化导致的负载不均衡，Ambry提出了分别针对静态集群，以及动态添加节点时候两种情况提出了自己的均衡算法。</p>

<h3 id="静态集群">静态集群</h3>

<p>针对静态集群，导致其负载均衡主要的原因是大量的大Blob导致数据分布的均衡，已经数据冷热不同从而导致访问的不均衡。对于大的blob，ambry将其分成固定大小(一般4M-8M)的chunk，分发到不同的partition；对于热门数据通过CDN缓存，从而可以减少冷热数据访问不均的问题。</p>

<h3 id="动态集群">动态集群</h3>

<p>对于新增节点的情况，因为新增节点，那么其上面的partition的状态都是可写的，因为新写入的数据往往有更大可能性被访问，因此新节点的partition往往承担大量的读和写，所以导致访问严重不均。Ambry使用了一个rebanlance的算法，该算法定义了一个理想下的读写，只读，磁盘使用率，计算方法都是根据整个集群当前读写状态的，只读磁盘，以及所有使用的磁盘除以整个集群所有的磁盘数得到；然后将每个磁盘上RW, RO状态的partition放入一个pool，最后将pool里的partition分发给低于理想状态的磁盘。整个算法的伪代码如下:
<img src="/media/archive/blog/images/ambry-reblance.png" alt="ambry-reblance.png" /></p>

<p>那么如何移动一个partition到另一个节点呢？Ambry分为三个阶段去安全的移动Partition
- 在新的datanode创键一个replica
- 同步旧的replica所有的Blob到新的replica，同步的时候如果有新的blob写入旧的replica，那么两个replica都要执行append操作
- 同步完成后，删除旧的replica，然后更新映射信息</p>

<h2 id="操作">操作</h2>

<p>Ambry主要提供了Put, Get, Delete三种操作，这三种操作都是客户端直接和Frontend交互，然后Frontend分发请求到相应的partition对应的Datanode，最后将数据或者操作状态返回给客户端。</p>

<p>Delete和Put操作相同，同步写本地数据中心的replica，然后异步写不同数据中心的replica, 只有返回管理员设置的k个成功响应即可。后续通过前面描述的replication算法达到副本的一致性。Put操作在Frontend层的时候会随机选择Partition希尔如，并且声称一个PartitionId + UUID的fid作为blob的唯一id，返回给客户端。对于Delete操作会置delete flag位，并且在后续周期性的compact操作中将其删除。</p>

<p>Get操作请求在Router模块，会提取请求的fid找到对应partitionId，向对应的replica节点发送请求，然后根据设置的成功请求数判断是否成功。如果get时候replica没有完成，则proxy到其他已经存在的数据中心。</p>

<h2 id="总结">总结</h2>

<p>前面介绍了Ambry的整体架构以及核心组件的实现，中间也多多少少和Haystack进行了对比。总的来说Ambry的设计是在Haystack基础上又前进了一步，主要是为了能够适应不同大小的对象，以及解决负载均衡的问题。整个系统印象最深的有三点：
- 将大对象分成多个固定大小的chunck，并用一个新的Blob记录了不同chunck的信息，虽然多消耗了一些空间，但是提高了get数据的并行度，从而能够减低延迟。
- 对Haystack存在的多个副本同步失败的问题，通过引入异步复制，按需设置一致性，后台定期执行replication算法实现最终一致性巧妙的解决了。
- 对于Haystack没有解决的负载均衡的问题，创新性的提出了负载 均衡算法，后台动态调整不同状态partition的分布，实现动态的reblance。</p>
</section>
                        <aside class="aside clearfix">
                            <a class="btn btn-primary btn-lg pull-right" href="/2017/7/2/%e5%88%86%e5%b8%83%e5%bc%8f%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8%e6%af%94%e8%be%83-Ambry.html">Read More</a>
                        </aside>
                    </div>
                </div>
            </article>
            
            <article class="article">
                <div class="row">
                    <div class="col-md-10 col-md-offset-1 panel panel-default">
                        <header class="header">
                            <div class="meta">
                        <span class="date">
                            <span class="month">6</span>
                            <span class="day">25</span>
                        </span>
                            </div>
                            <h3 class="title">
                                <a href="/2017/6/25/%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8%e6%af%94%e8%be%83-Haystack.html">对象存储比较 - Haystack</a>
                            </h3>
                        </header>
                        <section class="brief"><p>经过一段时间的调研，对分布式对象系统有了一点浅显的认识，暂不谈和文件系统的区别(其实还是有点傻傻分不清)，姑且认为对象存储和文件系统最大的区别就是API，文件系统提供了完整POSIX语义，往往具有层次化的目录结构，对文件可以进行精细的操作(open, read, write, seek, delete等)，相对会复杂一些。相比之下，对象存储就简单的多，对象本义是<strong>B</strong>inary <strong>L</strong>arge <strong>OB</strong>ject(BLOB), 是一个大的二进制文件，是作为一个整体出现，不能对其进行修改，因此对象存储系统有一个显著的特点就是Immutable，即不变性，正是因为这个特点对象存储系统一般只提供put, get, 和delete的操作，并且都是以key/val的形式，val一般是整个blob. 具体到实际的资源，主要指的是像图片，视频，文档，源码，二进制程序等这样的文件，这些文件往往都很小，一般在100k左右，视频可能会大一些，能达到几十兆甚至好几个G。</p>

<p>综合上面的一些特点学术界和工业界对对象存储系统也区别于文件系统做了很多优化，其中比较有名的当属10年Facebook在OSDI公开的一个分布式对象存储系统Haystack，针对其内部的业务场景(大量的小图片存储)，创新的提出了一个小文件合并成大文件的方法去存储海量图片的需求，并且得到了很好的实践考研。除了facebook之外，Amazon, twitter， linkdin等知名厂商也都提出了自己的对象存储，其中Amazon的S3作为一个商业服务也取得了巨大的成功。</p>

<p>本文主要调研了Facebook， twitter, linkdin内部使用的对象存储系统，企图从其系统的构建初衷到设计进行一次全面的探索，最后再进行一个对比，方便有需求的同学选择适合自己的系统。</p>
</section>
                        <aside class="aside clearfix">
                            <a class="btn btn-primary btn-lg pull-right" href="/2017/6/25/%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8%e6%af%94%e8%be%83-Haystack.html">Read More</a>
                        </aside>
                    </div>
                </div>
            </article>
            
            <article class="article">
                <div class="row">
                    <div class="col-md-10 col-md-offset-1 panel panel-default">
                        <header class="header">
                            <div class="meta">
                        <span class="date">
                            <span class="month">6</span>
                            <span class="day">18</span>
                        </span>
                            </div>
                            <h3 class="title">
                                <a href="/2017/6/18/SeaweedFs%e7%b4%a2%e5%bc%95-CompactMap.html">SeaweedFs索引-CompactMap</a>
                            </h3>
                        </header>
                        <section class="brief"><p>SeaweedFS提供了几种不同的needle索引策略，包括memory, btree, blotdb, leveldb四种，其中默认的是memory，也是其内部唯一自己实现的一种索引，btree使用google的btree开源实现，boltdb和leveldb都依赖一个db.</p>

<p>memory的索引实现，使用了一个叫CompactMap(这是作者自己起的名)的数据结构。本文后面将会重点介绍compactMap是一个怎样的数据结构，以及如何使用这个数据结构简历needle的索引。</p>

<!--more -->

<h2 id="compactmap">CompactMap</h2>

<p>compactMap本质是一个数组(Golang中可以动态扩展的数组)，其元素类型是CompactSection，定义如下：</p>
<pre><code class="language-go">//This map assumes mostly inserting increasing keys
type CompactMap struct {
    list []*CompactSection
}
</code></pre>

<p>CompactMap结构体内部只有一个CompactSection的结构体的指针数组，那么为什么作者假设插入的key是增序的呢？我们先看CompactSection是一个什么样的结构，答案后面就会很清楚。
下面是CompactSection的结构：</p>
<pre><code class="language-go">type CompactSection struct {
    sync.RWMutex                 //读写锁
    values   []NeedleValue       //NeedleValue数组，保存了Needle的key, offset, size
    overflow map[Key]NeedleValue //用于保存超出batch，但是在(start, end)范围的needleValue
    start    Key                 //该section保存的key最小值
    end      Key                 //该section保存的key最大值
    counter  int                 //当前该section的needleValue的数量
}
</code></pre>

<p>CompactSection保存了key值在[start, end]范围内的所有needleValue，每个section都会记录当前处于该段的所有needleValue数量，并且有一个固定的容量限制，目前版本的限制是100000，如果该段已经存储的needleValue已经达到了最大容量，但是其key那么就会将当前[start,end]范围，那么needleValue保存到map里，也就是说values数组最大长度为100000， 主要是因为当前使用二分搜索，查找key对应的needleValue, 所以数组还是不能太大。使用map也能够快速的定位。</p>

<p>上面简单介绍了compactSection的结构，先不管怎么向某个section插入一个needleValue, 我们只用知道一个compactSection保存从[start,end]范围内的所有needleValue。所以CompactMap的list就是将key分成一段一段管理，至于每一段的key是怎么管理的可以不用管，只用知道每个段的范围即可，当要插入一个key的时候，使用二分查找的方法找到该key所处的section, 然后插入到该section。</p>

<p>下面是二分查找的代码：</p>
<pre><code class="language-go">func (cm *CompactMap) binarySearchCompactSection(key Key) int {
    l, h := 0, len(cm.list)-1
    if h &lt; 0 {
        return -5
    }
    if cm.list[h].start &lt;= key {
        if cm.list[h].counter &lt; batch || key &lt;= cm.list[h].end {
            return h
        }
        return -4
    }
    for l &lt;= h {
        m := (l + h) / 2
        if key &lt; cm.list[m].start {
            h = m - 1
        } else { // cm.list[m].start &lt;= key
            if cm.list[m+1].start &lt;= key {
                l = m + 1
            } else {
                //貌似这里有bug, 如果处于(end, $$start_{m+1}$$), 则同样要插入一个新的section
                /*if(key &gt; cm.list[m].end){
                    return -5
                }*/
                return m
            }
        }
    }
    return -3
}
</code></pre>

<p>下面以上面的代码分析怎么找到一个NeedleValue应该插入的section:
* 如果list为空则直接返回一个负值
* 先判断是否能够插入到最后一个section, 能够插入的条件是该key大于section的最小key值，如果当前section没有超过容量，或者超过了但是处于[start, end]范围，则返回当前section, 否则返回一个负值。
* 前两个条件都不满足的话，就二分搜索前面所有section能够插入的section, 如果到第一个section都没有找到，即key&lt; list[0].start， 说明需要插入一个新的section, 返回一个负值</p>

<p>总的来说，搜索有两个结果，返回一个可以插入的section和返回一个负值，当返回一个负值的时候，就需要重新生成一个compactSection，使用插入排序的方法，将其插入到list
代码如下：</p>
<pre><code class="language-go">func (cm *CompactMap) Set(key Key, offset, size uint32) (oldOffset, oldSize uint32) {
    x := cm.binarySearchCompactSection(key)
    if x &lt; 0 {
        //println(x, &quot;creating&quot;, len(cm.list), &quot;section, starting&quot;, key)
        cm.list = append(cm.list, NewCompactSection(key))
        x = len(cm.list) - 1
        //keep compact section sorted by start
        for x &gt; 0 {
            //插入排序，保证start有序
            if cm.list[x-1].start &gt; cm.list[x].start {
                cm.list[x-1], cm.list[x] = cm.list[x], cm.list[x-1]
                x = x - 1
            } else {
                break
            }
        }
    }
    return cm.list[x].Set(key, offset, size)
}
</code></pre>

<blockquote>
<p><strong>Note:</strong> <del>感觉这里存在一个bug, 因为二分搜索的时候是根据start来查找能够插入的section， 但是存在一种情况，比如假设list[i].start &lt;= key, 且list[i+1].start &gt; key， 那么上面的程序就会返回插入到第i个section, 但是可能list[i].end &lt; key， 也就是说实际上这个时候应该生成一个新的setction插入到除以i和i+1之间。当然作者假设key是递增的（并不一定要连续），如果key是递增的，这样就能能够保证不会出现出现插入一个中间断节的list.但是如果假设是递增的，何必使用二分查找合适的section呢？肯定是最后一个或者需要重新在后面追加一个，所以目前感觉这个逻辑还是有问题的。考虑的非递增的情况，但是非递增的情况下，在二分查找section又没有考虑断节的情况，所以这里应该有问题，二分查找的代码里21-23行是我添加的，应该可以解决这个bug。后分析发现，因为插入除了最后一个section时候，同样也更新了end所以就不存在这个问题</del></p>
</blockquote>

<h3 id="compactsectionmap">CompactSectionMap</h3>

<p>前面已经简单的介绍了CompactSectionMap的结构，主要有一个数组和一个map组成，并且每个section的数组都有一个容量，下面主要分析怎么set和get一个needleValue。
下面是set的核心代码：</p>
<pre><code class="language-go">//return old entry size
func (cs *CompactSection) Set(key Key, offset, size uint32) (oldOffset, oldSize uint32) {
    cs.Lock()
    if key &gt; cs.end {
        cs.end = key
    }
    if i := cs.binarySearchValues(key); i &gt;= 0 {
        oldOffset, oldSize = cs.values[i].Offset, cs.values[i].Size
        //println(&quot;key&quot;, key, &quot;old size&quot;, ret)
        cs.values[i].Offset, cs.values[i].Size = offset, size
    } else {
        needOverflow := cs.counter &gt;= batch
        needOverflow = needOverflow || cs.counter &gt; 0 &amp;&amp; cs.values[cs.counter-1].Key &gt; key
        if needOverflow {
            //println(&quot;start&quot;, cs.start, &quot;counter&quot;, cs.counter, &quot;key&quot;, key)
            if oldValue, found := cs.overflow[key]; found {
                oldOffset, oldSize = oldValue.Offset, oldValue.Size
            }
            cs.overflow[key] = NeedleValue{Key: key, Offset: offset, Size: size}
        } else {
            p := &amp;cs.values[cs.counter]
            p.Key, p.Offset, p.Size = key, offset, size
            //println(&quot;added index&quot;, cs.counter, &quot;key&quot;, key, cs.values[cs.counter].Key)
            cs.counter++
        }
    }
    cs.Unlock()
    return
}
</code></pre>

<p>下面是set的过程：
* 在插入前，首先会进行加锁的操作
* 然后如果当前的key大于end, 那么更新end(这里优于，在执行set之前，compactMap的set里有个判断，如果key大于最后一个section时，如果小于当前容量，则直接返回最后一个section，如果大于当前容量，那么要求要小于最后一个sectiond的end, 这样的结果就是最后一个section的end只有在小于当前容量的时候才会更新。低于其他的section就没有这个限制，这样可以防止中间出现断节从而需要插入新的section)
* 使用二分查找在values数组中查找是否含有该key, 如果含有那么就更新对应的needleValue，返回旧的needleValue
* 如果没有找到，则判断是否需要插入到overflow map中，这里判断是否需要插入到溢出map中并不单单，根据overflow来判断，还有一种情况是，没有大于数组的容量，但是该key小于values数组中最后一个needleValue的key, 这样能够保证values数组的key是有序的
* 如果不需要溢出，那么久直接追加在values数组的最后面</p>

<p>从compactSectionMap中获取一个needleValue的操作比较简单，下面是核心代码：</p>
<pre><code class="language-go">func (cs *CompactSection) Get(key Key) (*NeedleValue, bool) {
    cs.RLock()
    if v, ok := cs.overflow[key]; ok {
        cs.RUnlock()
        return &amp;v, true
    }
    if i := cs.binarySearchValues(key); i &gt;= 0 {
        cs.RUnlock()
        return &amp;cs.values[i], true
    }
    cs.RUnlock()
    return nil, false
}
</code></pre>

<p>get的时候也同样要先加锁，然后先判断overflow map中 是否存在改key, 如果不存在二分搜索values数组，如果都不存在，则返回没有找到即可。</p>

<h2 id="总结">总结</h2>

<p>整个CompactMap的设计思路还是比较清晰的，很像skiplist，通过分段(section)，可以大大的加快查找速度，不过如果key比较随机，那么就会带来很多的二分查找，插入排序的操作，性能就不是那么理想，不过总的来说这个数据结构还是比较高效的。</p>
</section>
                        <aside class="aside clearfix">
                            <a class="btn btn-primary btn-lg pull-right" href="/2017/6/18/SeaweedFs%e7%b4%a2%e5%bc%95-CompactMap.html">Read More</a>
                        </aside>
                    </div>
                </div>
            </article>
            
            <article class="article">
                <div class="row">
                    <div class="col-md-10 col-md-offset-1 panel panel-default">
                        <header class="header">
                            <div class="meta">
                        <span class="date">
                            <span class="month">6</span>
                            <span class="day">13</span>
                        </span>
                            </div>
                            <h3 class="title">
                                <a href="/2017/6/13/%e5%88%86%e5%b8%83%e5%bc%8f%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8%e7%b3%bb%e7%bb%9fSeaweedFS.html">分布式对象存储系统SeaweedFS</a>
                            </h3>
                        </header>
                        <section class="brief"><p>SeaweedFS是Facebook的海量图片存储系统Haystack的一个开源实现，其目标是：
- 存储数亿张图片
- 快速响应的文件服务</p>

<p>seaweedfs提供一个Key-&gt; file的k/v服务，不支持POSIX的文件语义。虽然是Haystack的开源实现，但是也有所不同，比如Haystack论文里提到的使用一个中心节点保存所以文件的metadate。Seaweedfs实现中，中心master是负责文件卷标的管理，具体文件的metadata有相应的valume服务器负责。这样可以大大减缓中心节点的并发的压力，并且提供了快速的文件访问<strong>(一次磁盘操作)</strong>。
<!--more -->
<strong>每个文件的metadata只有50个字节，因此可以在O(1)的时间内从磁盘读取出来。</strong></p>

<p>额外的功能:
* 可以选择不同层次的副本策略：不备份，不同机架，不同数据中心
* 中心节点自动故障恢复 - 无单点问题
* 根据文件mime类型自动选择Gzip压缩
* 更新或者删除文件之后自动compaction
* 同一个集群的服务器可以有不同的磁盘大小，操作系统，文件系统
* 增加或者移除节点不会导致任何数据的re-balance
* 可选的Filer Server提供&rdquo;normal&rdquo;的目录和文件服务
* 可选的文件方向调整修复
* 支持Etag, Accept-Range, Last-Modified等
* 支持in-memory/leveldb/boltdb/btree 模式，以便调整性能/内存的平衡</p>

<h2 id="相关术语">相关术语</h2>

<h3 id="save-file-id">Save File Id</h3>

<p>fid的格式是<code>volumeId, filekey+fileCookie</code>, 其中volumeId是一个32位的无符号整数。fileKey是一个64位的无符号整数。fileCookie是一个64位的无符号整数，用来防止url暴力破解。volumeID, fileKey和fileCookie被编码成十六进制，<strong>获得的fid需要保存到应用服务器</strong>。</p>

<h3 id="writefile">WriteFile</h3>

<p>写文件分为两步：
* 从master获取可以保存的volume信息</p>
<pre><code class="language-shell">curl -X POST http://localhost:9333/dir/assign
</code></pre>

<p>可以返回volume node的相关信息，以及相应的fid</p>
<pre><code class="language-shell">{&quot;fid&quot;:&quot;11,026bfba733&quot;,&quot;url&quot;:&quot;127.0.0.1:8080&quot;,&quot;publicUrl&quot;:&quot;127.0.0.1:8080&quot;,&quot;count&quot;:1}
</code></pre>

<ul>
<li><p>保存文件到相应的volume server</p>
<pre><code class="language-shell">curl -X PUT -F file=@/Users/KDF5000/Documents/2017/Coding/ObjectStorage/SeaweedFS/seaweedfs/weed/vim-key.png http://127.0.0.1:8080/11,026bfba733 
</code></pre>

<p>成功的话返回上传的文件名和文件大小</p>
<pre><code>{&quot;name&quot;:&quot;vim-key.png&quot;,&quot;size&quot;:236702}
</code></pre></li>
</ul>

<h3 id="readfile">ReadFile</h3>

<p>读文件可以根据fid到任何一个节点(master,volumeserver)去读，他会自动的跳转到实际存储的节点。
请求的格式可以至此下面的几种格式：</p>
<pre><code> http://localhost:8080/3/01637037d6/my_preferred_name.jpg
 http://localhost:8080/3/01637037d6.jpg
 http://localhost:8080/3,01637037d6.jpg
 http://localhost:8080/3/01637037d6
 http://localhost:8080/3,01637037d6
</code></pre>

<p>也可以对图片进行伸缩</p>
<pre><code>http://localhost:8080/3/01637037d6.jpg?height=200&amp;width=200
http://localhost:8080/3/01637037d6.jpg?height=200&amp;width=200&amp;mode=fit
http://localhost:8080/3/01637037d6.jpg?height=200&amp;width=200&amp;mode=fill
</code></pre>

<h2 id="架构">架构</h2>

<p>seaweedfs主要有两种服务类型，master server和volume server。</p>

<p>master server可以部署多个节点，使用raft维护一致性，主要负责记录volumeId到volume server的映射关系。</p>

<p>Volume server 是主要的存储节点，有一系列的volume文件(默认32G),每个volume存储很多的小文件，与之对应还有有一个所以文件，记录volume中文件的编号，偏移，大小等信息。Volume server维护本地所有volume的元信息，初始化后保存在内存中，这样当读一个文件的时候只用一次磁盘操作即读出文件的实际内容。</p>

<h3 id="读写文件">读写文件</h3>

<p>写文件的时候需要首先与master server通信，获得可以保存文件的volume信息，master server会返回(volume id, file key, file cookie, volume node url)格式的信息用于保存文件。然后client在请求返回的volume node url以及文件路径保存文件到volume server。</p>

<p>读文件时候client可以根据之前记录的volume server信息，根据(volumeId, file key, file cookie)直接读取文件信息，也可以向master节点查询volume server的url, 然后去读取文件信息。</p>

<h3 id="replication">Replication</h3>

<p>在上传图片的时候，获取fid时候可以指定是否需要保存副本，保存的类型是什么</p>
<pre><code>curl -X POST http://localhost:9333/dir/assign?replication=001
</code></pre>

<p>seaweedfs一共有6种副本类型：
* 000: 没有副本(默认)
* 001: 在同一个机架保存一份副本
* 010: 在同一个数据中心的不同机架保存一份副本
* 100：在不同数据中心保存一份副本
* 200：在两个不同的数据中心保存两个副本
* 110：一个副本在不同机架，一个在不同数据中心</p>
</section>
                        <aside class="aside clearfix">
                            <a class="btn btn-primary btn-lg pull-right" href="/2017/6/13/%e5%88%86%e5%b8%83%e5%bc%8f%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8%e7%b3%bb%e7%bb%9fSeaweedFS.html">Read More</a>
                        </aside>
                    </div>
                </div>
            </article>
            
            <div class="article-pager text-center">
                
                
            </div>
        </div>
    </div>
</section>
<footer id="footer">
    <div class="container text-center">
        <p>© 2015 OpenHex.
            <a href="http://creativecommons.org/licenses/by/3.0/">Some rights reserved </a> |
            <a href="/feed.xml">Feed</a> |
            <a href="/sitemap.xml">Sitemap</a>
        </p>
        <p>Powered by <a href="https://github.com/go-xiaohei/pugo">PuGo 0.10.10 (beta)</a>. Theme by Default.
        </p>
        
    
    

    </div>
</footer>
<script src="/js/jquery-2.1.4.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/prism.min.js"></script>
<script>
    $(document).ready(function () {
        $("pre code").addClass("line-numbers")
    });
</script>
</body>
</html>
